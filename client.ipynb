{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import pearsonr, ks_2samp\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_check():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diabetes_data(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_v2(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    binary_categorical_cols = [col for col in df_base.columns if df_base[col].nunique() == 2 and df_base[col].dtype == 'int64']\n",
    "    continuous_numerical_cols = [col for col in df_base.columns if col not in binary_categorical_cols and df_base[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "    continuous_data = df_base[continuous_numerical_cols].values.astype('float32')\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    continuous_data = standardizer.fit_transform(continuous_data)\n",
    "\n",
    "    binary_data = df_base[binary_categorical_cols].values.astype('float32')\n",
    "\n",
    "    x = np.hstack((continuous_data, binary_data))\n",
    "    # Convert to torch tensor and move to device\n",
    "    x_train = torch.from_numpy(x).to(device)\n",
    "    return x_train, standardizer, df_target, continuous_numerical_cols, binary_categorical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_custom(x_tensor, standardizer, continuous_numerical_cols, binary_categorical_cols):\n",
    "    print(continuous_numerical_cols, binary_categorical_cols)\n",
    "    x_array = x_tensor\n",
    "    df = pd.DataFrame(x_array, columns=continuous_numerical_cols + binary_categorical_cols)\n",
    "    # Separate continuous and binary data\n",
    "    continuous_data = df[continuous_numerical_cols].values\n",
    "    binary_data = df[binary_categorical_cols].values\n",
    "    # Apply inverse transformation only to continuous data\n",
    "    continuous_data = standardizer.inverse_transform(continuous_data)\n",
    "    \n",
    "    # Reconstruct the dataframe to maintain original order\n",
    "    continuous_df = pd.DataFrame(continuous_data, columns=continuous_numerical_cols)\n",
    "    binary_df = pd.DataFrame(binary_data, columns=binary_categorical_cols)\n",
    "\n",
    "    processed_df = pd.concat([continuous_df, binary_df], axis=1)\n",
    "    processed_df = processed_df[df.columns]\n",
    "\n",
    "    return processed_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cardio_data(path, sep=\";\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bank_data(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abalone_data(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    ont_hot_encoder_abalone = OneHotEncoder(sparse_output=False)\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    one_hot_encoded = ont_hot_encoder_abalone.fit_transform(df[categorical_columns])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=ont_hot_encoder_abalone.get_feature_names_out(categorical_columns))\n",
    "\n",
    "    df_encoded = pd.concat([one_hot_df, df], axis=1)\n",
    "    # Drop the original categorical columns\n",
    "    df= df_encoded.drop(categorical_columns, axis=1)\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target, ont_hot_encoder_abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path, data_loader_func ,sep=\",\"):\n",
    "        data = data_loader_func(path, sep)\n",
    "        self.x, self.standardizer, self.outcome, self.continuous_numerical_cols, self.binary_categorical_cols = data\n",
    "        self.len=self.x.shape[0]\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n",
    "\n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "\n",
    "#         # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "#        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "#         # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "#         self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "#         self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "\n",
    "#         # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        lin1 = self.gelu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.gelu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.gelu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.fc1(lin3))\n",
    "\n",
    "        r1 = self.fc21(fc1) # Generating mu\n",
    "        r2 = self.fc22(fc1) # Generating sigma\n",
    "\n",
    "        return r1, r2\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_() # Convert it to std deviation\n",
    "            eps = Variable(std.data.new(std.size()).normal_()) # Generate a noise of same size as std\n",
    "            return eps.mul(std).add_(mu) # Perform reparameterization\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        fc3 = self.gelu(self.fc3(z)) # Not sure why these two are required. \n",
    "        fc4 = self.gelu(self.fc4(fc3))#.view(128, -1)\n",
    "\n",
    "        lin4 = self.gelu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.gelu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # self.decode(z) ist später recon_batch, mu ist mu und logvar ist logvar\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def embed(self,x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    # x_recon is the reconstruction batch created in the forward pass of the model, x is the original x batch, mu is mu, and logvar is logvar\n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_model(data_set, H, H2):\n",
    "    device = cuda_check()\n",
    "    D_in = data_set.x.shape[1]\n",
    "    model = Autoencoder(D_in, H, H2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_mse = customLoss()\n",
    "    return model, loss_mse, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trian the Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(DATA_PATH, loader_func, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    data_set=DataBuilder(DATA_PATH, loader_func , sep)\n",
    "    trainloader=DataLoader(dataset=data_set,batch_size=32)\n",
    "\n",
    "    model, loss_mse, optimizer = declare_model(data_set, 50, 12)\n",
    "    # Refactor\n",
    "    epochs = 1000\n",
    "    train_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, data in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        if epoch % 2 == 0:\n",
    "            print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "                epoch, train_loss / len(trainloader.dataset)))\n",
    "            train_losses.append(train_loss / len(trainloader.dataset))\n",
    "    return model, trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 15.2255\n",
      "====> Epoch: 4 Average loss: 13.2242\n",
      "====> Epoch: 6 Average loss: 11.6673\n",
      "====> Epoch: 8 Average loss: 10.8127\n",
      "====> Epoch: 10 Average loss: 10.0439\n",
      "====> Epoch: 12 Average loss: 9.6780\n",
      "====> Epoch: 14 Average loss: 9.1477\n",
      "====> Epoch: 16 Average loss: 8.7563\n",
      "====> Epoch: 18 Average loss: 8.3776\n",
      "====> Epoch: 20 Average loss: 8.1180\n",
      "====> Epoch: 22 Average loss: 7.8314\n",
      "====> Epoch: 24 Average loss: 7.7385\n",
      "====> Epoch: 26 Average loss: 7.5789\n",
      "====> Epoch: 28 Average loss: 7.3782\n",
      "====> Epoch: 30 Average loss: 7.1825\n",
      "====> Epoch: 32 Average loss: 7.1746\n",
      "====> Epoch: 34 Average loss: 7.0268\n",
      "====> Epoch: 36 Average loss: 6.9396\n",
      "====> Epoch: 38 Average loss: 6.7982\n",
      "====> Epoch: 40 Average loss: 6.8535\n",
      "====> Epoch: 42 Average loss: 6.7480\n",
      "====> Epoch: 44 Average loss: 6.5847\n",
      "====> Epoch: 46 Average loss: 6.6423\n",
      "====> Epoch: 48 Average loss: 6.5244\n",
      "====> Epoch: 50 Average loss: 6.5967\n",
      "====> Epoch: 52 Average loss: 6.4165\n",
      "====> Epoch: 54 Average loss: 6.3871\n",
      "====> Epoch: 56 Average loss: 6.4098\n",
      "====> Epoch: 58 Average loss: 6.4690\n",
      "====> Epoch: 60 Average loss: 6.4038\n",
      "====> Epoch: 62 Average loss: 6.4084\n",
      "====> Epoch: 64 Average loss: 6.2690\n",
      "====> Epoch: 66 Average loss: 6.2512\n",
      "====> Epoch: 68 Average loss: 6.2938\n",
      "====> Epoch: 70 Average loss: 6.1864\n",
      "====> Epoch: 72 Average loss: 6.2598\n",
      "====> Epoch: 74 Average loss: 6.2144\n",
      "====> Epoch: 76 Average loss: 6.3554\n",
      "====> Epoch: 78 Average loss: 6.2373\n",
      "====> Epoch: 80 Average loss: 6.1764\n",
      "====> Epoch: 82 Average loss: 6.1888\n",
      "====> Epoch: 84 Average loss: 6.1849\n",
      "====> Epoch: 86 Average loss: 6.1563\n",
      "====> Epoch: 88 Average loss: 6.1418\n",
      "====> Epoch: 90 Average loss: 6.1966\n",
      "====> Epoch: 92 Average loss: 6.1603\n",
      "====> Epoch: 94 Average loss: 6.2654\n",
      "====> Epoch: 96 Average loss: 6.2007\n",
      "====> Epoch: 98 Average loss: 6.2223\n",
      "====> Epoch: 100 Average loss: 6.2146\n",
      "====> Epoch: 102 Average loss: 6.2172\n",
      "====> Epoch: 104 Average loss: 6.1257\n",
      "====> Epoch: 106 Average loss: 6.1544\n",
      "====> Epoch: 108 Average loss: 6.2066\n",
      "====> Epoch: 110 Average loss: 6.1095\n",
      "====> Epoch: 112 Average loss: 6.1092\n",
      "====> Epoch: 114 Average loss: 6.1647\n",
      "====> Epoch: 116 Average loss: 6.1730\n",
      "====> Epoch: 118 Average loss: 6.0832\n",
      "====> Epoch: 120 Average loss: 6.2406\n",
      "====> Epoch: 122 Average loss: 6.1336\n",
      "====> Epoch: 124 Average loss: 6.0892\n",
      "====> Epoch: 126 Average loss: 6.1421\n",
      "====> Epoch: 128 Average loss: 6.2098\n",
      "====> Epoch: 130 Average loss: 6.0811\n",
      "====> Epoch: 132 Average loss: 6.1666\n",
      "====> Epoch: 134 Average loss: 6.1734\n",
      "====> Epoch: 136 Average loss: 6.0903\n",
      "====> Epoch: 138 Average loss: 6.0576\n",
      "====> Epoch: 140 Average loss: 6.1214\n",
      "====> Epoch: 142 Average loss: 6.1153\n",
      "====> Epoch: 144 Average loss: 6.0894\n",
      "====> Epoch: 146 Average loss: 6.0575\n",
      "====> Epoch: 148 Average loss: 6.1039\n",
      "====> Epoch: 150 Average loss: 6.1063\n",
      "====> Epoch: 152 Average loss: 6.0810\n",
      "====> Epoch: 154 Average loss: 6.0597\n",
      "====> Epoch: 156 Average loss: 6.2204\n",
      "====> Epoch: 158 Average loss: 6.0182\n",
      "====> Epoch: 160 Average loss: 6.0946\n",
      "====> Epoch: 162 Average loss: 6.1080\n",
      "====> Epoch: 164 Average loss: 6.0738\n",
      "====> Epoch: 166 Average loss: 6.0441\n",
      "====> Epoch: 168 Average loss: 6.1620\n",
      "====> Epoch: 170 Average loss: 6.0812\n",
      "====> Epoch: 172 Average loss: 6.0192\n",
      "====> Epoch: 174 Average loss: 6.1460\n",
      "====> Epoch: 176 Average loss: 6.0176\n",
      "====> Epoch: 178 Average loss: 6.0209\n",
      "====> Epoch: 180 Average loss: 6.0949\n",
      "====> Epoch: 182 Average loss: 6.0852\n",
      "====> Epoch: 184 Average loss: 6.0479\n",
      "====> Epoch: 186 Average loss: 6.0897\n",
      "====> Epoch: 188 Average loss: 6.1595\n",
      "====> Epoch: 190 Average loss: 6.0515\n",
      "====> Epoch: 192 Average loss: 6.0167\n",
      "====> Epoch: 194 Average loss: 6.0289\n",
      "====> Epoch: 196 Average loss: 6.0267\n",
      "====> Epoch: 198 Average loss: 6.0199\n",
      "====> Epoch: 200 Average loss: 6.0524\n",
      "====> Epoch: 202 Average loss: 6.0787\n",
      "====> Epoch: 204 Average loss: 6.0053\n",
      "====> Epoch: 206 Average loss: 6.0401\n",
      "====> Epoch: 208 Average loss: 6.0889\n",
      "====> Epoch: 210 Average loss: 5.9860\n",
      "====> Epoch: 212 Average loss: 6.0110\n",
      "====> Epoch: 214 Average loss: 6.0541\n",
      "====> Epoch: 216 Average loss: 6.0033\n",
      "====> Epoch: 218 Average loss: 6.0592\n",
      "====> Epoch: 220 Average loss: 6.0430\n",
      "====> Epoch: 222 Average loss: 6.0338\n",
      "====> Epoch: 224 Average loss: 6.0214\n",
      "====> Epoch: 226 Average loss: 6.0822\n",
      "====> Epoch: 228 Average loss: 6.0603\n",
      "====> Epoch: 230 Average loss: 5.9677\n",
      "====> Epoch: 232 Average loss: 5.9866\n",
      "====> Epoch: 234 Average loss: 5.9594\n",
      "====> Epoch: 236 Average loss: 5.9688\n",
      "====> Epoch: 238 Average loss: 6.0046\n",
      "====> Epoch: 240 Average loss: 5.9926\n",
      "====> Epoch: 242 Average loss: 6.0172\n",
      "====> Epoch: 244 Average loss: 5.9983\n",
      "====> Epoch: 246 Average loss: 5.9963\n",
      "====> Epoch: 248 Average loss: 5.9866\n",
      "====> Epoch: 250 Average loss: 6.0089\n",
      "====> Epoch: 252 Average loss: 6.0011\n",
      "====> Epoch: 254 Average loss: 6.0072\n",
      "====> Epoch: 256 Average loss: 6.0766\n",
      "====> Epoch: 258 Average loss: 6.0456\n",
      "====> Epoch: 260 Average loss: 6.0435\n",
      "====> Epoch: 262 Average loss: 6.0060\n",
      "====> Epoch: 264 Average loss: 5.9395\n",
      "====> Epoch: 266 Average loss: 5.9743\n",
      "====> Epoch: 268 Average loss: 6.0275\n",
      "====> Epoch: 270 Average loss: 6.0479\n",
      "====> Epoch: 272 Average loss: 6.0074\n",
      "====> Epoch: 274 Average loss: 5.9457\n",
      "====> Epoch: 276 Average loss: 6.0031\n",
      "====> Epoch: 278 Average loss: 5.9357\n",
      "====> Epoch: 280 Average loss: 5.9650\n",
      "====> Epoch: 282 Average loss: 5.9012\n",
      "====> Epoch: 284 Average loss: 6.0203\n",
      "====> Epoch: 286 Average loss: 6.0336\n",
      "====> Epoch: 288 Average loss: 5.8842\n",
      "====> Epoch: 290 Average loss: 6.0600\n",
      "====> Epoch: 292 Average loss: 5.9921\n",
      "====> Epoch: 294 Average loss: 5.9864\n",
      "====> Epoch: 296 Average loss: 5.9709\n",
      "====> Epoch: 298 Average loss: 5.9664\n",
      "====> Epoch: 300 Average loss: 5.9299\n",
      "====> Epoch: 302 Average loss: 5.8914\n",
      "====> Epoch: 304 Average loss: 5.9916\n",
      "====> Epoch: 306 Average loss: 6.0162\n",
      "====> Epoch: 308 Average loss: 5.8672\n",
      "====> Epoch: 310 Average loss: 5.9308\n",
      "====> Epoch: 312 Average loss: 5.9593\n",
      "====> Epoch: 314 Average loss: 5.9432\n",
      "====> Epoch: 316 Average loss: 5.9063\n",
      "====> Epoch: 318 Average loss: 5.9387\n",
      "====> Epoch: 320 Average loss: 5.9729\n",
      "====> Epoch: 322 Average loss: 5.9215\n",
      "====> Epoch: 324 Average loss: 5.9134\n",
      "====> Epoch: 326 Average loss: 5.9334\n",
      "====> Epoch: 328 Average loss: 5.9510\n",
      "====> Epoch: 330 Average loss: 5.8696\n",
      "====> Epoch: 332 Average loss: 5.9059\n",
      "====> Epoch: 334 Average loss: 5.9695\n",
      "====> Epoch: 336 Average loss: 5.9302\n",
      "====> Epoch: 338 Average loss: 5.8741\n",
      "====> Epoch: 340 Average loss: 5.9342\n",
      "====> Epoch: 342 Average loss: 5.9579\n",
      "====> Epoch: 344 Average loss: 5.9515\n",
      "====> Epoch: 346 Average loss: 5.9721\n",
      "====> Epoch: 348 Average loss: 5.9578\n",
      "====> Epoch: 350 Average loss: 5.9224\n",
      "====> Epoch: 352 Average loss: 5.9736\n",
      "====> Epoch: 354 Average loss: 5.8554\n",
      "====> Epoch: 356 Average loss: 5.8923\n",
      "====> Epoch: 358 Average loss: 5.8881\n",
      "====> Epoch: 360 Average loss: 5.9006\n",
      "====> Epoch: 362 Average loss: 5.8341\n",
      "====> Epoch: 364 Average loss: 5.8484\n",
      "====> Epoch: 366 Average loss: 5.8642\n",
      "====> Epoch: 368 Average loss: 5.9239\n",
      "====> Epoch: 370 Average loss: 5.8692\n",
      "====> Epoch: 372 Average loss: 5.9109\n",
      "====> Epoch: 374 Average loss: 5.8931\n",
      "====> Epoch: 376 Average loss: 5.8738\n",
      "====> Epoch: 378 Average loss: 5.8886\n",
      "====> Epoch: 380 Average loss: 5.7963\n",
      "====> Epoch: 382 Average loss: 5.8972\n",
      "====> Epoch: 384 Average loss: 5.8749\n",
      "====> Epoch: 386 Average loss: 5.9097\n",
      "====> Epoch: 388 Average loss: 5.8955\n",
      "====> Epoch: 390 Average loss: 5.7977\n",
      "====> Epoch: 392 Average loss: 5.8962\n",
      "====> Epoch: 394 Average loss: 5.9463\n",
      "====> Epoch: 396 Average loss: 5.8476\n",
      "====> Epoch: 398 Average loss: 5.8496\n",
      "====> Epoch: 400 Average loss: 5.9918\n",
      "====> Epoch: 402 Average loss: 5.9496\n",
      "====> Epoch: 404 Average loss: 5.9059\n",
      "====> Epoch: 406 Average loss: 5.9378\n",
      "====> Epoch: 408 Average loss: 5.9575\n",
      "====> Epoch: 410 Average loss: 5.8678\n",
      "====> Epoch: 412 Average loss: 5.8227\n",
      "====> Epoch: 414 Average loss: 5.8907\n",
      "====> Epoch: 416 Average loss: 5.8792\n",
      "====> Epoch: 418 Average loss: 5.8645\n",
      "====> Epoch: 420 Average loss: 5.9534\n",
      "====> Epoch: 422 Average loss: 5.9322\n",
      "====> Epoch: 424 Average loss: 5.9637\n",
      "====> Epoch: 426 Average loss: 5.8031\n",
      "====> Epoch: 428 Average loss: 5.8298\n",
      "====> Epoch: 430 Average loss: 5.8999\n",
      "====> Epoch: 432 Average loss: 5.8958\n",
      "====> Epoch: 434 Average loss: 5.9068\n",
      "====> Epoch: 436 Average loss: 5.9102\n",
      "====> Epoch: 438 Average loss: 5.8584\n",
      "====> Epoch: 440 Average loss: 5.9046\n",
      "====> Epoch: 442 Average loss: 5.8928\n",
      "====> Epoch: 444 Average loss: 5.8759\n",
      "====> Epoch: 446 Average loss: 5.7468\n",
      "====> Epoch: 448 Average loss: 5.8266\n",
      "====> Epoch: 450 Average loss: 5.7858\n",
      "====> Epoch: 452 Average loss: 5.9292\n",
      "====> Epoch: 454 Average loss: 5.9097\n",
      "====> Epoch: 456 Average loss: 5.8553\n",
      "====> Epoch: 458 Average loss: 5.8267\n",
      "====> Epoch: 460 Average loss: 5.8513\n",
      "====> Epoch: 462 Average loss: 5.8596\n",
      "====> Epoch: 464 Average loss: 5.9016\n",
      "====> Epoch: 466 Average loss: 5.8744\n",
      "====> Epoch: 468 Average loss: 5.8176\n",
      "====> Epoch: 470 Average loss: 5.8937\n",
      "====> Epoch: 472 Average loss: 5.8605\n",
      "====> Epoch: 474 Average loss: 5.8347\n",
      "====> Epoch: 476 Average loss: 5.7884\n",
      "====> Epoch: 478 Average loss: 5.8462\n",
      "====> Epoch: 480 Average loss: 5.8960\n",
      "====> Epoch: 482 Average loss: 5.8760\n",
      "====> Epoch: 484 Average loss: 5.7953\n",
      "====> Epoch: 486 Average loss: 5.9447\n",
      "====> Epoch: 488 Average loss: 5.9261\n",
      "====> Epoch: 490 Average loss: 5.8362\n",
      "====> Epoch: 492 Average loss: 5.8357\n",
      "====> Epoch: 494 Average loss: 5.8529\n",
      "====> Epoch: 496 Average loss: 5.8264\n",
      "====> Epoch: 498 Average loss: 5.8290\n",
      "====> Epoch: 500 Average loss: 5.8841\n",
      "====> Epoch: 502 Average loss: 5.7909\n",
      "====> Epoch: 504 Average loss: 5.8905\n",
      "====> Epoch: 506 Average loss: 5.8270\n",
      "====> Epoch: 508 Average loss: 5.8996\n",
      "====> Epoch: 510 Average loss: 5.8462\n",
      "====> Epoch: 512 Average loss: 5.8690\n",
      "====> Epoch: 514 Average loss: 5.8743\n",
      "====> Epoch: 516 Average loss: 5.8416\n",
      "====> Epoch: 518 Average loss: 5.9078\n",
      "====> Epoch: 520 Average loss: 5.8460\n",
      "====> Epoch: 522 Average loss: 5.9218\n",
      "====> Epoch: 524 Average loss: 5.8280\n",
      "====> Epoch: 526 Average loss: 5.9144\n",
      "====> Epoch: 528 Average loss: 5.7837\n",
      "====> Epoch: 530 Average loss: 5.8517\n",
      "====> Epoch: 532 Average loss: 5.8537\n",
      "====> Epoch: 534 Average loss: 5.8048\n",
      "====> Epoch: 536 Average loss: 5.8119\n",
      "====> Epoch: 538 Average loss: 5.8058\n",
      "====> Epoch: 540 Average loss: 5.9100\n",
      "====> Epoch: 542 Average loss: 5.9313\n",
      "====> Epoch: 544 Average loss: 5.8117\n",
      "====> Epoch: 546 Average loss: 5.7997\n",
      "====> Epoch: 548 Average loss: 5.7852\n",
      "====> Epoch: 550 Average loss: 5.7961\n",
      "====> Epoch: 552 Average loss: 5.8468\n",
      "====> Epoch: 554 Average loss: 5.8259\n",
      "====> Epoch: 556 Average loss: 5.8439\n",
      "====> Epoch: 558 Average loss: 5.8164\n",
      "====> Epoch: 560 Average loss: 5.8026\n",
      "====> Epoch: 562 Average loss: 5.8305\n",
      "====> Epoch: 564 Average loss: 5.8157\n",
      "====> Epoch: 566 Average loss: 5.8399\n",
      "====> Epoch: 568 Average loss: 5.8267\n",
      "====> Epoch: 570 Average loss: 5.7808\n",
      "====> Epoch: 572 Average loss: 5.7988\n",
      "====> Epoch: 574 Average loss: 5.9310\n",
      "====> Epoch: 576 Average loss: 5.8082\n",
      "====> Epoch: 578 Average loss: 5.8839\n",
      "====> Epoch: 580 Average loss: 5.8537\n",
      "====> Epoch: 582 Average loss: 5.8007\n",
      "====> Epoch: 584 Average loss: 5.7966\n",
      "====> Epoch: 586 Average loss: 5.7690\n",
      "====> Epoch: 588 Average loss: 5.7916\n",
      "====> Epoch: 590 Average loss: 5.8013\n",
      "====> Epoch: 592 Average loss: 5.8212\n",
      "====> Epoch: 594 Average loss: 5.8060\n",
      "====> Epoch: 596 Average loss: 5.8043\n",
      "====> Epoch: 598 Average loss: 5.8853\n",
      "====> Epoch: 600 Average loss: 5.8324\n",
      "====> Epoch: 602 Average loss: 5.8150\n",
      "====> Epoch: 604 Average loss: 5.8412\n",
      "====> Epoch: 606 Average loss: 5.7712\n",
      "====> Epoch: 608 Average loss: 5.7716\n",
      "====> Epoch: 610 Average loss: 5.8390\n",
      "====> Epoch: 612 Average loss: 5.7703\n",
      "====> Epoch: 614 Average loss: 5.8369\n",
      "====> Epoch: 616 Average loss: 5.7703\n",
      "====> Epoch: 618 Average loss: 5.7294\n",
      "====> Epoch: 620 Average loss: 5.8205\n",
      "====> Epoch: 622 Average loss: 5.7300\n",
      "====> Epoch: 624 Average loss: 5.7964\n",
      "====> Epoch: 626 Average loss: 5.6322\n",
      "====> Epoch: 628 Average loss: 5.8155\n",
      "====> Epoch: 630 Average loss: 5.8112\n",
      "====> Epoch: 632 Average loss: 5.7209\n",
      "====> Epoch: 634 Average loss: 5.8069\n",
      "====> Epoch: 636 Average loss: 5.8189\n",
      "====> Epoch: 638 Average loss: 5.8632\n",
      "====> Epoch: 640 Average loss: 5.7953\n",
      "====> Epoch: 642 Average loss: 5.8039\n",
      "====> Epoch: 644 Average loss: 5.7001\n",
      "====> Epoch: 646 Average loss: 5.7504\n",
      "====> Epoch: 648 Average loss: 5.7995\n",
      "====> Epoch: 650 Average loss: 5.8135\n",
      "====> Epoch: 652 Average loss: 5.7614\n",
      "====> Epoch: 654 Average loss: 5.8275\n",
      "====> Epoch: 656 Average loss: 5.7826\n",
      "====> Epoch: 658 Average loss: 5.9217\n",
      "====> Epoch: 660 Average loss: 5.7747\n",
      "====> Epoch: 662 Average loss: 5.7688\n",
      "====> Epoch: 664 Average loss: 5.8715\n",
      "====> Epoch: 666 Average loss: 5.7858\n",
      "====> Epoch: 668 Average loss: 5.7974\n",
      "====> Epoch: 670 Average loss: 5.8208\n",
      "====> Epoch: 672 Average loss: 5.7830\n",
      "====> Epoch: 674 Average loss: 5.8782\n",
      "====> Epoch: 676 Average loss: 5.7997\n",
      "====> Epoch: 678 Average loss: 5.8743\n",
      "====> Epoch: 680 Average loss: 5.7396\n",
      "====> Epoch: 682 Average loss: 5.7867\n",
      "====> Epoch: 684 Average loss: 5.7847\n",
      "====> Epoch: 686 Average loss: 5.7314\n",
      "====> Epoch: 688 Average loss: 5.7455\n",
      "====> Epoch: 690 Average loss: 5.7522\n",
      "====> Epoch: 692 Average loss: 5.8016\n",
      "====> Epoch: 694 Average loss: 5.7250\n",
      "====> Epoch: 696 Average loss: 5.7857\n",
      "====> Epoch: 698 Average loss: 5.7305\n",
      "====> Epoch: 700 Average loss: 5.8017\n",
      "====> Epoch: 702 Average loss: 5.7876\n",
      "====> Epoch: 704 Average loss: 5.7496\n",
      "====> Epoch: 706 Average loss: 5.6781\n",
      "====> Epoch: 708 Average loss: 5.8570\n",
      "====> Epoch: 710 Average loss: 5.8375\n",
      "====> Epoch: 712 Average loss: 5.7748\n",
      "====> Epoch: 714 Average loss: 5.7316\n",
      "====> Epoch: 716 Average loss: 5.7956\n",
      "====> Epoch: 718 Average loss: 5.6925\n",
      "====> Epoch: 720 Average loss: 5.6888\n",
      "====> Epoch: 722 Average loss: 5.7442\n",
      "====> Epoch: 724 Average loss: 5.7844\n",
      "====> Epoch: 726 Average loss: 5.7660\n",
      "====> Epoch: 728 Average loss: 5.7984\n",
      "====> Epoch: 730 Average loss: 5.8112\n",
      "====> Epoch: 732 Average loss: 5.8035\n",
      "====> Epoch: 734 Average loss: 5.7556\n",
      "====> Epoch: 736 Average loss: 5.7071\n",
      "====> Epoch: 738 Average loss: 5.7711\n",
      "====> Epoch: 740 Average loss: 5.7701\n",
      "====> Epoch: 742 Average loss: 5.7568\n",
      "====> Epoch: 744 Average loss: 5.7656\n",
      "====> Epoch: 746 Average loss: 5.7006\n",
      "====> Epoch: 748 Average loss: 5.7923\n",
      "====> Epoch: 750 Average loss: 5.7313\n",
      "====> Epoch: 752 Average loss: 5.7588\n",
      "====> Epoch: 754 Average loss: 5.7999\n",
      "====> Epoch: 756 Average loss: 5.6490\n",
      "====> Epoch: 758 Average loss: 5.7256\n",
      "====> Epoch: 760 Average loss: 5.7294\n",
      "====> Epoch: 762 Average loss: 5.7163\n",
      "====> Epoch: 764 Average loss: 5.7289\n",
      "====> Epoch: 766 Average loss: 5.7400\n",
      "====> Epoch: 768 Average loss: 5.7272\n",
      "====> Epoch: 770 Average loss: 5.8815\n",
      "====> Epoch: 772 Average loss: 5.7217\n",
      "====> Epoch: 774 Average loss: 5.7718\n",
      "====> Epoch: 776 Average loss: 5.7548\n",
      "====> Epoch: 778 Average loss: 5.6791\n",
      "====> Epoch: 780 Average loss: 5.7408\n",
      "====> Epoch: 782 Average loss: 5.7464\n",
      "====> Epoch: 784 Average loss: 5.8165\n",
      "====> Epoch: 786 Average loss: 5.7439\n",
      "====> Epoch: 788 Average loss: 5.7161\n",
      "====> Epoch: 790 Average loss: 5.7470\n",
      "====> Epoch: 792 Average loss: 5.7847\n",
      "====> Epoch: 794 Average loss: 5.7417\n",
      "====> Epoch: 796 Average loss: 5.7658\n",
      "====> Epoch: 798 Average loss: 5.8024\n",
      "====> Epoch: 800 Average loss: 5.7530\n",
      "====> Epoch: 802 Average loss: 5.7882\n",
      "====> Epoch: 804 Average loss: 5.7179\n",
      "====> Epoch: 806 Average loss: 5.7151\n",
      "====> Epoch: 808 Average loss: 5.7729\n",
      "====> Epoch: 810 Average loss: 5.7138\n",
      "====> Epoch: 812 Average loss: 5.7822\n",
      "====> Epoch: 814 Average loss: 5.7725\n",
      "====> Epoch: 816 Average loss: 5.7397\n",
      "====> Epoch: 818 Average loss: 5.7114\n",
      "====> Epoch: 820 Average loss: 5.6700\n",
      "====> Epoch: 822 Average loss: 5.6958\n",
      "====> Epoch: 824 Average loss: 5.7511\n",
      "====> Epoch: 826 Average loss: 5.7037\n",
      "====> Epoch: 828 Average loss: 5.7603\n",
      "====> Epoch: 830 Average loss: 5.7945\n",
      "====> Epoch: 832 Average loss: 5.7062\n",
      "====> Epoch: 834 Average loss: 5.6793\n",
      "====> Epoch: 836 Average loss: 5.6660\n",
      "====> Epoch: 838 Average loss: 5.7905\n",
      "====> Epoch: 840 Average loss: 5.7679\n",
      "====> Epoch: 842 Average loss: 5.6612\n",
      "====> Epoch: 844 Average loss: 5.7797\n",
      "====> Epoch: 846 Average loss: 5.6616\n",
      "====> Epoch: 848 Average loss: 5.7460\n",
      "====> Epoch: 850 Average loss: 5.7146\n",
      "====> Epoch: 852 Average loss: 5.7389\n",
      "====> Epoch: 854 Average loss: 5.7731\n",
      "====> Epoch: 856 Average loss: 5.7874\n",
      "====> Epoch: 858 Average loss: 5.7766\n",
      "====> Epoch: 860 Average loss: 5.6013\n",
      "====> Epoch: 862 Average loss: 5.8012\n",
      "====> Epoch: 864 Average loss: 5.7907\n",
      "====> Epoch: 866 Average loss: 5.8183\n",
      "====> Epoch: 868 Average loss: 5.7617\n",
      "====> Epoch: 870 Average loss: 5.7251\n",
      "====> Epoch: 872 Average loss: 5.8115\n",
      "====> Epoch: 874 Average loss: 5.7633\n",
      "====> Epoch: 876 Average loss: 5.6814\n",
      "====> Epoch: 878 Average loss: 5.7301\n",
      "====> Epoch: 880 Average loss: 5.6919\n",
      "====> Epoch: 882 Average loss: 5.6978\n",
      "====> Epoch: 884 Average loss: 5.6757\n",
      "====> Epoch: 886 Average loss: 5.6843\n",
      "====> Epoch: 888 Average loss: 5.7641\n",
      "====> Epoch: 890 Average loss: 5.6984\n",
      "====> Epoch: 892 Average loss: 5.7156\n",
      "====> Epoch: 894 Average loss: 5.7326\n",
      "====> Epoch: 896 Average loss: 5.7252\n",
      "====> Epoch: 898 Average loss: 5.6544\n",
      "====> Epoch: 900 Average loss: 5.6964\n",
      "====> Epoch: 902 Average loss: 5.6898\n",
      "====> Epoch: 904 Average loss: 5.7263\n",
      "====> Epoch: 906 Average loss: 5.6940\n",
      "====> Epoch: 908 Average loss: 5.7139\n",
      "====> Epoch: 910 Average loss: 5.7169\n",
      "====> Epoch: 912 Average loss: 5.7893\n",
      "====> Epoch: 914 Average loss: 5.6777\n",
      "====> Epoch: 916 Average loss: 5.7198\n",
      "====> Epoch: 918 Average loss: 5.7654\n",
      "====> Epoch: 920 Average loss: 5.7240\n",
      "====> Epoch: 922 Average loss: 5.7553\n",
      "====> Epoch: 924 Average loss: 5.7567\n",
      "====> Epoch: 926 Average loss: 5.7460\n",
      "====> Epoch: 928 Average loss: 5.7208\n",
      "====> Epoch: 930 Average loss: 5.7324\n",
      "====> Epoch: 932 Average loss: 5.7299\n",
      "====> Epoch: 934 Average loss: 5.8264\n",
      "====> Epoch: 936 Average loss: 5.7535\n",
      "====> Epoch: 938 Average loss: 5.7374\n",
      "====> Epoch: 940 Average loss: 5.6365\n",
      "====> Epoch: 942 Average loss: 5.6778\n",
      "====> Epoch: 944 Average loss: 5.7002\n",
      "====> Epoch: 946 Average loss: 5.7109\n",
      "====> Epoch: 948 Average loss: 5.7538\n",
      "====> Epoch: 950 Average loss: 5.6816\n",
      "====> Epoch: 952 Average loss: 5.7720\n",
      "====> Epoch: 954 Average loss: 5.7457\n",
      "====> Epoch: 956 Average loss: 5.7217\n",
      "====> Epoch: 958 Average loss: 5.7519\n",
      "====> Epoch: 960 Average loss: 5.7331\n",
      "====> Epoch: 962 Average loss: 5.6742\n",
      "====> Epoch: 964 Average loss: 5.7753\n",
      "====> Epoch: 966 Average loss: 5.7511\n",
      "====> Epoch: 968 Average loss: 5.7007\n",
      "====> Epoch: 970 Average loss: 5.6942\n",
      "====> Epoch: 972 Average loss: 5.6922\n",
      "====> Epoch: 974 Average loss: 5.6500\n",
      "====> Epoch: 976 Average loss: 5.8051\n",
      "====> Epoch: 978 Average loss: 5.6897\n",
      "====> Epoch: 980 Average loss: 5.7332\n",
      "====> Epoch: 982 Average loss: 5.7007\n",
      "====> Epoch: 984 Average loss: 5.6651\n",
      "====> Epoch: 986 Average loss: 5.6184\n",
      "====> Epoch: 988 Average loss: 5.6313\n",
      "====> Epoch: 990 Average loss: 5.7117\n",
      "====> Epoch: 992 Average loss: 5.7600\n",
      "====> Epoch: 994 Average loss: 5.7206\n",
      "====> Epoch: 996 Average loss: 5.7068\n",
      "====> Epoch: 998 Average loss: 5.6692\n",
      "====> Epoch: 1000 Average loss: 5.6663\n"
     ]
    }
   ],
   "source": [
    "model, standardizer = train_encoder('Data/diabetes.csv', load_diabetes_data, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_for_diabetes_ds(model, path=\"latent_data/diabetes_latent.csv\"):\n",
    "    DATA_PATH = \"Data/diabetes.csv\"\n",
    "    df = load_diabetes_data(DATA_PATH, sep=\",\")\n",
    "    actual_data = df[0]\n",
    "    outcomes = df[2]\n",
    "    outcomes_numeric = [1 if outcome == \"b'tested_positive'\" else 0 for outcome in outcomes]\n",
    "\n",
    "    latents = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, e in enumerate(actual_data):\n",
    "            sample = e.unsqueeze(0)  # Add batch dimension\n",
    "            latent = model.embed(sample)  # Get the latent representation\n",
    "            latents.append(latent.squeeze().cpu().numpy())\n",
    "\n",
    "    latents_df = pd.DataFrame(latents)\n",
    "    outcomes_df = pd.DataFrame(outcomes_numeric)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_for_diabetes_ds(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_of_latent(model, npy_x_data_file_path, npy_y_data_file_path, dataset):\n",
    "    generated_latent_x = np.load(npy_x_data_file_path)\n",
    "    generaated_latent_y = np.load(npy_y_data_file_path)\n",
    "    generated_torch_data = torch.from_numpy(generated_latent_x).float()\n",
    "\n",
    "    z = model.decode(generated_torch_data)\n",
    "\n",
    "    # generated_data_x = standardizer.inverse_transform(z.cpu().detach().numpy())\n",
    "    generated_data_x = inverse_transform_custom(z.detach().numpy(), dataset.standardizer , dataset.continuous_numerical_cols, dataset.binary_categorical_cols)\n",
    "    return generated_data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real = load_diabetes_data(\"Data/diabetes.csv\")\n",
    "x_real = x_real[1].inverse_transform(x_real[0].cpu().detach().numpy())\n",
    "x_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_syn = reconstruction_of_latent(model, 'X_num_train.npy', 'y_diabetes_train.npy', standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorical_similarity(col_real, col_synthetic):\n",
    "    # Compute Theil's U for categorical features\n",
    "    p_real = pd.Series(col_real).value_counts(normalize=True)\n",
    "    p_synthetic = pd.Series(col_synthetic).value_counts(normalize=True)\n",
    "    u = (p_real * np.log(p_real / p_synthetic)).sum()\n",
    "    return 1 - u\n",
    "def column_similarity(real_data, synthetic_data):\n",
    "    similarities = []\n",
    "    for col_real, col_synthetic in zip(real_data, synthetic_data):\n",
    "        correlation, _ = pearsonr(col_real, col_synthetic)\n",
    "        similarity = correlation\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "def correlation_similarity(real_data, synthetic_data):\n",
    "    real_corr = np.corrcoef(real_data, rowvar=False)\n",
    "    synthetic_corr = np.corrcoef(synthetic_data, rowvar=False)\n",
    "    correlation, _ = pearsonr(real_corr.flatten(), synthetic_corr.flatten())\n",
    "    return correlation\n",
    "def jensen_shannon_similarity(real_data, synthetic_data):\n",
    "    similarities = []\n",
    "    for col_real, col_synthetic in zip(real_data.T, synthetic_data.T):\n",
    "        # Compute probability distributions and Jensen-Shannon divergence\n",
    "        p_real = np.histogram(col_real, bins=10, density=True)[0]\n",
    "        p_synthetic = np.histogram(col_synthetic, bins=10, density=True)[0]\n",
    "        similarity = 1 - jensenshannon(p_real, p_synthetic)\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "def kolmogorov_smirnov_similarity(real_data, synthetic_data):\n",
    "    similarities = []\n",
    "    for col_real, col_synthetic in zip(real_data.T, synthetic_data.T):\n",
    "        # Compute cumulative distributions and Kolmogorov-Smirnov distance\n",
    "        similarity, _ = ks_2samp(col_real, col_synthetic)\n",
    "        similarity = 1 - similarity\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "def propensity_mean_absolute_similarity(real_data, synthetic_data):\n",
    "    # Train XGBoost classifier to discriminate between real and synthetic samples\n",
    "    X = np.vstack([real_data, synthetic_data])\n",
    "    y = np.concatenate([np.ones(len(real_data)), np.zeros(len(synthetic_data))])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    classifier = XGBClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Compute mean absolute error of classifier probabilities\n",
    "    y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    error = mean_absolute_error(y_test, y_pred_proba)\n",
    "    return 1 - error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resemblance_measure(real_data, synthetic_data):\n",
    "    resemblance_score = (\n",
    "        column_similarity(real_data, synthetic_data) +\n",
    "        correlation_similarity(real_data, synthetic_data) +\n",
    "        jensen_shannon_similarity(real_data, synthetic_data) +\n",
    "        kolmogorov_smirnov_similarity(real_data, synthetic_data) +\n",
    "        propensity_mean_absolute_similarity(real_data, synthetic_data)\n",
    "    ) / 5\n",
    "    print(\"Resemblance Score:\", resemblance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10504327449016271"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data = np.random.randn(100, 5)\n",
    "synthetic_data = np.random.randn(100, 5)\n",
    "propensity_mean_absolute_similarity(synthetic_data, synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7588793579549271\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_syn[:768], x_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cardio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 9.8872\n",
      "====> Epoch: 4 Average loss: 9.6558\n",
      "====> Epoch: 6 Average loss: 9.5732\n",
      "====> Epoch: 8 Average loss: 9.4705\n",
      "====> Epoch: 10 Average loss: 9.4279\n",
      "====> Epoch: 12 Average loss: 9.3978\n",
      "====> Epoch: 14 Average loss: 9.3661\n",
      "====> Epoch: 16 Average loss: 9.3469\n",
      "====> Epoch: 18 Average loss: 9.3409\n",
      "====> Epoch: 20 Average loss: 9.3224\n",
      "====> Epoch: 22 Average loss: 9.3005\n",
      "====> Epoch: 24 Average loss: 9.2791\n",
      "====> Epoch: 26 Average loss: 9.2800\n",
      "====> Epoch: 28 Average loss: 9.2714\n",
      "====> Epoch: 30 Average loss: 9.2637\n",
      "====> Epoch: 32 Average loss: 9.2540\n",
      "====> Epoch: 34 Average loss: 9.2464\n",
      "====> Epoch: 36 Average loss: 9.2477\n",
      "====> Epoch: 38 Average loss: 9.2233\n",
      "====> Epoch: 40 Average loss: 9.2268\n",
      "====> Epoch: 42 Average loss: 9.2185\n",
      "====> Epoch: 44 Average loss: 9.2141\n",
      "====> Epoch: 46 Average loss: 9.2151\n",
      "====> Epoch: 48 Average loss: 9.2138\n",
      "====> Epoch: 50 Average loss: 9.2058\n",
      "====> Epoch: 52 Average loss: 9.1916\n",
      "====> Epoch: 54 Average loss: 9.1927\n",
      "====> Epoch: 56 Average loss: 9.1916\n",
      "====> Epoch: 58 Average loss: 9.2004\n",
      "====> Epoch: 60 Average loss: 9.1845\n",
      "====> Epoch: 62 Average loss: 9.1757\n",
      "====> Epoch: 64 Average loss: 9.1734\n",
      "====> Epoch: 66 Average loss: 9.1695\n",
      "====> Epoch: 68 Average loss: 9.1796\n",
      "====> Epoch: 70 Average loss: 9.1648\n",
      "====> Epoch: 72 Average loss: 9.1730\n",
      "====> Epoch: 74 Average loss: 9.1710\n",
      "====> Epoch: 76 Average loss: 9.1635\n",
      "====> Epoch: 78 Average loss: 9.1540\n",
      "====> Epoch: 80 Average loss: 9.1527\n",
      "====> Epoch: 82 Average loss: 9.1537\n",
      "====> Epoch: 84 Average loss: 9.1543\n",
      "====> Epoch: 86 Average loss: 9.1468\n",
      "====> Epoch: 88 Average loss: 9.1508\n",
      "====> Epoch: 90 Average loss: 9.1432\n",
      "====> Epoch: 92 Average loss: 9.1460\n",
      "====> Epoch: 94 Average loss: 9.1468\n",
      "====> Epoch: 96 Average loss: 9.1407\n",
      "====> Epoch: 98 Average loss: 9.1363\n",
      "====> Epoch: 100 Average loss: 9.1406\n",
      "====> Epoch: 102 Average loss: 9.1384\n",
      "====> Epoch: 104 Average loss: 9.1314\n",
      "====> Epoch: 106 Average loss: 9.1359\n",
      "====> Epoch: 108 Average loss: 9.1270\n",
      "====> Epoch: 110 Average loss: 9.1329\n",
      "====> Epoch: 112 Average loss: 9.1311\n",
      "====> Epoch: 114 Average loss: 9.1359\n",
      "====> Epoch: 116 Average loss: 9.1259\n",
      "====> Epoch: 118 Average loss: 9.1256\n",
      "====> Epoch: 120 Average loss: 9.1257\n",
      "====> Epoch: 122 Average loss: 9.1314\n",
      "====> Epoch: 124 Average loss: 9.1286\n",
      "====> Epoch: 126 Average loss: 9.1179\n",
      "====> Epoch: 128 Average loss: 9.1294\n",
      "====> Epoch: 130 Average loss: 9.1141\n",
      "====> Epoch: 132 Average loss: 9.1122\n",
      "====> Epoch: 134 Average loss: 9.1187\n",
      "====> Epoch: 136 Average loss: 9.1074\n",
      "====> Epoch: 138 Average loss: 9.1176\n",
      "====> Epoch: 140 Average loss: 9.1179\n",
      "====> Epoch: 142 Average loss: 9.1213\n",
      "====> Epoch: 144 Average loss: 9.1103\n",
      "====> Epoch: 146 Average loss: 9.1269\n",
      "====> Epoch: 148 Average loss: 9.1165\n",
      "====> Epoch: 150 Average loss: 9.1154\n",
      "====> Epoch: 152 Average loss: 9.1065\n",
      "====> Epoch: 154 Average loss: 9.1152\n",
      "====> Epoch: 156 Average loss: 9.1115\n",
      "====> Epoch: 158 Average loss: 9.1228\n",
      "====> Epoch: 160 Average loss: 9.1062\n",
      "====> Epoch: 162 Average loss: 9.1099\n",
      "====> Epoch: 164 Average loss: 9.1030\n",
      "====> Epoch: 166 Average loss: 9.1106\n",
      "====> Epoch: 168 Average loss: 9.1181\n",
      "====> Epoch: 170 Average loss: 9.1156\n",
      "====> Epoch: 172 Average loss: 9.1138\n",
      "====> Epoch: 174 Average loss: 9.1158\n",
      "====> Epoch: 176 Average loss: 9.1126\n",
      "====> Epoch: 178 Average loss: 9.0951\n",
      "====> Epoch: 180 Average loss: 9.1114\n",
      "====> Epoch: 182 Average loss: 9.1137\n",
      "====> Epoch: 184 Average loss: 9.1079\n",
      "====> Epoch: 186 Average loss: 9.0964\n",
      "====> Epoch: 188 Average loss: 9.1074\n",
      "====> Epoch: 190 Average loss: 9.1073\n",
      "====> Epoch: 192 Average loss: 9.1126\n",
      "====> Epoch: 194 Average loss: 9.1159\n",
      "====> Epoch: 196 Average loss: 9.1060\n",
      "====> Epoch: 198 Average loss: 9.0949\n",
      "====> Epoch: 200 Average loss: 9.1050\n",
      "====> Epoch: 202 Average loss: 9.1042\n",
      "====> Epoch: 204 Average loss: 9.0982\n",
      "====> Epoch: 206 Average loss: 9.0969\n",
      "====> Epoch: 208 Average loss: 9.0960\n",
      "====> Epoch: 210 Average loss: 9.0811\n",
      "====> Epoch: 212 Average loss: 9.0910\n",
      "====> Epoch: 214 Average loss: 9.0889\n",
      "====> Epoch: 216 Average loss: 9.0979\n",
      "====> Epoch: 218 Average loss: 9.0955\n",
      "====> Epoch: 220 Average loss: 9.0972\n",
      "====> Epoch: 222 Average loss: 9.0945\n",
      "====> Epoch: 224 Average loss: 9.0957\n",
      "====> Epoch: 226 Average loss: 9.0950\n",
      "====> Epoch: 228 Average loss: 9.0910\n",
      "====> Epoch: 230 Average loss: 9.0904\n",
      "====> Epoch: 232 Average loss: 9.0906\n",
      "====> Epoch: 234 Average loss: 9.0880\n",
      "====> Epoch: 236 Average loss: 9.0856\n",
      "====> Epoch: 238 Average loss: 9.0868\n",
      "====> Epoch: 240 Average loss: 9.0856\n",
      "====> Epoch: 242 Average loss: 9.0907\n",
      "====> Epoch: 244 Average loss: 9.0813\n",
      "====> Epoch: 246 Average loss: 9.0874\n",
      "====> Epoch: 248 Average loss: 9.0782\n",
      "====> Epoch: 250 Average loss: 9.0802\n",
      "====> Epoch: 252 Average loss: 9.0737\n",
      "====> Epoch: 254 Average loss: 9.0779\n",
      "====> Epoch: 256 Average loss: 9.0817\n",
      "====> Epoch: 258 Average loss: 9.0843\n",
      "====> Epoch: 260 Average loss: 9.0850\n",
      "====> Epoch: 262 Average loss: 9.0759\n",
      "====> Epoch: 264 Average loss: 9.0801\n",
      "====> Epoch: 266 Average loss: 9.0762\n",
      "====> Epoch: 268 Average loss: 9.0822\n",
      "====> Epoch: 270 Average loss: 9.0811\n",
      "====> Epoch: 272 Average loss: 9.0796\n",
      "====> Epoch: 274 Average loss: 9.0829\n",
      "====> Epoch: 276 Average loss: 9.0776\n",
      "====> Epoch: 278 Average loss: 9.0964\n",
      "====> Epoch: 280 Average loss: 9.0846\n",
      "====> Epoch: 282 Average loss: 9.0778\n",
      "====> Epoch: 284 Average loss: 9.0766\n",
      "====> Epoch: 286 Average loss: 9.0745\n",
      "====> Epoch: 288 Average loss: 9.0707\n",
      "====> Epoch: 290 Average loss: 9.0779\n",
      "====> Epoch: 292 Average loss: 9.0879\n",
      "====> Epoch: 294 Average loss: 9.0828\n",
      "====> Epoch: 296 Average loss: 9.0712\n",
      "====> Epoch: 298 Average loss: 9.0713\n",
      "====> Epoch: 300 Average loss: 9.0782\n",
      "====> Epoch: 302 Average loss: 9.0698\n",
      "====> Epoch: 304 Average loss: 9.0757\n",
      "====> Epoch: 306 Average loss: 9.0849\n",
      "====> Epoch: 308 Average loss: 9.0780\n",
      "====> Epoch: 310 Average loss: 9.0610\n",
      "====> Epoch: 312 Average loss: 9.0797\n",
      "====> Epoch: 314 Average loss: 9.0801\n",
      "====> Epoch: 316 Average loss: 9.0632\n",
      "====> Epoch: 318 Average loss: 9.0766\n",
      "====> Epoch: 320 Average loss: 9.0640\n",
      "====> Epoch: 322 Average loss: 9.0759\n",
      "====> Epoch: 324 Average loss: 9.0706\n",
      "====> Epoch: 326 Average loss: 9.0725\n",
      "====> Epoch: 328 Average loss: 9.0660\n",
      "====> Epoch: 330 Average loss: 9.0644\n",
      "====> Epoch: 332 Average loss: 9.0664\n",
      "====> Epoch: 334 Average loss: 9.0642\n",
      "====> Epoch: 336 Average loss: 9.0621\n",
      "====> Epoch: 338 Average loss: 9.0579\n",
      "====> Epoch: 340 Average loss: 9.0642\n",
      "====> Epoch: 342 Average loss: 9.0724\n",
      "====> Epoch: 344 Average loss: 9.0723\n",
      "====> Epoch: 346 Average loss: 9.0682\n",
      "====> Epoch: 348 Average loss: 9.0770\n",
      "====> Epoch: 350 Average loss: 9.0626\n",
      "====> Epoch: 352 Average loss: 9.0562\n",
      "====> Epoch: 354 Average loss: 9.0680\n",
      "====> Epoch: 356 Average loss: 9.0732\n",
      "====> Epoch: 358 Average loss: 9.0603\n",
      "====> Epoch: 360 Average loss: 9.0694\n",
      "====> Epoch: 362 Average loss: 9.0608\n",
      "====> Epoch: 364 Average loss: 9.0755\n",
      "====> Epoch: 366 Average loss: 9.0569\n",
      "====> Epoch: 368 Average loss: 9.0617\n",
      "====> Epoch: 370 Average loss: 9.0583\n",
      "====> Epoch: 372 Average loss: 9.0615\n",
      "====> Epoch: 374 Average loss: 9.0620\n",
      "====> Epoch: 376 Average loss: 9.0657\n",
      "====> Epoch: 378 Average loss: 9.0687\n",
      "====> Epoch: 380 Average loss: 9.0705\n",
      "====> Epoch: 382 Average loss: 9.0654\n",
      "====> Epoch: 384 Average loss: 9.0598\n",
      "====> Epoch: 386 Average loss: 9.0554\n",
      "====> Epoch: 388 Average loss: 9.0629\n",
      "====> Epoch: 390 Average loss: 9.0637\n",
      "====> Epoch: 392 Average loss: 9.0579\n",
      "====> Epoch: 394 Average loss: 9.0573\n",
      "====> Epoch: 396 Average loss: 9.0568\n",
      "====> Epoch: 398 Average loss: 9.0641\n",
      "====> Epoch: 400 Average loss: 9.0700\n",
      "====> Epoch: 402 Average loss: 9.0540\n",
      "====> Epoch: 404 Average loss: 9.0652\n",
      "====> Epoch: 406 Average loss: 9.0554\n",
      "====> Epoch: 408 Average loss: 9.0611\n",
      "====> Epoch: 410 Average loss: 9.0500\n",
      "====> Epoch: 412 Average loss: 9.0589\n",
      "====> Epoch: 414 Average loss: 9.0592\n",
      "====> Epoch: 416 Average loss: 9.0574\n",
      "====> Epoch: 418 Average loss: 9.0569\n",
      "====> Epoch: 420 Average loss: 9.0542\n",
      "====> Epoch: 422 Average loss: 9.0568\n",
      "====> Epoch: 424 Average loss: 9.0508\n",
      "====> Epoch: 426 Average loss: 9.0622\n",
      "====> Epoch: 428 Average loss: 9.0619\n",
      "====> Epoch: 430 Average loss: 9.0510\n",
      "====> Epoch: 432 Average loss: 9.0613\n",
      "====> Epoch: 434 Average loss: 9.0499\n",
      "====> Epoch: 436 Average loss: 9.0517\n",
      "====> Epoch: 438 Average loss: 9.0524\n",
      "====> Epoch: 440 Average loss: 9.0572\n",
      "====> Epoch: 442 Average loss: 9.0587\n",
      "====> Epoch: 444 Average loss: 9.0617\n",
      "====> Epoch: 446 Average loss: 9.0462\n",
      "====> Epoch: 448 Average loss: 9.0579\n",
      "====> Epoch: 450 Average loss: 9.0518\n",
      "====> Epoch: 452 Average loss: 9.0507\n",
      "====> Epoch: 454 Average loss: 9.0603\n",
      "====> Epoch: 456 Average loss: 9.0570\n",
      "====> Epoch: 458 Average loss: 9.0520\n",
      "====> Epoch: 460 Average loss: 9.0499\n",
      "====> Epoch: 462 Average loss: 9.0523\n",
      "====> Epoch: 464 Average loss: 9.0520\n",
      "====> Epoch: 466 Average loss: 9.0511\n",
      "====> Epoch: 468 Average loss: 9.0623\n",
      "====> Epoch: 470 Average loss: 9.0566\n",
      "====> Epoch: 472 Average loss: 9.0533\n",
      "====> Epoch: 474 Average loss: 9.0566\n",
      "====> Epoch: 476 Average loss: 9.0624\n",
      "====> Epoch: 478 Average loss: 9.0632\n",
      "====> Epoch: 480 Average loss: 9.0611\n",
      "====> Epoch: 482 Average loss: 9.0524\n",
      "====> Epoch: 484 Average loss: 9.0504\n",
      "====> Epoch: 486 Average loss: 9.0491\n",
      "====> Epoch: 488 Average loss: 9.0572\n",
      "====> Epoch: 490 Average loss: 9.0484\n",
      "====> Epoch: 492 Average loss: 9.0512\n",
      "====> Epoch: 494 Average loss: 9.0663\n",
      "====> Epoch: 496 Average loss: 9.0593\n",
      "====> Epoch: 498 Average loss: 9.0538\n",
      "====> Epoch: 500 Average loss: 9.0543\n",
      "====> Epoch: 502 Average loss: 9.0524\n",
      "====> Epoch: 504 Average loss: 9.0510\n",
      "====> Epoch: 506 Average loss: 9.0534\n",
      "====> Epoch: 508 Average loss: 9.0543\n",
      "====> Epoch: 510 Average loss: 9.0550\n",
      "====> Epoch: 512 Average loss: 9.0427\n",
      "====> Epoch: 514 Average loss: 9.0550\n",
      "====> Epoch: 516 Average loss: 9.0613\n",
      "====> Epoch: 518 Average loss: 9.0559\n",
      "====> Epoch: 520 Average loss: 9.0507\n",
      "====> Epoch: 522 Average loss: 9.0514\n",
      "====> Epoch: 524 Average loss: 9.0511\n",
      "====> Epoch: 526 Average loss: 9.0437\n",
      "====> Epoch: 528 Average loss: 9.0485\n",
      "====> Epoch: 530 Average loss: 9.0529\n",
      "====> Epoch: 532 Average loss: 9.0539\n",
      "====> Epoch: 534 Average loss: 9.0557\n",
      "====> Epoch: 536 Average loss: 9.0574\n",
      "====> Epoch: 538 Average loss: 9.0531\n",
      "====> Epoch: 540 Average loss: 9.0515\n",
      "====> Epoch: 542 Average loss: 9.0542\n",
      "====> Epoch: 544 Average loss: 9.0435\n",
      "====> Epoch: 546 Average loss: 9.0537\n",
      "====> Epoch: 548 Average loss: 9.0512\n",
      "====> Epoch: 550 Average loss: 9.0497\n",
      "====> Epoch: 552 Average loss: 9.0470\n",
      "====> Epoch: 554 Average loss: 9.0516\n",
      "====> Epoch: 556 Average loss: 9.0442\n",
      "====> Epoch: 558 Average loss: 9.0499\n",
      "====> Epoch: 560 Average loss: 9.0507\n",
      "====> Epoch: 562 Average loss: 9.0369\n",
      "====> Epoch: 564 Average loss: 9.0439\n",
      "====> Epoch: 566 Average loss: 9.0430\n",
      "====> Epoch: 568 Average loss: 9.0432\n",
      "====> Epoch: 570 Average loss: 9.0499\n",
      "====> Epoch: 572 Average loss: 9.0496\n",
      "====> Epoch: 574 Average loss: 9.0409\n",
      "====> Epoch: 576 Average loss: 9.0489\n",
      "====> Epoch: 578 Average loss: 9.0414\n",
      "====> Epoch: 580 Average loss: 9.0501\n",
      "====> Epoch: 582 Average loss: 9.0402\n",
      "====> Epoch: 584 Average loss: 9.0537\n",
      "====> Epoch: 586 Average loss: 9.0451\n",
      "====> Epoch: 588 Average loss: 9.0494\n",
      "====> Epoch: 590 Average loss: 9.0459\n",
      "====> Epoch: 592 Average loss: 9.0399\n",
      "====> Epoch: 594 Average loss: 9.0381\n",
      "====> Epoch: 596 Average loss: 9.0384\n",
      "====> Epoch: 598 Average loss: 9.0426\n",
      "====> Epoch: 600 Average loss: 9.0333\n",
      "====> Epoch: 602 Average loss: 9.0403\n",
      "====> Epoch: 604 Average loss: 9.0414\n",
      "====> Epoch: 606 Average loss: 9.0306\n",
      "====> Epoch: 608 Average loss: 9.0446\n",
      "====> Epoch: 610 Average loss: 9.0403\n",
      "====> Epoch: 612 Average loss: 9.0475\n",
      "====> Epoch: 614 Average loss: 9.0413\n",
      "====> Epoch: 616 Average loss: 9.0454\n",
      "====> Epoch: 618 Average loss: 9.0510\n",
      "====> Epoch: 620 Average loss: 9.0476\n",
      "====> Epoch: 622 Average loss: 9.0348\n",
      "====> Epoch: 624 Average loss: 9.0377\n",
      "====> Epoch: 626 Average loss: 9.0503\n",
      "====> Epoch: 628 Average loss: 9.0341\n",
      "====> Epoch: 630 Average loss: 9.0419\n",
      "====> Epoch: 632 Average loss: 9.0374\n",
      "====> Epoch: 634 Average loss: 9.0490\n",
      "====> Epoch: 636 Average loss: 9.0481\n",
      "====> Epoch: 638 Average loss: 9.0415\n",
      "====> Epoch: 640 Average loss: 9.0465\n",
      "====> Epoch: 642 Average loss: 9.0375\n",
      "====> Epoch: 644 Average loss: 9.0414\n",
      "====> Epoch: 646 Average loss: 9.0506\n",
      "====> Epoch: 648 Average loss: 9.0384\n",
      "====> Epoch: 650 Average loss: 9.0371\n",
      "====> Epoch: 652 Average loss: 9.0439\n",
      "====> Epoch: 654 Average loss: 9.0389\n",
      "====> Epoch: 656 Average loss: 9.0400\n",
      "====> Epoch: 658 Average loss: 9.0286\n",
      "====> Epoch: 660 Average loss: 9.0311\n",
      "====> Epoch: 662 Average loss: 9.0369\n",
      "====> Epoch: 664 Average loss: 9.0284\n",
      "====> Epoch: 666 Average loss: 9.0330\n",
      "====> Epoch: 668 Average loss: 9.0427\n",
      "====> Epoch: 670 Average loss: 9.0373\n",
      "====> Epoch: 672 Average loss: 9.0353\n",
      "====> Epoch: 674 Average loss: 9.0351\n",
      "====> Epoch: 676 Average loss: 9.0280\n",
      "====> Epoch: 678 Average loss: 9.0492\n",
      "====> Epoch: 680 Average loss: 9.0314\n",
      "====> Epoch: 682 Average loss: 9.0324\n",
      "====> Epoch: 684 Average loss: 9.0341\n",
      "====> Epoch: 686 Average loss: 9.0335\n",
      "====> Epoch: 688 Average loss: 9.0310\n",
      "====> Epoch: 690 Average loss: 9.0422\n",
      "====> Epoch: 692 Average loss: 9.0300\n",
      "====> Epoch: 694 Average loss: 9.0332\n",
      "====> Epoch: 696 Average loss: 9.0320\n",
      "====> Epoch: 698 Average loss: 9.0237\n",
      "====> Epoch: 700 Average loss: 9.0305\n",
      "====> Epoch: 702 Average loss: 9.0270\n",
      "====> Epoch: 704 Average loss: 9.0364\n",
      "====> Epoch: 706 Average loss: 9.0200\n",
      "====> Epoch: 708 Average loss: 9.0244\n",
      "====> Epoch: 710 Average loss: 9.0212\n",
      "====> Epoch: 712 Average loss: 9.0257\n",
      "====> Epoch: 714 Average loss: 9.0301\n",
      "====> Epoch: 716 Average loss: 9.0215\n",
      "====> Epoch: 718 Average loss: 9.0265\n",
      "====> Epoch: 720 Average loss: 9.0314\n",
      "====> Epoch: 722 Average loss: 9.0287\n",
      "====> Epoch: 724 Average loss: 9.0332\n",
      "====> Epoch: 726 Average loss: 9.0272\n",
      "====> Epoch: 728 Average loss: 9.0194\n",
      "====> Epoch: 730 Average loss: 9.0296\n",
      "====> Epoch: 732 Average loss: 9.0238\n",
      "====> Epoch: 734 Average loss: 9.0338\n",
      "====> Epoch: 736 Average loss: 9.0235\n",
      "====> Epoch: 738 Average loss: 9.0255\n",
      "====> Epoch: 740 Average loss: 9.0177\n",
      "====> Epoch: 742 Average loss: 9.0216\n",
      "====> Epoch: 744 Average loss: 9.0201\n",
      "====> Epoch: 746 Average loss: 9.0112\n",
      "====> Epoch: 748 Average loss: 9.0251\n",
      "====> Epoch: 750 Average loss: 9.0216\n",
      "====> Epoch: 752 Average loss: 9.0241\n",
      "====> Epoch: 754 Average loss: 9.0259\n",
      "====> Epoch: 756 Average loss: 9.0201\n",
      "====> Epoch: 758 Average loss: 9.0160\n",
      "====> Epoch: 760 Average loss: 9.0223\n",
      "====> Epoch: 762 Average loss: 9.0164\n",
      "====> Epoch: 764 Average loss: 9.0199\n",
      "====> Epoch: 766 Average loss: 9.0256\n",
      "====> Epoch: 768 Average loss: 9.0196\n",
      "====> Epoch: 770 Average loss: 9.0087\n",
      "====> Epoch: 772 Average loss: 9.0178\n",
      "====> Epoch: 774 Average loss: 9.0166\n",
      "====> Epoch: 776 Average loss: 9.0085\n",
      "====> Epoch: 778 Average loss: 9.0178\n",
      "====> Epoch: 780 Average loss: 9.0259\n",
      "====> Epoch: 782 Average loss: 9.0125\n",
      "====> Epoch: 784 Average loss: 9.0167\n",
      "====> Epoch: 786 Average loss: 9.0192\n",
      "====> Epoch: 788 Average loss: 9.0215\n",
      "====> Epoch: 790 Average loss: 9.0196\n",
      "====> Epoch: 792 Average loss: 9.0162\n",
      "====> Epoch: 794 Average loss: 9.0165\n",
      "====> Epoch: 796 Average loss: 9.0161\n",
      "====> Epoch: 798 Average loss: 9.0137\n",
      "====> Epoch: 800 Average loss: 9.0202\n",
      "====> Epoch: 802 Average loss: 9.0205\n",
      "====> Epoch: 804 Average loss: 9.0167\n",
      "====> Epoch: 806 Average loss: 9.0152\n",
      "====> Epoch: 808 Average loss: 9.0233\n",
      "====> Epoch: 810 Average loss: 9.0187\n",
      "====> Epoch: 812 Average loss: 9.0149\n",
      "====> Epoch: 814 Average loss: 9.0097\n",
      "====> Epoch: 816 Average loss: 9.0033\n",
      "====> Epoch: 818 Average loss: 9.0070\n",
      "====> Epoch: 820 Average loss: 9.0122\n",
      "====> Epoch: 822 Average loss: 9.0193\n",
      "====> Epoch: 824 Average loss: 9.0092\n",
      "====> Epoch: 826 Average loss: 9.0169\n",
      "====> Epoch: 828 Average loss: 9.0143\n",
      "====> Epoch: 830 Average loss: 9.0048\n",
      "====> Epoch: 832 Average loss: 9.0160\n",
      "====> Epoch: 834 Average loss: 9.0084\n",
      "====> Epoch: 836 Average loss: 9.0115\n",
      "====> Epoch: 838 Average loss: 9.0120\n",
      "====> Epoch: 840 Average loss: 9.0243\n",
      "====> Epoch: 842 Average loss: 9.0165\n",
      "====> Epoch: 844 Average loss: 9.0084\n",
      "====> Epoch: 846 Average loss: 9.0131\n",
      "====> Epoch: 848 Average loss: 9.0153\n",
      "====> Epoch: 850 Average loss: 9.0137\n",
      "====> Epoch: 852 Average loss: 9.0079\n",
      "====> Epoch: 854 Average loss: 9.0151\n",
      "====> Epoch: 856 Average loss: 9.0097\n",
      "====> Epoch: 858 Average loss: 9.0091\n",
      "====> Epoch: 860 Average loss: 9.0191\n",
      "====> Epoch: 862 Average loss: 9.0084\n",
      "====> Epoch: 864 Average loss: 9.0062\n",
      "====> Epoch: 866 Average loss: 9.0133\n",
      "====> Epoch: 868 Average loss: 9.0203\n",
      "====> Epoch: 870 Average loss: 9.0027\n",
      "====> Epoch: 872 Average loss: 8.9952\n",
      "====> Epoch: 874 Average loss: 8.9983\n",
      "====> Epoch: 876 Average loss: 9.0157\n",
      "====> Epoch: 878 Average loss: 9.0114\n",
      "====> Epoch: 880 Average loss: 9.0172\n",
      "====> Epoch: 882 Average loss: 9.0148\n",
      "====> Epoch: 884 Average loss: 9.0036\n",
      "====> Epoch: 886 Average loss: 9.0065\n",
      "====> Epoch: 888 Average loss: 9.0097\n",
      "====> Epoch: 890 Average loss: 9.0083\n",
      "====> Epoch: 892 Average loss: 9.0144\n",
      "====> Epoch: 894 Average loss: 9.0131\n",
      "====> Epoch: 896 Average loss: 9.0066\n",
      "====> Epoch: 898 Average loss: 9.0164\n",
      "====> Epoch: 900 Average loss: 9.0150\n",
      "====> Epoch: 902 Average loss: 9.0052\n",
      "====> Epoch: 904 Average loss: 9.0073\n",
      "====> Epoch: 906 Average loss: 9.0057\n",
      "====> Epoch: 908 Average loss: 8.9988\n",
      "====> Epoch: 910 Average loss: 9.0047\n",
      "====> Epoch: 912 Average loss: 9.0098\n",
      "====> Epoch: 914 Average loss: 8.9970\n",
      "====> Epoch: 916 Average loss: 9.0105\n",
      "====> Epoch: 918 Average loss: 9.0104\n",
      "====> Epoch: 920 Average loss: 9.0037\n",
      "====> Epoch: 922 Average loss: 9.0005\n",
      "====> Epoch: 924 Average loss: 9.0093\n",
      "====> Epoch: 926 Average loss: 9.0114\n",
      "====> Epoch: 928 Average loss: 9.0193\n",
      "====> Epoch: 930 Average loss: 9.0091\n",
      "====> Epoch: 932 Average loss: 9.0079\n",
      "====> Epoch: 934 Average loss: 9.0028\n",
      "====> Epoch: 936 Average loss: 9.0228\n",
      "====> Epoch: 938 Average loss: 9.0106\n",
      "====> Epoch: 940 Average loss: 9.0135\n",
      "====> Epoch: 942 Average loss: 9.0170\n",
      "====> Epoch: 944 Average loss: 9.0128\n",
      "====> Epoch: 946 Average loss: 9.0207\n",
      "====> Epoch: 948 Average loss: 9.0021\n",
      "====> Epoch: 950 Average loss: 8.9969\n",
      "====> Epoch: 952 Average loss: 8.9948\n",
      "====> Epoch: 954 Average loss: 9.0058\n",
      "====> Epoch: 956 Average loss: 9.0072\n",
      "====> Epoch: 958 Average loss: 9.0155\n",
      "====> Epoch: 960 Average loss: 9.0075\n",
      "====> Epoch: 962 Average loss: 9.0144\n",
      "====> Epoch: 964 Average loss: 9.0114\n",
      "====> Epoch: 966 Average loss: 9.0028\n",
      "====> Epoch: 968 Average loss: 9.0054\n",
      "====> Epoch: 970 Average loss: 9.0087\n",
      "====> Epoch: 972 Average loss: 9.0016\n",
      "====> Epoch: 974 Average loss: 8.9947\n",
      "====> Epoch: 976 Average loss: 9.0040\n",
      "====> Epoch: 978 Average loss: 9.0023\n",
      "====> Epoch: 980 Average loss: 9.0077\n",
      "====> Epoch: 982 Average loss: 9.0049\n",
      "====> Epoch: 984 Average loss: 9.0004\n",
      "====> Epoch: 986 Average loss: 9.0176\n",
      "====> Epoch: 988 Average loss: 9.0094\n",
      "====> Epoch: 990 Average loss: 8.9873\n",
      "====> Epoch: 992 Average loss: 9.0166\n",
      "====> Epoch: 994 Average loss: 8.9940\n",
      "====> Epoch: 996 Average loss: 9.0028\n",
      "====> Epoch: 998 Average loss: 8.9971\n",
      "====> Epoch: 1000 Average loss: 8.9971\n"
     ]
    }
   ],
   "source": [
    "model_cardio, standardizer_cardio = train_encoder('Data/cardio_train.csv', load_diabetes_data, \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_for_cardio_ds(model):\n",
    "    DATA_PATH = \"Data/cardio_train.csv\"\n",
    "    df = load_diabetes_data(DATA_PATH, sep=\";\")\n",
    "    actual_data = df[0]\n",
    "    outcomes = df[2]\n",
    "\n",
    "    latents = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, e in enumerate(actual_data):\n",
    "            sample = e.unsqueeze(0)  # Add batch dimension\n",
    "            latent = model.embed(sample)  # Get the latent representation\n",
    "            latents.append(latent.squeeze().cpu().numpy())\n",
    "\n",
    "    latents_df = pd.DataFrame(latents)\n",
    "    outcomes_df = pd.DataFrame(outcomes)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv('latent_data/cardio_latent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_for_cardio_ds(model_cardio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cardio_syn = reconstruction_of_latent(model_cardio, 'syn_latent/cardio_synthetic/X_num_unnorm.npy', 'syn_latent/cardio_synthetic/y_train.npy', standardizer_cardio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 12)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cardio_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 12)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cardio_real = load_diabetes_data(\"Data/cardio_train.csv\", \";\")\n",
    "x_cardio_real = x_cardio_real[1].inverse_transform(x_cardio_real[0].cpu().detach().numpy())\n",
    "x_cardio_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7650878484389891\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_cardio_syn, x_cardio_real[:9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Medium Level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 18.2724\n",
      "====> Epoch: 4 Average loss: 14.4822\n",
      "====> Epoch: 6 Average loss: 12.8783\n",
      "====> Epoch: 8 Average loss: 12.2937\n",
      "====> Epoch: 10 Average loss: 11.9030\n",
      "====> Epoch: 12 Average loss: 11.7320\n",
      "====> Epoch: 14 Average loss: 11.5080\n",
      "====> Epoch: 16 Average loss: 11.3381\n",
      "====> Epoch: 18 Average loss: 11.2089\n",
      "====> Epoch: 20 Average loss: 11.1627\n",
      "====> Epoch: 22 Average loss: 11.0722\n",
      "====> Epoch: 24 Average loss: 11.0496\n",
      "====> Epoch: 26 Average loss: 10.9655\n",
      "====> Epoch: 28 Average loss: 10.9049\n",
      "====> Epoch: 30 Average loss: 10.8614\n",
      "====> Epoch: 32 Average loss: 10.7989\n",
      "====> Epoch: 34 Average loss: 10.7828\n",
      "====> Epoch: 36 Average loss: 10.7792\n",
      "====> Epoch: 38 Average loss: 10.7873\n",
      "====> Epoch: 40 Average loss: 10.6894\n",
      "====> Epoch: 42 Average loss: 10.6745\n",
      "====> Epoch: 44 Average loss: 10.6588\n",
      "====> Epoch: 46 Average loss: 10.6109\n",
      "====> Epoch: 48 Average loss: 10.6120\n",
      "====> Epoch: 50 Average loss: 10.6297\n",
      "====> Epoch: 52 Average loss: 10.6019\n",
      "====> Epoch: 54 Average loss: 10.5862\n",
      "====> Epoch: 56 Average loss: 10.5884\n",
      "====> Epoch: 58 Average loss: 10.5513\n",
      "====> Epoch: 60 Average loss: 10.5472\n",
      "====> Epoch: 62 Average loss: 10.5122\n",
      "====> Epoch: 64 Average loss: 10.5320\n",
      "====> Epoch: 66 Average loss: 10.4930\n",
      "====> Epoch: 68 Average loss: 10.4784\n",
      "====> Epoch: 70 Average loss: 10.4554\n",
      "====> Epoch: 72 Average loss: 10.4956\n",
      "====> Epoch: 74 Average loss: 10.4964\n",
      "====> Epoch: 76 Average loss: 10.4473\n",
      "====> Epoch: 78 Average loss: 10.4761\n",
      "====> Epoch: 80 Average loss: 10.4103\n",
      "====> Epoch: 82 Average loss: 10.4463\n",
      "====> Epoch: 84 Average loss: 10.4260\n",
      "====> Epoch: 86 Average loss: 10.4583\n",
      "====> Epoch: 88 Average loss: 10.4451\n",
      "====> Epoch: 90 Average loss: 10.4033\n",
      "====> Epoch: 92 Average loss: 10.4168\n",
      "====> Epoch: 94 Average loss: 10.4337\n",
      "====> Epoch: 96 Average loss: 10.4256\n",
      "====> Epoch: 98 Average loss: 10.3873\n",
      "====> Epoch: 100 Average loss: 10.4283\n",
      "====> Epoch: 102 Average loss: 10.3847\n",
      "====> Epoch: 104 Average loss: 10.3665\n",
      "====> Epoch: 106 Average loss: 10.3965\n",
      "====> Epoch: 108 Average loss: 10.3899\n",
      "====> Epoch: 110 Average loss: 10.4026\n",
      "====> Epoch: 112 Average loss: 10.3487\n",
      "====> Epoch: 114 Average loss: 10.3551\n",
      "====> Epoch: 116 Average loss: 10.3723\n",
      "====> Epoch: 118 Average loss: 10.3450\n",
      "====> Epoch: 120 Average loss: 10.3315\n",
      "====> Epoch: 122 Average loss: 10.3014\n",
      "====> Epoch: 124 Average loss: 10.3456\n",
      "====> Epoch: 126 Average loss: 10.3372\n",
      "====> Epoch: 128 Average loss: 10.3306\n",
      "====> Epoch: 130 Average loss: 10.2968\n",
      "====> Epoch: 132 Average loss: 10.3163\n",
      "====> Epoch: 134 Average loss: 10.3295\n",
      "====> Epoch: 136 Average loss: 10.3029\n",
      "====> Epoch: 138 Average loss: 10.3004\n",
      "====> Epoch: 140 Average loss: 10.2826\n",
      "====> Epoch: 142 Average loss: 10.2756\n",
      "====> Epoch: 144 Average loss: 10.2932\n",
      "====> Epoch: 146 Average loss: 10.3090\n",
      "====> Epoch: 148 Average loss: 10.2703\n",
      "====> Epoch: 150 Average loss: 10.2448\n",
      "====> Epoch: 152 Average loss: 10.2545\n",
      "====> Epoch: 154 Average loss: 10.2418\n",
      "====> Epoch: 156 Average loss: 10.2436\n",
      "====> Epoch: 158 Average loss: 10.3362\n",
      "====> Epoch: 160 Average loss: 10.3031\n",
      "====> Epoch: 162 Average loss: 10.2651\n",
      "====> Epoch: 164 Average loss: 10.2275\n",
      "====> Epoch: 166 Average loss: 10.2092\n",
      "====> Epoch: 168 Average loss: 10.2461\n",
      "====> Epoch: 170 Average loss: 10.2450\n",
      "====> Epoch: 172 Average loss: 10.2386\n",
      "====> Epoch: 174 Average loss: 10.2672\n",
      "====> Epoch: 176 Average loss: 10.2128\n",
      "====> Epoch: 178 Average loss: 10.2634\n",
      "====> Epoch: 180 Average loss: 10.2344\n",
      "====> Epoch: 182 Average loss: 10.2392\n",
      "====> Epoch: 184 Average loss: 10.1930\n",
      "====> Epoch: 186 Average loss: 10.2051\n",
      "====> Epoch: 188 Average loss: 10.1816\n",
      "====> Epoch: 190 Average loss: 10.1835\n",
      "====> Epoch: 192 Average loss: 10.2236\n",
      "====> Epoch: 194 Average loss: 10.1675\n",
      "====> Epoch: 196 Average loss: 10.2110\n",
      "====> Epoch: 198 Average loss: 10.1557\n",
      "====> Epoch: 200 Average loss: 10.2166\n",
      "====> Epoch: 202 Average loss: 10.1803\n",
      "====> Epoch: 204 Average loss: 10.1603\n",
      "====> Epoch: 206 Average loss: 10.2368\n",
      "====> Epoch: 208 Average loss: 10.2203\n",
      "====> Epoch: 210 Average loss: 10.1829\n",
      "====> Epoch: 212 Average loss: 10.1460\n",
      "====> Epoch: 214 Average loss: 10.1418\n",
      "====> Epoch: 216 Average loss: 10.1828\n",
      "====> Epoch: 218 Average loss: 10.1713\n",
      "====> Epoch: 220 Average loss: 10.1602\n",
      "====> Epoch: 222 Average loss: 10.1871\n",
      "====> Epoch: 224 Average loss: 10.1662\n",
      "====> Epoch: 226 Average loss: 10.1462\n",
      "====> Epoch: 228 Average loss: 10.1707\n",
      "====> Epoch: 230 Average loss: 10.1680\n",
      "====> Epoch: 232 Average loss: 10.1386\n",
      "====> Epoch: 234 Average loss: 10.1808\n",
      "====> Epoch: 236 Average loss: 10.2425\n",
      "====> Epoch: 238 Average loss: 10.1636\n",
      "====> Epoch: 240 Average loss: 10.1773\n",
      "====> Epoch: 242 Average loss: 10.1599\n",
      "====> Epoch: 244 Average loss: 10.1137\n",
      "====> Epoch: 246 Average loss: 10.0957\n",
      "====> Epoch: 248 Average loss: 10.1347\n",
      "====> Epoch: 250 Average loss: 10.1073\n",
      "====> Epoch: 252 Average loss: 10.1129\n",
      "====> Epoch: 254 Average loss: 10.1224\n",
      "====> Epoch: 256 Average loss: 10.0982\n",
      "====> Epoch: 258 Average loss: 10.0868\n",
      "====> Epoch: 260 Average loss: 10.1204\n",
      "====> Epoch: 262 Average loss: 10.1080\n",
      "====> Epoch: 264 Average loss: 10.1372\n",
      "====> Epoch: 266 Average loss: 10.0794\n",
      "====> Epoch: 268 Average loss: 10.1130\n",
      "====> Epoch: 270 Average loss: 10.0921\n",
      "====> Epoch: 272 Average loss: 10.1068\n",
      "====> Epoch: 274 Average loss: 10.1193\n",
      "====> Epoch: 276 Average loss: 10.1077\n",
      "====> Epoch: 278 Average loss: 10.0878\n",
      "====> Epoch: 280 Average loss: 10.1131\n",
      "====> Epoch: 282 Average loss: 10.1000\n",
      "====> Epoch: 284 Average loss: 10.0696\n",
      "====> Epoch: 286 Average loss: 10.0906\n",
      "====> Epoch: 288 Average loss: 10.1008\n",
      "====> Epoch: 290 Average loss: 10.0598\n",
      "====> Epoch: 292 Average loss: 10.0410\n",
      "====> Epoch: 294 Average loss: 10.0556\n",
      "====> Epoch: 296 Average loss: 10.0750\n",
      "====> Epoch: 298 Average loss: 10.0557\n",
      "====> Epoch: 300 Average loss: 10.0517\n",
      "====> Epoch: 302 Average loss: 10.0682\n",
      "====> Epoch: 304 Average loss: 10.0742\n",
      "====> Epoch: 306 Average loss: 10.0875\n",
      "====> Epoch: 308 Average loss: 10.0700\n",
      "====> Epoch: 310 Average loss: 10.0452\n",
      "====> Epoch: 312 Average loss: 10.0457\n",
      "====> Epoch: 314 Average loss: 10.0211\n",
      "====> Epoch: 316 Average loss: 10.0360\n",
      "====> Epoch: 318 Average loss: 10.0384\n",
      "====> Epoch: 320 Average loss: 10.0581\n",
      "====> Epoch: 322 Average loss: 10.0472\n",
      "====> Epoch: 324 Average loss: 10.0165\n",
      "====> Epoch: 326 Average loss: 9.9981\n",
      "====> Epoch: 328 Average loss: 10.0167\n",
      "====> Epoch: 330 Average loss: 10.0233\n",
      "====> Epoch: 332 Average loss: 10.0366\n",
      "====> Epoch: 334 Average loss: 10.0200\n",
      "====> Epoch: 336 Average loss: 10.0383\n",
      "====> Epoch: 338 Average loss: 10.0348\n",
      "====> Epoch: 340 Average loss: 10.0293\n",
      "====> Epoch: 342 Average loss: 10.0049\n",
      "====> Epoch: 344 Average loss: 10.0012\n",
      "====> Epoch: 346 Average loss: 10.0211\n",
      "====> Epoch: 348 Average loss: 10.0195\n",
      "====> Epoch: 350 Average loss: 9.9962\n",
      "====> Epoch: 352 Average loss: 9.9922\n",
      "====> Epoch: 354 Average loss: 10.0201\n",
      "====> Epoch: 356 Average loss: 9.9982\n",
      "====> Epoch: 358 Average loss: 10.0248\n",
      "====> Epoch: 360 Average loss: 10.0071\n",
      "====> Epoch: 362 Average loss: 10.0044\n",
      "====> Epoch: 364 Average loss: 10.0057\n",
      "====> Epoch: 366 Average loss: 9.9890\n",
      "====> Epoch: 368 Average loss: 10.0128\n",
      "====> Epoch: 370 Average loss: 9.9762\n",
      "====> Epoch: 372 Average loss: 10.0070\n",
      "====> Epoch: 374 Average loss: 10.0124\n",
      "====> Epoch: 376 Average loss: 9.9630\n",
      "====> Epoch: 378 Average loss: 9.9824\n",
      "====> Epoch: 380 Average loss: 9.9577\n",
      "====> Epoch: 382 Average loss: 10.0122\n",
      "====> Epoch: 384 Average loss: 9.9768\n",
      "====> Epoch: 386 Average loss: 9.9889\n",
      "====> Epoch: 388 Average loss: 9.9267\n",
      "====> Epoch: 390 Average loss: 9.9891\n",
      "====> Epoch: 392 Average loss: 9.9560\n",
      "====> Epoch: 394 Average loss: 9.9751\n",
      "====> Epoch: 396 Average loss: 9.9377\n",
      "====> Epoch: 398 Average loss: 9.9578\n",
      "====> Epoch: 400 Average loss: 9.9984\n",
      "====> Epoch: 402 Average loss: 9.9722\n",
      "====> Epoch: 404 Average loss: 9.9641\n",
      "====> Epoch: 406 Average loss: 9.9499\n",
      "====> Epoch: 408 Average loss: 9.9562\n",
      "====> Epoch: 410 Average loss: 9.9542\n",
      "====> Epoch: 412 Average loss: 9.9678\n",
      "====> Epoch: 414 Average loss: 9.9734\n",
      "====> Epoch: 416 Average loss: 9.9670\n",
      "====> Epoch: 418 Average loss: 9.9508\n",
      "====> Epoch: 420 Average loss: 9.9505\n",
      "====> Epoch: 422 Average loss: 9.9419\n",
      "====> Epoch: 424 Average loss: 9.9807\n",
      "====> Epoch: 426 Average loss: 9.9141\n",
      "====> Epoch: 428 Average loss: 9.9613\n",
      "====> Epoch: 430 Average loss: 9.9481\n",
      "====> Epoch: 432 Average loss: 9.9833\n",
      "====> Epoch: 434 Average loss: 9.9729\n",
      "====> Epoch: 436 Average loss: 9.9663\n",
      "====> Epoch: 438 Average loss: 9.9335\n",
      "====> Epoch: 440 Average loss: 9.9555\n",
      "====> Epoch: 442 Average loss: 9.9548\n",
      "====> Epoch: 444 Average loss: 9.9406\n",
      "====> Epoch: 446 Average loss: 9.9326\n",
      "====> Epoch: 448 Average loss: 9.9609\n",
      "====> Epoch: 450 Average loss: 9.9452\n",
      "====> Epoch: 452 Average loss: 9.9348\n",
      "====> Epoch: 454 Average loss: 9.9358\n",
      "====> Epoch: 456 Average loss: 9.9427\n",
      "====> Epoch: 458 Average loss: 9.9429\n",
      "====> Epoch: 460 Average loss: 9.9259\n",
      "====> Epoch: 462 Average loss: 9.9664\n",
      "====> Epoch: 464 Average loss: 9.9121\n",
      "====> Epoch: 466 Average loss: 9.9944\n",
      "====> Epoch: 468 Average loss: 9.9252\n",
      "====> Epoch: 470 Average loss: 9.9250\n",
      "====> Epoch: 472 Average loss: 9.9660\n",
      "====> Epoch: 474 Average loss: 9.9248\n",
      "====> Epoch: 476 Average loss: 9.9145\n",
      "====> Epoch: 478 Average loss: 9.9276\n",
      "====> Epoch: 480 Average loss: 9.9258\n",
      "====> Epoch: 482 Average loss: 9.9421\n",
      "====> Epoch: 484 Average loss: 9.9392\n",
      "====> Epoch: 486 Average loss: 9.9403\n",
      "====> Epoch: 488 Average loss: 9.9265\n",
      "====> Epoch: 490 Average loss: 9.9044\n",
      "====> Epoch: 492 Average loss: 9.9334\n",
      "====> Epoch: 494 Average loss: 9.9303\n",
      "====> Epoch: 496 Average loss: 9.9120\n",
      "====> Epoch: 498 Average loss: 9.9192\n",
      "====> Epoch: 500 Average loss: 9.9330\n",
      "====> Epoch: 502 Average loss: 9.9384\n",
      "====> Epoch: 504 Average loss: 9.9049\n",
      "====> Epoch: 506 Average loss: 9.9104\n",
      "====> Epoch: 508 Average loss: 9.9229\n",
      "====> Epoch: 510 Average loss: 9.9278\n",
      "====> Epoch: 512 Average loss: 9.9087\n",
      "====> Epoch: 514 Average loss: 9.9079\n",
      "====> Epoch: 516 Average loss: 9.8994\n",
      "====> Epoch: 518 Average loss: 9.9258\n",
      "====> Epoch: 520 Average loss: 9.9617\n",
      "====> Epoch: 522 Average loss: 9.9182\n",
      "====> Epoch: 524 Average loss: 9.9299\n",
      "====> Epoch: 526 Average loss: 9.9206\n",
      "====> Epoch: 528 Average loss: 9.9176\n",
      "====> Epoch: 530 Average loss: 9.8960\n",
      "====> Epoch: 532 Average loss: 9.9405\n",
      "====> Epoch: 534 Average loss: 9.8856\n",
      "====> Epoch: 536 Average loss: 9.8962\n",
      "====> Epoch: 538 Average loss: 9.9070\n",
      "====> Epoch: 540 Average loss: 9.8890\n",
      "====> Epoch: 542 Average loss: 9.8823\n",
      "====> Epoch: 544 Average loss: 9.8907\n",
      "====> Epoch: 546 Average loss: 9.9209\n",
      "====> Epoch: 548 Average loss: 9.8962\n",
      "====> Epoch: 550 Average loss: 9.8852\n",
      "====> Epoch: 552 Average loss: 9.8686\n",
      "====> Epoch: 554 Average loss: 9.8916\n",
      "====> Epoch: 556 Average loss: 9.9204\n",
      "====> Epoch: 558 Average loss: 9.9055\n",
      "====> Epoch: 560 Average loss: 9.8837\n",
      "====> Epoch: 562 Average loss: 9.8979\n",
      "====> Epoch: 564 Average loss: 9.8724\n",
      "====> Epoch: 566 Average loss: 9.9311\n",
      "====> Epoch: 568 Average loss: 9.8900\n",
      "====> Epoch: 570 Average loss: 9.8896\n",
      "====> Epoch: 572 Average loss: 9.8944\n",
      "====> Epoch: 574 Average loss: 9.8867\n",
      "====> Epoch: 576 Average loss: 9.9014\n",
      "====> Epoch: 578 Average loss: 9.8844\n",
      "====> Epoch: 580 Average loss: 9.8941\n",
      "====> Epoch: 582 Average loss: 9.9092\n",
      "====> Epoch: 584 Average loss: 9.8863\n",
      "====> Epoch: 586 Average loss: 9.8932\n",
      "====> Epoch: 588 Average loss: 9.8941\n",
      "====> Epoch: 590 Average loss: 9.8853\n",
      "====> Epoch: 592 Average loss: 9.8729\n",
      "====> Epoch: 594 Average loss: 9.9201\n",
      "====> Epoch: 596 Average loss: 9.8813\n",
      "====> Epoch: 598 Average loss: 9.8840\n",
      "====> Epoch: 600 Average loss: 9.9295\n",
      "====> Epoch: 602 Average loss: 9.8849\n",
      "====> Epoch: 604 Average loss: 9.8603\n",
      "====> Epoch: 606 Average loss: 9.8614\n",
      "====> Epoch: 608 Average loss: 9.8921\n",
      "====> Epoch: 610 Average loss: 9.8553\n",
      "====> Epoch: 612 Average loss: 9.8687\n",
      "====> Epoch: 614 Average loss: 9.8544\n",
      "====> Epoch: 616 Average loss: 9.8374\n",
      "====> Epoch: 618 Average loss: 9.8524\n",
      "====> Epoch: 620 Average loss: 9.8668\n",
      "====> Epoch: 622 Average loss: 9.8796\n",
      "====> Epoch: 624 Average loss: 9.8879\n",
      "====> Epoch: 626 Average loss: 9.8475\n",
      "====> Epoch: 628 Average loss: 9.8733\n",
      "====> Epoch: 630 Average loss: 9.8731\n",
      "====> Epoch: 632 Average loss: 9.8887\n",
      "====> Epoch: 634 Average loss: 9.8822\n",
      "====> Epoch: 636 Average loss: 9.8502\n",
      "====> Epoch: 638 Average loss: 9.8528\n",
      "====> Epoch: 640 Average loss: 9.8845\n",
      "====> Epoch: 642 Average loss: 9.8431\n",
      "====> Epoch: 644 Average loss: 9.8647\n",
      "====> Epoch: 646 Average loss: 9.8474\n",
      "====> Epoch: 648 Average loss: 9.8753\n",
      "====> Epoch: 650 Average loss: 9.8412\n",
      "====> Epoch: 652 Average loss: 9.8369\n",
      "====> Epoch: 654 Average loss: 9.8262\n",
      "====> Epoch: 656 Average loss: 9.8751\n",
      "====> Epoch: 658 Average loss: 9.8378\n",
      "====> Epoch: 660 Average loss: 9.8508\n",
      "====> Epoch: 662 Average loss: 9.8430\n",
      "====> Epoch: 664 Average loss: 9.8562\n",
      "====> Epoch: 666 Average loss: 9.8454\n",
      "====> Epoch: 668 Average loss: 9.8502\n",
      "====> Epoch: 670 Average loss: 9.8529\n",
      "====> Epoch: 672 Average loss: 9.8738\n",
      "====> Epoch: 674 Average loss: 9.8205\n",
      "====> Epoch: 676 Average loss: 9.8065\n",
      "====> Epoch: 678 Average loss: 9.8472\n",
      "====> Epoch: 680 Average loss: 9.8475\n",
      "====> Epoch: 682 Average loss: 9.8355\n",
      "====> Epoch: 684 Average loss: 9.8274\n",
      "====> Epoch: 686 Average loss: 9.8227\n",
      "====> Epoch: 688 Average loss: 9.8572\n",
      "====> Epoch: 690 Average loss: 9.8296\n",
      "====> Epoch: 692 Average loss: 9.8832\n",
      "====> Epoch: 694 Average loss: 9.8474\n",
      "====> Epoch: 696 Average loss: 9.8339\n",
      "====> Epoch: 698 Average loss: 9.8540\n",
      "====> Epoch: 700 Average loss: 9.8365\n",
      "====> Epoch: 702 Average loss: 9.8487\n",
      "====> Epoch: 704 Average loss: 9.8428\n",
      "====> Epoch: 706 Average loss: 9.8143\n",
      "====> Epoch: 708 Average loss: 9.8378\n",
      "====> Epoch: 710 Average loss: 9.8431\n",
      "====> Epoch: 712 Average loss: 9.8301\n",
      "====> Epoch: 714 Average loss: 9.8286\n",
      "====> Epoch: 716 Average loss: 9.8527\n",
      "====> Epoch: 718 Average loss: 9.8128\n",
      "====> Epoch: 720 Average loss: 9.8173\n",
      "====> Epoch: 722 Average loss: 9.8125\n",
      "====> Epoch: 724 Average loss: 9.8343\n",
      "====> Epoch: 726 Average loss: 9.8530\n",
      "====> Epoch: 728 Average loss: 9.8685\n",
      "====> Epoch: 730 Average loss: 9.8515\n",
      "====> Epoch: 732 Average loss: 9.8370\n",
      "====> Epoch: 734 Average loss: 9.7971\n",
      "====> Epoch: 736 Average loss: 9.8087\n",
      "====> Epoch: 738 Average loss: 9.8264\n",
      "====> Epoch: 740 Average loss: 9.8247\n",
      "====> Epoch: 742 Average loss: 9.8209\n",
      "====> Epoch: 744 Average loss: 9.8093\n",
      "====> Epoch: 746 Average loss: 9.8338\n",
      "====> Epoch: 748 Average loss: 9.7999\n",
      "====> Epoch: 750 Average loss: 9.8273\n",
      "====> Epoch: 752 Average loss: 9.8392\n",
      "====> Epoch: 754 Average loss: 9.8290\n",
      "====> Epoch: 756 Average loss: 9.8076\n",
      "====> Epoch: 758 Average loss: 9.8155\n",
      "====> Epoch: 760 Average loss: 9.8000\n",
      "====> Epoch: 762 Average loss: 9.8299\n",
      "====> Epoch: 764 Average loss: 9.8111\n",
      "====> Epoch: 766 Average loss: 9.7839\n",
      "====> Epoch: 768 Average loss: 9.8092\n",
      "====> Epoch: 770 Average loss: 9.8085\n",
      "====> Epoch: 772 Average loss: 9.8363\n",
      "====> Epoch: 774 Average loss: 9.7730\n",
      "====> Epoch: 776 Average loss: 9.8173\n",
      "====> Epoch: 778 Average loss: 9.8011\n",
      "====> Epoch: 780 Average loss: 9.8098\n",
      "====> Epoch: 782 Average loss: 9.8055\n",
      "====> Epoch: 784 Average loss: 9.8081\n",
      "====> Epoch: 786 Average loss: 9.8101\n",
      "====> Epoch: 788 Average loss: 9.8168\n",
      "====> Epoch: 790 Average loss: 9.8331\n",
      "====> Epoch: 792 Average loss: 9.8086\n",
      "====> Epoch: 794 Average loss: 9.8351\n",
      "====> Epoch: 796 Average loss: 9.8051\n",
      "====> Epoch: 798 Average loss: 9.7950\n",
      "====> Epoch: 800 Average loss: 9.8330\n",
      "====> Epoch: 802 Average loss: 9.8180\n",
      "====> Epoch: 804 Average loss: 9.7840\n",
      "====> Epoch: 806 Average loss: 9.8314\n",
      "====> Epoch: 808 Average loss: 9.7851\n",
      "====> Epoch: 810 Average loss: 9.8127\n",
      "====> Epoch: 812 Average loss: 9.8169\n",
      "====> Epoch: 814 Average loss: 9.8183\n",
      "====> Epoch: 816 Average loss: 9.8096\n",
      "====> Epoch: 818 Average loss: 9.8069\n",
      "====> Epoch: 820 Average loss: 9.8180\n",
      "====> Epoch: 822 Average loss: 9.7988\n",
      "====> Epoch: 824 Average loss: 9.8181\n",
      "====> Epoch: 826 Average loss: 9.7809\n",
      "====> Epoch: 828 Average loss: 9.8517\n",
      "====> Epoch: 830 Average loss: 9.8091\n",
      "====> Epoch: 832 Average loss: 9.7938\n",
      "====> Epoch: 834 Average loss: 9.7838\n",
      "====> Epoch: 836 Average loss: 9.8314\n",
      "====> Epoch: 838 Average loss: 9.8494\n",
      "====> Epoch: 840 Average loss: 9.8070\n",
      "====> Epoch: 842 Average loss: 9.8020\n",
      "====> Epoch: 844 Average loss: 9.8002\n",
      "====> Epoch: 846 Average loss: 9.8036\n",
      "====> Epoch: 848 Average loss: 9.8015\n",
      "====> Epoch: 850 Average loss: 9.8132\n",
      "====> Epoch: 852 Average loss: 9.7923\n",
      "====> Epoch: 854 Average loss: 9.8001\n",
      "====> Epoch: 856 Average loss: 9.7759\n",
      "====> Epoch: 858 Average loss: 9.7772\n",
      "====> Epoch: 860 Average loss: 9.8056\n",
      "====> Epoch: 862 Average loss: 9.8034\n",
      "====> Epoch: 864 Average loss: 9.8482\n",
      "====> Epoch: 866 Average loss: 9.7745\n",
      "====> Epoch: 868 Average loss: 9.8069\n",
      "====> Epoch: 870 Average loss: 9.7935\n",
      "====> Epoch: 872 Average loss: 9.7709\n",
      "====> Epoch: 874 Average loss: 9.7862\n",
      "====> Epoch: 876 Average loss: 9.7783\n",
      "====> Epoch: 878 Average loss: 9.7558\n",
      "====> Epoch: 880 Average loss: 9.7827\n",
      "====> Epoch: 882 Average loss: 9.8163\n",
      "====> Epoch: 884 Average loss: 9.7876\n",
      "====> Epoch: 886 Average loss: 9.7952\n",
      "====> Epoch: 888 Average loss: 9.8220\n",
      "====> Epoch: 890 Average loss: 9.7846\n",
      "====> Epoch: 892 Average loss: 9.7755\n",
      "====> Epoch: 894 Average loss: 9.7913\n",
      "====> Epoch: 896 Average loss: 9.7787\n",
      "====> Epoch: 898 Average loss: 9.7812\n",
      "====> Epoch: 900 Average loss: 9.7920\n",
      "====> Epoch: 902 Average loss: 9.7660\n",
      "====> Epoch: 904 Average loss: 9.7805\n",
      "====> Epoch: 906 Average loss: 9.8058\n",
      "====> Epoch: 908 Average loss: 9.8176\n",
      "====> Epoch: 910 Average loss: 9.7908\n",
      "====> Epoch: 912 Average loss: 9.7922\n",
      "====> Epoch: 914 Average loss: 9.7606\n",
      "====> Epoch: 916 Average loss: 9.7848\n",
      "====> Epoch: 918 Average loss: 9.7641\n",
      "====> Epoch: 920 Average loss: 9.7878\n",
      "====> Epoch: 922 Average loss: 9.7872\n",
      "====> Epoch: 924 Average loss: 9.7819\n",
      "====> Epoch: 926 Average loss: 9.7580\n",
      "====> Epoch: 928 Average loss: 9.8807\n",
      "====> Epoch: 930 Average loss: 9.7842\n",
      "====> Epoch: 932 Average loss: 9.7698\n",
      "====> Epoch: 934 Average loss: 9.7653\n",
      "====> Epoch: 936 Average loss: 9.8369\n",
      "====> Epoch: 938 Average loss: 9.7691\n",
      "====> Epoch: 940 Average loss: 9.7737\n",
      "====> Epoch: 942 Average loss: 9.7790\n",
      "====> Epoch: 944 Average loss: 9.7761\n",
      "====> Epoch: 946 Average loss: 9.7740\n",
      "====> Epoch: 948 Average loss: 9.7484\n",
      "====> Epoch: 950 Average loss: 9.7507\n",
      "====> Epoch: 952 Average loss: 9.7958\n",
      "====> Epoch: 954 Average loss: 9.7643\n",
      "====> Epoch: 956 Average loss: 9.7750\n",
      "====> Epoch: 958 Average loss: 9.7720\n",
      "====> Epoch: 960 Average loss: 9.7786\n",
      "====> Epoch: 962 Average loss: 9.7689\n",
      "====> Epoch: 964 Average loss: 9.7579\n",
      "====> Epoch: 966 Average loss: 9.7624\n",
      "====> Epoch: 968 Average loss: 9.7933\n",
      "====> Epoch: 970 Average loss: 9.7664\n",
      "====> Epoch: 972 Average loss: 9.7714\n",
      "====> Epoch: 974 Average loss: 9.7733\n",
      "====> Epoch: 976 Average loss: 9.7906\n",
      "====> Epoch: 978 Average loss: 9.7791\n",
      "====> Epoch: 980 Average loss: 9.7492\n",
      "====> Epoch: 982 Average loss: 9.7777\n",
      "====> Epoch: 984 Average loss: 9.7502\n",
      "====> Epoch: 986 Average loss: 9.7782\n",
      "====> Epoch: 988 Average loss: 9.8000\n",
      "====> Epoch: 990 Average loss: 9.7653\n",
      "====> Epoch: 992 Average loss: 9.8140\n",
      "====> Epoch: 994 Average loss: 9.7618\n",
      "====> Epoch: 996 Average loss: 9.7855\n",
      "====> Epoch: 998 Average loss: 9.7611\n",
      "====> Epoch: 1000 Average loss: 9.7660\n"
     ]
    }
   ],
   "source": [
    "model_bank, bank_standardizer = train_encoder(\"Data/bank.csv\", load_bank_data, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_for_bank_ds(model):\n",
    "    DATA_PATH = \"Data/bank.csv\"\n",
    "    df = load_diabetes_data(DATA_PATH, sep=\",\")\n",
    "    actual_data = df[0]\n",
    "    outcomes = df[2]\n",
    "\n",
    "    latents = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, e in enumerate(actual_data):\n",
    "            sample = e.unsqueeze(0)  # Add batch dimension\n",
    "            latent = model.embed(sample)  # Get the latent representation\n",
    "            latents.append(latent.squeeze().cpu().numpy())\n",
    "\n",
    "    latents_df = pd.DataFrame(latents)\n",
    "    outcomes_df = pd.DataFrame(outcomes)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv('latent_data/bank_latent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_for_bank_ds(model_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bank_syn = reconstruction_of_latent(model_bank, 'syn_latent/bank_synth/X_num_unnorm.npy', 'syn_latent/bank_synth/y_train.npy', bank_standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 13)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bank_real = load_bank_data(\"Data/bank.csv\", \",\")\n",
    "x_bank_real = x_bank_real[1].inverse_transform(x_bank_real[0].cpu().detach().numpy())\n",
    "x_bank_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 13)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bank_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8743903071420271\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_bank_syn[:4999], x_bank_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertical Partitioning Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Columns in 3 clients\n",
    "### Run Individual Auto Encoder\n",
    "### Generate N Latents\n",
    "### Get Generated Data From Tabddpm Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  insu  mass   pedi  age               class\n",
       "0       6   148    72    35     0  33.6  0.627   50  b'tested_positive'\n",
       "1       1    85    66    29     0  26.6  0.351   31  b'tested_negative'\n",
       "2       8   183    64     0     0  23.3  0.672   32  b'tested_positive'\n",
       "3       1    89    66    23    94  28.1  0.167   21  b'tested_negative'\n",
       "4       0   137    40    35   168  43.1  2.288   33  b'tested_positive'\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...                 ...\n",
       "763    10   101    76    48   180  32.9  0.171   63  b'tested_negative'\n",
       "764     2   122    70    27     0  36.8  0.340   27  b'tested_negative'\n",
       "765     5   121    72    23   112  26.2  0.245   30  b'tested_negative'\n",
       "766     1   126    60     0     0  30.1  0.349   47  b'tested_positive'\n",
       "767     1    93    70    31     0  30.4  0.315   23  b'tested_negative'\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_all_clients = pd.read_csv(\"Data/diabetes.csv\")\n",
    "cardio_all_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = cardio_all_clients.iloc[:, -1:]\n",
    "df_client_1 = cardio_all_clients.iloc[:, :2]\n",
    "df_client_2 = cardio_all_clients.iloc[:, 2:4]\n",
    "df_client_3 = cardio_all_clients.iloc[:, 4:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/ynmqlkx14wd993676tgwybz80000gn/T/ipykernel_2863/3254862835.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_label['class_binary'] = le.fit_transform(df_label['class'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_binary\n",
       "0               1\n",
       "1               0\n",
       "2               1\n",
       "3               0\n",
       "4               1\n",
       "..            ...\n",
       "763             0\n",
       "764             0\n",
       "765             0\n",
       "766             1\n",
       "767             0\n",
       "\n",
       "[768 rows x 1 columns]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_label['class_binary'] = le.fit_transform(df_label['class'])\n",
    "df_label = df_label.drop(columns=[\"class\"])\n",
    "\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_1 = pd.concat([df_client_1, df_label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_1.to_csv(\"client_n_data/diabetes_clients/client_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_2 = pd.concat([df_client_2, df_label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_2.to_csv(\"client_n_data/diabetes_clients/client_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_3 = pd.concat([df_client_3, df_label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_3.to_csv(\"client_n_data/diabetes_clients/client_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 3.5405\n",
      "====> Epoch: 4 Average loss: 3.0714\n",
      "====> Epoch: 6 Average loss: 2.4556\n",
      "====> Epoch: 8 Average loss: 2.2651\n",
      "====> Epoch: 10 Average loss: 2.0781\n",
      "====> Epoch: 12 Average loss: 2.0319\n",
      "====> Epoch: 14 Average loss: 1.9991\n",
      "====> Epoch: 16 Average loss: 1.8858\n",
      "====> Epoch: 18 Average loss: 1.8025\n",
      "====> Epoch: 20 Average loss: 1.8712\n",
      "====> Epoch: 22 Average loss: 1.8883\n",
      "====> Epoch: 24 Average loss: 1.8504\n",
      "====> Epoch: 26 Average loss: 1.7753\n",
      "====> Epoch: 28 Average loss: 1.8197\n",
      "====> Epoch: 30 Average loss: 1.7261\n",
      "====> Epoch: 32 Average loss: 1.7277\n",
      "====> Epoch: 34 Average loss: 1.7315\n",
      "====> Epoch: 36 Average loss: 1.7500\n",
      "====> Epoch: 38 Average loss: 1.7442\n",
      "====> Epoch: 40 Average loss: 1.7476\n",
      "====> Epoch: 42 Average loss: 1.7582\n",
      "====> Epoch: 44 Average loss: 1.7040\n",
      "====> Epoch: 46 Average loss: 1.7155\n",
      "====> Epoch: 48 Average loss: 1.7261\n",
      "====> Epoch: 50 Average loss: 1.6683\n",
      "====> Epoch: 52 Average loss: 1.7289\n",
      "====> Epoch: 54 Average loss: 1.7096\n",
      "====> Epoch: 56 Average loss: 1.7347\n",
      "====> Epoch: 58 Average loss: 1.7447\n",
      "====> Epoch: 60 Average loss: 1.8215\n",
      "====> Epoch: 62 Average loss: 1.7088\n",
      "====> Epoch: 64 Average loss: 1.7215\n",
      "====> Epoch: 66 Average loss: 1.7551\n",
      "====> Epoch: 68 Average loss: 1.6839\n",
      "====> Epoch: 70 Average loss: 1.7051\n",
      "====> Epoch: 72 Average loss: 1.6771\n",
      "====> Epoch: 74 Average loss: 1.6501\n",
      "====> Epoch: 76 Average loss: 1.7071\n",
      "====> Epoch: 78 Average loss: 1.7152\n",
      "====> Epoch: 80 Average loss: 1.7246\n",
      "====> Epoch: 82 Average loss: 1.7413\n",
      "====> Epoch: 84 Average loss: 1.7018\n",
      "====> Epoch: 86 Average loss: 1.6997\n",
      "====> Epoch: 88 Average loss: 1.7203\n",
      "====> Epoch: 90 Average loss: 1.7386\n",
      "====> Epoch: 92 Average loss: 1.7158\n",
      "====> Epoch: 94 Average loss: 1.6656\n",
      "====> Epoch: 96 Average loss: 1.7341\n",
      "====> Epoch: 98 Average loss: 1.7274\n",
      "====> Epoch: 100 Average loss: 1.7314\n",
      "====> Epoch: 102 Average loss: 1.6627\n",
      "====> Epoch: 104 Average loss: 1.7201\n",
      "====> Epoch: 106 Average loss: 1.7170\n",
      "====> Epoch: 108 Average loss: 1.6640\n",
      "====> Epoch: 110 Average loss: 1.6679\n",
      "====> Epoch: 112 Average loss: 1.6976\n",
      "====> Epoch: 114 Average loss: 1.7334\n",
      "====> Epoch: 116 Average loss: 1.6897\n",
      "====> Epoch: 118 Average loss: 1.7156\n",
      "====> Epoch: 120 Average loss: 1.6679\n",
      "====> Epoch: 122 Average loss: 1.7064\n",
      "====> Epoch: 124 Average loss: 1.7281\n",
      "====> Epoch: 126 Average loss: 1.7047\n",
      "====> Epoch: 128 Average loss: 1.7129\n",
      "====> Epoch: 130 Average loss: 1.6930\n",
      "====> Epoch: 132 Average loss: 1.7182\n",
      "====> Epoch: 134 Average loss: 1.7333\n",
      "====> Epoch: 136 Average loss: 1.6536\n",
      "====> Epoch: 138 Average loss: 1.6854\n",
      "====> Epoch: 140 Average loss: 1.7034\n",
      "====> Epoch: 142 Average loss: 1.6306\n",
      "====> Epoch: 144 Average loss: 1.6402\n",
      "====> Epoch: 146 Average loss: 1.7012\n",
      "====> Epoch: 148 Average loss: 1.6855\n",
      "====> Epoch: 150 Average loss: 1.6918\n",
      "====> Epoch: 152 Average loss: 1.6850\n",
      "====> Epoch: 154 Average loss: 1.6562\n",
      "====> Epoch: 156 Average loss: 1.7031\n",
      "====> Epoch: 158 Average loss: 1.7279\n",
      "====> Epoch: 160 Average loss: 1.6555\n",
      "====> Epoch: 162 Average loss: 1.6544\n",
      "====> Epoch: 164 Average loss: 1.6529\n",
      "====> Epoch: 166 Average loss: 1.7080\n",
      "====> Epoch: 168 Average loss: 1.7333\n",
      "====> Epoch: 170 Average loss: 1.6962\n",
      "====> Epoch: 172 Average loss: 1.6966\n",
      "====> Epoch: 174 Average loss: 1.6657\n",
      "====> Epoch: 176 Average loss: 1.6611\n",
      "====> Epoch: 178 Average loss: 1.6888\n",
      "====> Epoch: 180 Average loss: 1.6954\n",
      "====> Epoch: 182 Average loss: 1.5925\n",
      "====> Epoch: 184 Average loss: 1.7268\n",
      "====> Epoch: 186 Average loss: 1.6994\n",
      "====> Epoch: 188 Average loss: 1.6932\n",
      "====> Epoch: 190 Average loss: 1.6426\n",
      "====> Epoch: 192 Average loss: 1.6839\n",
      "====> Epoch: 194 Average loss: 1.6478\n",
      "====> Epoch: 196 Average loss: 1.6989\n",
      "====> Epoch: 198 Average loss: 1.6682\n",
      "====> Epoch: 200 Average loss: 1.7006\n",
      "====> Epoch: 202 Average loss: 1.6961\n",
      "====> Epoch: 204 Average loss: 1.6266\n",
      "====> Epoch: 206 Average loss: 1.6750\n",
      "====> Epoch: 208 Average loss: 1.7163\n",
      "====> Epoch: 210 Average loss: 1.6957\n",
      "====> Epoch: 212 Average loss: 1.7064\n",
      "====> Epoch: 214 Average loss: 1.6684\n",
      "====> Epoch: 216 Average loss: 1.7103\n",
      "====> Epoch: 218 Average loss: 1.6973\n",
      "====> Epoch: 220 Average loss: 1.7155\n",
      "====> Epoch: 222 Average loss: 1.7005\n",
      "====> Epoch: 224 Average loss: 1.6243\n",
      "====> Epoch: 226 Average loss: 1.6541\n",
      "====> Epoch: 228 Average loss: 1.6990\n",
      "====> Epoch: 230 Average loss: 1.6778\n",
      "====> Epoch: 232 Average loss: 1.6540\n",
      "====> Epoch: 234 Average loss: 1.7032\n",
      "====> Epoch: 236 Average loss: 1.6991\n",
      "====> Epoch: 238 Average loss: 1.7054\n",
      "====> Epoch: 240 Average loss: 1.7092\n",
      "====> Epoch: 242 Average loss: 1.6284\n",
      "====> Epoch: 244 Average loss: 1.6743\n",
      "====> Epoch: 246 Average loss: 1.6394\n",
      "====> Epoch: 248 Average loss: 1.6163\n",
      "====> Epoch: 250 Average loss: 1.6725\n",
      "====> Epoch: 252 Average loss: 1.6793\n",
      "====> Epoch: 254 Average loss: 1.6568\n",
      "====> Epoch: 256 Average loss: 1.6589\n",
      "====> Epoch: 258 Average loss: 1.6937\n",
      "====> Epoch: 260 Average loss: 1.6651\n",
      "====> Epoch: 262 Average loss: 1.7347\n",
      "====> Epoch: 264 Average loss: 1.6769\n",
      "====> Epoch: 266 Average loss: 1.6829\n",
      "====> Epoch: 268 Average loss: 1.6563\n",
      "====> Epoch: 270 Average loss: 1.6986\n",
      "====> Epoch: 272 Average loss: 1.6853\n",
      "====> Epoch: 274 Average loss: 1.6510\n",
      "====> Epoch: 276 Average loss: 1.7111\n",
      "====> Epoch: 278 Average loss: 1.7118\n",
      "====> Epoch: 280 Average loss: 1.6843\n",
      "====> Epoch: 282 Average loss: 1.7448\n",
      "====> Epoch: 284 Average loss: 1.6993\n",
      "====> Epoch: 286 Average loss: 1.6678\n",
      "====> Epoch: 288 Average loss: 1.6781\n",
      "====> Epoch: 290 Average loss: 1.7163\n",
      "====> Epoch: 292 Average loss: 1.6590\n",
      "====> Epoch: 294 Average loss: 1.6804\n",
      "====> Epoch: 296 Average loss: 1.6558\n",
      "====> Epoch: 298 Average loss: 1.6893\n",
      "====> Epoch: 300 Average loss: 1.6553\n",
      "====> Epoch: 302 Average loss: 1.7134\n",
      "====> Epoch: 304 Average loss: 1.6780\n",
      "====> Epoch: 306 Average loss: 1.6905\n",
      "====> Epoch: 308 Average loss: 1.6932\n",
      "====> Epoch: 310 Average loss: 1.6915\n",
      "====> Epoch: 312 Average loss: 1.6403\n",
      "====> Epoch: 314 Average loss: 1.6572\n",
      "====> Epoch: 316 Average loss: 1.6839\n",
      "====> Epoch: 318 Average loss: 1.7001\n",
      "====> Epoch: 320 Average loss: 1.6598\n",
      "====> Epoch: 322 Average loss: 1.6607\n",
      "====> Epoch: 324 Average loss: 1.6715\n",
      "====> Epoch: 326 Average loss: 1.6911\n",
      "====> Epoch: 328 Average loss: 1.6297\n",
      "====> Epoch: 330 Average loss: 1.6785\n",
      "====> Epoch: 332 Average loss: 1.6860\n",
      "====> Epoch: 334 Average loss: 1.7193\n",
      "====> Epoch: 336 Average loss: 1.7319\n",
      "====> Epoch: 338 Average loss: 1.6854\n",
      "====> Epoch: 340 Average loss: 1.6730\n",
      "====> Epoch: 342 Average loss: 1.6655\n",
      "====> Epoch: 344 Average loss: 1.6574\n",
      "====> Epoch: 346 Average loss: 1.7010\n",
      "====> Epoch: 348 Average loss: 1.6907\n",
      "====> Epoch: 350 Average loss: 1.7339\n",
      "====> Epoch: 352 Average loss: 1.6428\n",
      "====> Epoch: 354 Average loss: 1.7168\n",
      "====> Epoch: 356 Average loss: 1.6769\n",
      "====> Epoch: 358 Average loss: 1.6693\n",
      "====> Epoch: 360 Average loss: 1.6839\n",
      "====> Epoch: 362 Average loss: 1.7187\n",
      "====> Epoch: 364 Average loss: 1.6586\n",
      "====> Epoch: 366 Average loss: 1.7163\n",
      "====> Epoch: 368 Average loss: 1.6732\n",
      "====> Epoch: 370 Average loss: 1.7223\n",
      "====> Epoch: 372 Average loss: 1.6767\n",
      "====> Epoch: 374 Average loss: 1.6901\n",
      "====> Epoch: 376 Average loss: 1.7141\n",
      "====> Epoch: 378 Average loss: 1.7484\n",
      "====> Epoch: 380 Average loss: 1.6679\n",
      "====> Epoch: 382 Average loss: 1.6654\n",
      "====> Epoch: 384 Average loss: 1.6655\n",
      "====> Epoch: 386 Average loss: 1.6355\n",
      "====> Epoch: 388 Average loss: 1.6195\n",
      "====> Epoch: 390 Average loss: 1.6567\n",
      "====> Epoch: 392 Average loss: 1.6788\n",
      "====> Epoch: 394 Average loss: 1.7011\n",
      "====> Epoch: 396 Average loss: 1.7171\n",
      "====> Epoch: 398 Average loss: 1.6893\n",
      "====> Epoch: 400 Average loss: 1.6784\n",
      "====> Epoch: 402 Average loss: 1.6661\n",
      "====> Epoch: 404 Average loss: 1.6873\n",
      "====> Epoch: 406 Average loss: 1.6576\n",
      "====> Epoch: 408 Average loss: 1.6591\n",
      "====> Epoch: 410 Average loss: 1.6778\n",
      "====> Epoch: 412 Average loss: 1.6623\n",
      "====> Epoch: 414 Average loss: 1.6842\n",
      "====> Epoch: 416 Average loss: 1.6888\n",
      "====> Epoch: 418 Average loss: 1.6930\n",
      "====> Epoch: 420 Average loss: 1.7110\n",
      "====> Epoch: 422 Average loss: 1.7234\n",
      "====> Epoch: 424 Average loss: 1.6904\n",
      "====> Epoch: 426 Average loss: 1.6951\n",
      "====> Epoch: 428 Average loss: 1.7115\n",
      "====> Epoch: 430 Average loss: 1.7164\n",
      "====> Epoch: 432 Average loss: 1.6643\n",
      "====> Epoch: 434 Average loss: 1.6748\n",
      "====> Epoch: 436 Average loss: 1.6698\n",
      "====> Epoch: 438 Average loss: 1.6555\n",
      "====> Epoch: 440 Average loss: 1.6715\n",
      "====> Epoch: 442 Average loss: 1.7048\n",
      "====> Epoch: 444 Average loss: 1.6991\n",
      "====> Epoch: 446 Average loss: 1.7517\n",
      "====> Epoch: 448 Average loss: 1.6896\n",
      "====> Epoch: 450 Average loss: 1.6421\n",
      "====> Epoch: 452 Average loss: 1.6742\n",
      "====> Epoch: 454 Average loss: 1.6809\n",
      "====> Epoch: 456 Average loss: 1.6859\n",
      "====> Epoch: 458 Average loss: 1.7035\n",
      "====> Epoch: 460 Average loss: 1.7057\n",
      "====> Epoch: 462 Average loss: 1.6916\n",
      "====> Epoch: 464 Average loss: 1.6988\n",
      "====> Epoch: 466 Average loss: 1.7015\n",
      "====> Epoch: 468 Average loss: 1.6794\n",
      "====> Epoch: 470 Average loss: 1.7087\n",
      "====> Epoch: 472 Average loss: 1.7705\n",
      "====> Epoch: 474 Average loss: 1.6568\n",
      "====> Epoch: 476 Average loss: 1.6987\n",
      "====> Epoch: 478 Average loss: 1.6803\n",
      "====> Epoch: 480 Average loss: 1.6809\n",
      "====> Epoch: 482 Average loss: 1.6129\n",
      "====> Epoch: 484 Average loss: 1.6969\n",
      "====> Epoch: 486 Average loss: 1.6325\n",
      "====> Epoch: 488 Average loss: 1.6842\n",
      "====> Epoch: 490 Average loss: 1.7834\n",
      "====> Epoch: 492 Average loss: 1.7021\n",
      "====> Epoch: 494 Average loss: 1.6346\n",
      "====> Epoch: 496 Average loss: 1.6900\n",
      "====> Epoch: 498 Average loss: 1.6949\n",
      "====> Epoch: 500 Average loss: 1.6705\n",
      "====> Epoch: 502 Average loss: 1.7133\n",
      "====> Epoch: 504 Average loss: 1.6939\n",
      "====> Epoch: 506 Average loss: 1.6809\n",
      "====> Epoch: 508 Average loss: 1.6598\n",
      "====> Epoch: 510 Average loss: 1.7459\n",
      "====> Epoch: 512 Average loss: 1.6816\n",
      "====> Epoch: 514 Average loss: 1.6867\n",
      "====> Epoch: 516 Average loss: 1.7010\n",
      "====> Epoch: 518 Average loss: 1.6567\n",
      "====> Epoch: 520 Average loss: 1.6790\n",
      "====> Epoch: 522 Average loss: 1.6581\n",
      "====> Epoch: 524 Average loss: 1.6637\n",
      "====> Epoch: 526 Average loss: 1.6777\n",
      "====> Epoch: 528 Average loss: 1.6975\n",
      "====> Epoch: 530 Average loss: 1.6486\n",
      "====> Epoch: 532 Average loss: 1.6931\n",
      "====> Epoch: 534 Average loss: 1.6566\n",
      "====> Epoch: 536 Average loss: 1.6466\n",
      "====> Epoch: 538 Average loss: 1.6655\n",
      "====> Epoch: 540 Average loss: 1.6814\n",
      "====> Epoch: 542 Average loss: 1.6413\n",
      "====> Epoch: 544 Average loss: 1.7032\n",
      "====> Epoch: 546 Average loss: 1.6616\n",
      "====> Epoch: 548 Average loss: 1.7463\n",
      "====> Epoch: 550 Average loss: 1.6942\n",
      "====> Epoch: 552 Average loss: 1.6528\n",
      "====> Epoch: 554 Average loss: 1.6502\n",
      "====> Epoch: 556 Average loss: 1.7033\n",
      "====> Epoch: 558 Average loss: 1.6583\n",
      "====> Epoch: 560 Average loss: 1.6574\n",
      "====> Epoch: 562 Average loss: 1.7021\n",
      "====> Epoch: 564 Average loss: 1.6513\n",
      "====> Epoch: 566 Average loss: 1.7231\n",
      "====> Epoch: 568 Average loss: 1.6577\n",
      "====> Epoch: 570 Average loss: 1.7030\n",
      "====> Epoch: 572 Average loss: 1.6447\n",
      "====> Epoch: 574 Average loss: 1.6819\n",
      "====> Epoch: 576 Average loss: 1.6227\n",
      "====> Epoch: 578 Average loss: 1.6620\n",
      "====> Epoch: 580 Average loss: 1.6988\n",
      "====> Epoch: 582 Average loss: 1.6442\n",
      "====> Epoch: 584 Average loss: 1.6418\n",
      "====> Epoch: 586 Average loss: 1.6352\n",
      "====> Epoch: 588 Average loss: 1.7316\n",
      "====> Epoch: 590 Average loss: 1.6207\n",
      "====> Epoch: 592 Average loss: 1.6653\n",
      "====> Epoch: 594 Average loss: 1.6827\n",
      "====> Epoch: 596 Average loss: 1.6516\n",
      "====> Epoch: 598 Average loss: 1.6932\n",
      "====> Epoch: 600 Average loss: 1.7247\n",
      "====> Epoch: 602 Average loss: 1.6720\n",
      "====> Epoch: 604 Average loss: 1.6465\n",
      "====> Epoch: 606 Average loss: 1.6603\n",
      "====> Epoch: 608 Average loss: 1.6947\n",
      "====> Epoch: 610 Average loss: 1.6879\n",
      "====> Epoch: 612 Average loss: 1.6393\n",
      "====> Epoch: 614 Average loss: 1.6491\n",
      "====> Epoch: 616 Average loss: 1.7106\n",
      "====> Epoch: 618 Average loss: 1.6099\n",
      "====> Epoch: 620 Average loss: 1.6999\n",
      "====> Epoch: 622 Average loss: 1.6494\n",
      "====> Epoch: 624 Average loss: 1.6889\n",
      "====> Epoch: 626 Average loss: 1.6710\n",
      "====> Epoch: 628 Average loss: 1.6214\n",
      "====> Epoch: 630 Average loss: 1.6891\n",
      "====> Epoch: 632 Average loss: 1.6569\n",
      "====> Epoch: 634 Average loss: 1.7201\n",
      "====> Epoch: 636 Average loss: 1.6672\n",
      "====> Epoch: 638 Average loss: 1.6872\n",
      "====> Epoch: 640 Average loss: 1.6827\n",
      "====> Epoch: 642 Average loss: 1.6564\n",
      "====> Epoch: 644 Average loss: 1.6862\n",
      "====> Epoch: 646 Average loss: 1.6763\n",
      "====> Epoch: 648 Average loss: 1.6848\n",
      "====> Epoch: 650 Average loss: 1.7122\n",
      "====> Epoch: 652 Average loss: 1.7484\n",
      "====> Epoch: 654 Average loss: 1.6994\n",
      "====> Epoch: 656 Average loss: 1.6567\n",
      "====> Epoch: 658 Average loss: 1.6879\n",
      "====> Epoch: 660 Average loss: 1.6440\n",
      "====> Epoch: 662 Average loss: 1.6499\n",
      "====> Epoch: 664 Average loss: 1.6636\n",
      "====> Epoch: 666 Average loss: 1.6329\n",
      "====> Epoch: 668 Average loss: 1.7234\n",
      "====> Epoch: 670 Average loss: 1.6594\n",
      "====> Epoch: 672 Average loss: 1.6977\n",
      "====> Epoch: 674 Average loss: 1.7025\n",
      "====> Epoch: 676 Average loss: 1.6905\n",
      "====> Epoch: 678 Average loss: 1.7150\n",
      "====> Epoch: 680 Average loss: 1.6845\n",
      "====> Epoch: 682 Average loss: 1.6776\n",
      "====> Epoch: 684 Average loss: 1.6312\n",
      "====> Epoch: 686 Average loss: 1.6799\n",
      "====> Epoch: 688 Average loss: 1.6727\n",
      "====> Epoch: 690 Average loss: 1.6603\n",
      "====> Epoch: 692 Average loss: 1.7061\n",
      "====> Epoch: 694 Average loss: 1.7156\n",
      "====> Epoch: 696 Average loss: 1.6069\n",
      "====> Epoch: 698 Average loss: 1.6475\n",
      "====> Epoch: 700 Average loss: 1.6469\n",
      "====> Epoch: 702 Average loss: 1.7095\n",
      "====> Epoch: 704 Average loss: 1.6750\n",
      "====> Epoch: 706 Average loss: 1.6713\n",
      "====> Epoch: 708 Average loss: 1.6352\n",
      "====> Epoch: 710 Average loss: 1.6948\n",
      "====> Epoch: 712 Average loss: 1.6186\n",
      "====> Epoch: 714 Average loss: 1.6394\n",
      "====> Epoch: 716 Average loss: 1.6623\n",
      "====> Epoch: 718 Average loss: 1.6924\n",
      "====> Epoch: 720 Average loss: 1.6584\n",
      "====> Epoch: 722 Average loss: 1.7294\n",
      "====> Epoch: 724 Average loss: 1.7086\n",
      "====> Epoch: 726 Average loss: 1.6463\n",
      "====> Epoch: 728 Average loss: 1.7167\n",
      "====> Epoch: 730 Average loss: 1.7040\n",
      "====> Epoch: 732 Average loss: 1.7001\n",
      "====> Epoch: 734 Average loss: 1.7032\n",
      "====> Epoch: 736 Average loss: 1.6901\n",
      "====> Epoch: 738 Average loss: 1.6902\n",
      "====> Epoch: 740 Average loss: 1.6850\n",
      "====> Epoch: 742 Average loss: 1.7123\n",
      "====> Epoch: 744 Average loss: 1.7041\n",
      "====> Epoch: 746 Average loss: 1.6572\n",
      "====> Epoch: 748 Average loss: 1.7103\n",
      "====> Epoch: 750 Average loss: 1.6953\n",
      "====> Epoch: 752 Average loss: 1.7822\n",
      "====> Epoch: 754 Average loss: 1.7445\n",
      "====> Epoch: 756 Average loss: 1.6919\n",
      "====> Epoch: 758 Average loss: 1.6277\n",
      "====> Epoch: 760 Average loss: 1.7048\n",
      "====> Epoch: 762 Average loss: 1.6909\n",
      "====> Epoch: 764 Average loss: 1.6701\n",
      "====> Epoch: 766 Average loss: 1.6452\n",
      "====> Epoch: 768 Average loss: 1.6916\n",
      "====> Epoch: 770 Average loss: 1.6789\n",
      "====> Epoch: 772 Average loss: 1.6499\n",
      "====> Epoch: 774 Average loss: 1.6849\n",
      "====> Epoch: 776 Average loss: 1.6661\n",
      "====> Epoch: 778 Average loss: 1.7347\n",
      "====> Epoch: 780 Average loss: 1.7136\n",
      "====> Epoch: 782 Average loss: 1.6392\n",
      "====> Epoch: 784 Average loss: 1.6832\n",
      "====> Epoch: 786 Average loss: 1.7032\n",
      "====> Epoch: 788 Average loss: 1.6816\n",
      "====> Epoch: 790 Average loss: 1.6644\n",
      "====> Epoch: 792 Average loss: 1.6941\n",
      "====> Epoch: 794 Average loss: 1.6853\n",
      "====> Epoch: 796 Average loss: 1.6612\n",
      "====> Epoch: 798 Average loss: 1.6929\n",
      "====> Epoch: 800 Average loss: 1.6388\n",
      "====> Epoch: 802 Average loss: 1.6406\n",
      "====> Epoch: 804 Average loss: 1.7209\n",
      "====> Epoch: 806 Average loss: 1.6433\n",
      "====> Epoch: 808 Average loss: 1.6534\n",
      "====> Epoch: 810 Average loss: 1.6528\n",
      "====> Epoch: 812 Average loss: 1.6692\n",
      "====> Epoch: 814 Average loss: 1.7043\n",
      "====> Epoch: 816 Average loss: 1.6930\n",
      "====> Epoch: 818 Average loss: 1.6743\n",
      "====> Epoch: 820 Average loss: 1.6861\n",
      "====> Epoch: 822 Average loss: 1.6519\n",
      "====> Epoch: 824 Average loss: 1.6652\n",
      "====> Epoch: 826 Average loss: 1.7021\n",
      "====> Epoch: 828 Average loss: 1.6403\n",
      "====> Epoch: 830 Average loss: 1.6471\n",
      "====> Epoch: 832 Average loss: 1.6936\n",
      "====> Epoch: 834 Average loss: 1.6759\n",
      "====> Epoch: 836 Average loss: 1.6675\n",
      "====> Epoch: 838 Average loss: 1.6851\n",
      "====> Epoch: 840 Average loss: 1.6075\n",
      "====> Epoch: 842 Average loss: 1.7165\n",
      "====> Epoch: 844 Average loss: 1.6646\n",
      "====> Epoch: 846 Average loss: 1.7138\n",
      "====> Epoch: 848 Average loss: 1.6867\n",
      "====> Epoch: 850 Average loss: 1.7296\n",
      "====> Epoch: 852 Average loss: 1.6692\n",
      "====> Epoch: 854 Average loss: 1.6110\n",
      "====> Epoch: 856 Average loss: 1.6765\n",
      "====> Epoch: 858 Average loss: 1.6199\n",
      "====> Epoch: 860 Average loss: 1.6699\n",
      "====> Epoch: 862 Average loss: 1.6750\n",
      "====> Epoch: 864 Average loss: 1.6980\n",
      "====> Epoch: 866 Average loss: 1.6672\n",
      "====> Epoch: 868 Average loss: 1.7162\n",
      "====> Epoch: 870 Average loss: 1.7061\n",
      "====> Epoch: 872 Average loss: 1.6577\n",
      "====> Epoch: 874 Average loss: 1.6802\n",
      "====> Epoch: 876 Average loss: 1.6276\n",
      "====> Epoch: 878 Average loss: 1.6891\n",
      "====> Epoch: 880 Average loss: 1.7076\n",
      "====> Epoch: 882 Average loss: 1.6973\n",
      "====> Epoch: 884 Average loss: 1.6389\n",
      "====> Epoch: 886 Average loss: 1.7004\n",
      "====> Epoch: 888 Average loss: 1.6355\n",
      "====> Epoch: 890 Average loss: 1.7071\n",
      "====> Epoch: 892 Average loss: 1.6231\n",
      "====> Epoch: 894 Average loss: 1.6891\n",
      "====> Epoch: 896 Average loss: 1.7217\n",
      "====> Epoch: 898 Average loss: 1.6459\n",
      "====> Epoch: 900 Average loss: 1.6441\n",
      "====> Epoch: 902 Average loss: 1.7007\n",
      "====> Epoch: 904 Average loss: 1.6114\n",
      "====> Epoch: 906 Average loss: 1.6361\n",
      "====> Epoch: 908 Average loss: 1.7044\n",
      "====> Epoch: 910 Average loss: 1.6948\n",
      "====> Epoch: 912 Average loss: 1.6509\n",
      "====> Epoch: 914 Average loss: 1.6875\n",
      "====> Epoch: 916 Average loss: 1.6270\n",
      "====> Epoch: 918 Average loss: 1.6614\n",
      "====> Epoch: 920 Average loss: 1.6017\n",
      "====> Epoch: 922 Average loss: 1.6530\n",
      "====> Epoch: 924 Average loss: 1.6821\n",
      "====> Epoch: 926 Average loss: 1.6165\n",
      "====> Epoch: 928 Average loss: 1.6767\n",
      "====> Epoch: 930 Average loss: 1.6551\n",
      "====> Epoch: 932 Average loss: 1.6826\n",
      "====> Epoch: 934 Average loss: 1.6733\n",
      "====> Epoch: 936 Average loss: 1.7372\n",
      "====> Epoch: 938 Average loss: 1.6575\n",
      "====> Epoch: 940 Average loss: 1.6718\n",
      "====> Epoch: 942 Average loss: 1.6693\n",
      "====> Epoch: 944 Average loss: 1.7000\n",
      "====> Epoch: 946 Average loss: 1.6204\n",
      "====> Epoch: 948 Average loss: 1.6551\n",
      "====> Epoch: 950 Average loss: 1.6795\n",
      "====> Epoch: 952 Average loss: 1.6281\n",
      "====> Epoch: 954 Average loss: 1.6848\n",
      "====> Epoch: 956 Average loss: 1.7242\n",
      "====> Epoch: 958 Average loss: 1.7185\n",
      "====> Epoch: 960 Average loss: 1.6604\n",
      "====> Epoch: 962 Average loss: 1.6941\n",
      "====> Epoch: 964 Average loss: 1.6445\n",
      "====> Epoch: 966 Average loss: 1.6577\n",
      "====> Epoch: 968 Average loss: 1.6880\n",
      "====> Epoch: 970 Average loss: 1.6571\n",
      "====> Epoch: 972 Average loss: 1.6741\n",
      "====> Epoch: 974 Average loss: 1.7195\n",
      "====> Epoch: 976 Average loss: 1.7176\n",
      "====> Epoch: 978 Average loss: 1.7022\n",
      "====> Epoch: 980 Average loss: 1.6914\n",
      "====> Epoch: 982 Average loss: 1.6751\n",
      "====> Epoch: 984 Average loss: 1.6428\n",
      "====> Epoch: 986 Average loss: 1.7034\n",
      "====> Epoch: 988 Average loss: 1.6750\n",
      "====> Epoch: 990 Average loss: 1.7349\n",
      "====> Epoch: 992 Average loss: 1.6608\n",
      "====> Epoch: 994 Average loss: 1.6655\n",
      "====> Epoch: 996 Average loss: 1.6712\n",
      "====> Epoch: 998 Average loss: 1.6692\n",
      "====> Epoch: 1000 Average loss: 1.7366\n"
     ]
    }
   ],
   "source": [
    "model_c1, dataset = train_encoder(\"client_n_data/diabetes_clients/client_1.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_client_n_for_diabetes_ds(model, source='client_n_data/diabetes_clients/client_1.csv', path=\"client_n_data/latent/client_1.csv\"):\n",
    "    DATA_PATH = source\n",
    "    df = load_data_v2(DATA_PATH, sep=\",\")\n",
    "    actual_data = df[0]\n",
    "    outcomes = df[2]\n",
    "\n",
    "    latents = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, e in enumerate(actual_data):\n",
    "            sample = e.unsqueeze(0)  # Add batch dimension\n",
    "            latent = model.embed(sample)  # Get the latent representation\n",
    "            latents.append(latent.squeeze().cpu().numpy())\n",
    "\n",
    "    latents_df = pd.DataFrame(latents)\n",
    "    outcomes_df = pd.DataFrame(outcomes)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_client_n_for_diabetes_ds(model_c1, 'client_n_data/diabetes_clients/client_1.csv', 'client_n_data/latent/client_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 3.9282\n",
      "====> Epoch: 4 Average loss: 3.3327\n",
      "====> Epoch: 6 Average loss: 2.8338\n",
      "====> Epoch: 8 Average loss: 2.5488\n",
      "====> Epoch: 10 Average loss: 2.3846\n",
      "====> Epoch: 12 Average loss: 2.1060\n",
      "====> Epoch: 14 Average loss: 1.9519\n",
      "====> Epoch: 16 Average loss: 1.8267\n",
      "====> Epoch: 18 Average loss: 1.8156\n",
      "====> Epoch: 20 Average loss: 1.8492\n",
      "====> Epoch: 22 Average loss: 1.8044\n",
      "====> Epoch: 24 Average loss: 1.7568\n",
      "====> Epoch: 26 Average loss: 1.7837\n",
      "====> Epoch: 28 Average loss: 1.7341\n",
      "====> Epoch: 30 Average loss: 1.7869\n",
      "====> Epoch: 32 Average loss: 1.6593\n",
      "====> Epoch: 34 Average loss: 1.7508\n",
      "====> Epoch: 36 Average loss: 1.6803\n",
      "====> Epoch: 38 Average loss: 1.6675\n",
      "====> Epoch: 40 Average loss: 1.7030\n",
      "====> Epoch: 42 Average loss: 1.6460\n",
      "====> Epoch: 44 Average loss: 1.7300\n",
      "====> Epoch: 46 Average loss: 1.6988\n",
      "====> Epoch: 48 Average loss: 1.6349\n",
      "====> Epoch: 50 Average loss: 1.7173\n",
      "====> Epoch: 52 Average loss: 1.7015\n",
      "====> Epoch: 54 Average loss: 1.7105\n",
      "====> Epoch: 56 Average loss: 1.6696\n",
      "====> Epoch: 58 Average loss: 1.6451\n",
      "====> Epoch: 60 Average loss: 1.7201\n",
      "====> Epoch: 62 Average loss: 1.6634\n",
      "====> Epoch: 64 Average loss: 1.7246\n",
      "====> Epoch: 66 Average loss: 1.6966\n",
      "====> Epoch: 68 Average loss: 1.6320\n",
      "====> Epoch: 70 Average loss: 1.6926\n",
      "====> Epoch: 72 Average loss: 1.6328\n",
      "====> Epoch: 74 Average loss: 1.7237\n",
      "====> Epoch: 76 Average loss: 1.6676\n",
      "====> Epoch: 78 Average loss: 1.7059\n",
      "====> Epoch: 80 Average loss: 1.6937\n",
      "====> Epoch: 82 Average loss: 1.6402\n",
      "====> Epoch: 84 Average loss: 1.6473\n",
      "====> Epoch: 86 Average loss: 1.7029\n",
      "====> Epoch: 88 Average loss: 1.7039\n",
      "====> Epoch: 90 Average loss: 1.7056\n",
      "====> Epoch: 92 Average loss: 1.7695\n",
      "====> Epoch: 94 Average loss: 1.6432\n",
      "====> Epoch: 96 Average loss: 1.6670\n",
      "====> Epoch: 98 Average loss: 1.6840\n",
      "====> Epoch: 100 Average loss: 1.6470\n",
      "====> Epoch: 102 Average loss: 1.6592\n",
      "====> Epoch: 104 Average loss: 1.6598\n",
      "====> Epoch: 106 Average loss: 1.6223\n",
      "====> Epoch: 108 Average loss: 1.6297\n",
      "====> Epoch: 110 Average loss: 1.6462\n",
      "====> Epoch: 112 Average loss: 1.6970\n",
      "====> Epoch: 114 Average loss: 1.6305\n",
      "====> Epoch: 116 Average loss: 1.6832\n",
      "====> Epoch: 118 Average loss: 1.6137\n",
      "====> Epoch: 120 Average loss: 1.6550\n",
      "====> Epoch: 122 Average loss: 1.6520\n",
      "====> Epoch: 124 Average loss: 1.6202\n",
      "====> Epoch: 126 Average loss: 1.6197\n",
      "====> Epoch: 128 Average loss: 1.6180\n",
      "====> Epoch: 130 Average loss: 1.5952\n",
      "====> Epoch: 132 Average loss: 1.6419\n",
      "====> Epoch: 134 Average loss: 1.6655\n",
      "====> Epoch: 136 Average loss: 1.5979\n",
      "====> Epoch: 138 Average loss: 1.6185\n",
      "====> Epoch: 140 Average loss: 1.6658\n",
      "====> Epoch: 142 Average loss: 1.6765\n",
      "====> Epoch: 144 Average loss: 1.6122\n",
      "====> Epoch: 146 Average loss: 1.6498\n",
      "====> Epoch: 148 Average loss: 1.6097\n",
      "====> Epoch: 150 Average loss: 1.5757\n",
      "====> Epoch: 152 Average loss: 1.6556\n",
      "====> Epoch: 154 Average loss: 1.6510\n",
      "====> Epoch: 156 Average loss: 1.6069\n",
      "====> Epoch: 158 Average loss: 1.6477\n",
      "====> Epoch: 160 Average loss: 1.5772\n",
      "====> Epoch: 162 Average loss: 1.6768\n",
      "====> Epoch: 164 Average loss: 1.6400\n",
      "====> Epoch: 166 Average loss: 1.5921\n",
      "====> Epoch: 168 Average loss: 1.6032\n",
      "====> Epoch: 170 Average loss: 1.6918\n",
      "====> Epoch: 172 Average loss: 1.6052\n",
      "====> Epoch: 174 Average loss: 1.6623\n",
      "====> Epoch: 176 Average loss: 1.6436\n",
      "====> Epoch: 178 Average loss: 1.5787\n",
      "====> Epoch: 180 Average loss: 1.6337\n",
      "====> Epoch: 182 Average loss: 1.6546\n",
      "====> Epoch: 184 Average loss: 1.6100\n",
      "====> Epoch: 186 Average loss: 1.6666\n",
      "====> Epoch: 188 Average loss: 1.6053\n",
      "====> Epoch: 190 Average loss: 1.6150\n",
      "====> Epoch: 192 Average loss: 1.5807\n",
      "====> Epoch: 194 Average loss: 1.6285\n",
      "====> Epoch: 196 Average loss: 1.6654\n",
      "====> Epoch: 198 Average loss: 1.6267\n",
      "====> Epoch: 200 Average loss: 1.6284\n",
      "====> Epoch: 202 Average loss: 1.6436\n",
      "====> Epoch: 204 Average loss: 1.6114\n",
      "====> Epoch: 206 Average loss: 1.6144\n",
      "====> Epoch: 208 Average loss: 1.6035\n",
      "====> Epoch: 210 Average loss: 1.6185\n",
      "====> Epoch: 212 Average loss: 1.5883\n",
      "====> Epoch: 214 Average loss: 1.5981\n",
      "====> Epoch: 216 Average loss: 1.5528\n",
      "====> Epoch: 218 Average loss: 1.6023\n",
      "====> Epoch: 220 Average loss: 1.6554\n",
      "====> Epoch: 222 Average loss: 1.6225\n",
      "====> Epoch: 224 Average loss: 1.5601\n",
      "====> Epoch: 226 Average loss: 1.5784\n",
      "====> Epoch: 228 Average loss: 1.5857\n",
      "====> Epoch: 230 Average loss: 1.6237\n",
      "====> Epoch: 232 Average loss: 1.6328\n",
      "====> Epoch: 234 Average loss: 1.6927\n",
      "====> Epoch: 236 Average loss: 1.5723\n",
      "====> Epoch: 238 Average loss: 1.5411\n",
      "====> Epoch: 240 Average loss: 1.6094\n",
      "====> Epoch: 242 Average loss: 1.5943\n",
      "====> Epoch: 244 Average loss: 1.6585\n",
      "====> Epoch: 246 Average loss: 1.6333\n",
      "====> Epoch: 248 Average loss: 1.5668\n",
      "====> Epoch: 250 Average loss: 1.5457\n",
      "====> Epoch: 252 Average loss: 1.6111\n",
      "====> Epoch: 254 Average loss: 1.6291\n",
      "====> Epoch: 256 Average loss: 1.6282\n",
      "====> Epoch: 258 Average loss: 1.5854\n",
      "====> Epoch: 260 Average loss: 1.5839\n",
      "====> Epoch: 262 Average loss: 1.5987\n",
      "====> Epoch: 264 Average loss: 1.6189\n",
      "====> Epoch: 266 Average loss: 1.6406\n",
      "====> Epoch: 268 Average loss: 1.5988\n",
      "====> Epoch: 270 Average loss: 1.6409\n",
      "====> Epoch: 272 Average loss: 1.6109\n",
      "====> Epoch: 274 Average loss: 1.6612\n",
      "====> Epoch: 276 Average loss: 1.5853\n",
      "====> Epoch: 278 Average loss: 1.5952\n",
      "====> Epoch: 280 Average loss: 1.6292\n",
      "====> Epoch: 282 Average loss: 1.5989\n",
      "====> Epoch: 284 Average loss: 1.6095\n",
      "====> Epoch: 286 Average loss: 1.6295\n",
      "====> Epoch: 288 Average loss: 1.5459\n",
      "====> Epoch: 290 Average loss: 1.6047\n",
      "====> Epoch: 292 Average loss: 1.6023\n",
      "====> Epoch: 294 Average loss: 1.5932\n",
      "====> Epoch: 296 Average loss: 1.5896\n",
      "====> Epoch: 298 Average loss: 1.5649\n",
      "====> Epoch: 300 Average loss: 1.6150\n",
      "====> Epoch: 302 Average loss: 1.5869\n",
      "====> Epoch: 304 Average loss: 1.5972\n",
      "====> Epoch: 306 Average loss: 1.5830\n",
      "====> Epoch: 308 Average loss: 1.6861\n",
      "====> Epoch: 310 Average loss: 1.5889\n",
      "====> Epoch: 312 Average loss: 1.6129\n",
      "====> Epoch: 314 Average loss: 1.5851\n",
      "====> Epoch: 316 Average loss: 1.5598\n",
      "====> Epoch: 318 Average loss: 1.6730\n",
      "====> Epoch: 320 Average loss: 1.6232\n",
      "====> Epoch: 322 Average loss: 1.5530\n",
      "====> Epoch: 324 Average loss: 1.5659\n",
      "====> Epoch: 326 Average loss: 1.5744\n",
      "====> Epoch: 328 Average loss: 1.5875\n",
      "====> Epoch: 330 Average loss: 1.6499\n",
      "====> Epoch: 332 Average loss: 1.6310\n",
      "====> Epoch: 334 Average loss: 1.6186\n",
      "====> Epoch: 336 Average loss: 1.6515\n",
      "====> Epoch: 338 Average loss: 1.5893\n",
      "====> Epoch: 340 Average loss: 1.5629\n",
      "====> Epoch: 342 Average loss: 1.6346\n",
      "====> Epoch: 344 Average loss: 1.5802\n",
      "====> Epoch: 346 Average loss: 1.5431\n",
      "====> Epoch: 348 Average loss: 1.5961\n",
      "====> Epoch: 350 Average loss: 1.6534\n",
      "====> Epoch: 352 Average loss: 1.6268\n",
      "====> Epoch: 354 Average loss: 1.5908\n",
      "====> Epoch: 356 Average loss: 1.5690\n",
      "====> Epoch: 358 Average loss: 1.5962\n",
      "====> Epoch: 360 Average loss: 1.6062\n",
      "====> Epoch: 362 Average loss: 1.6715\n",
      "====> Epoch: 364 Average loss: 1.5913\n",
      "====> Epoch: 366 Average loss: 1.5906\n",
      "====> Epoch: 368 Average loss: 1.6219\n",
      "====> Epoch: 370 Average loss: 1.6514\n",
      "====> Epoch: 372 Average loss: 1.5200\n",
      "====> Epoch: 374 Average loss: 1.5928\n",
      "====> Epoch: 376 Average loss: 1.6357\n",
      "====> Epoch: 378 Average loss: 1.6177\n",
      "====> Epoch: 380 Average loss: 1.5883\n",
      "====> Epoch: 382 Average loss: 1.6447\n",
      "====> Epoch: 384 Average loss: 1.5573\n",
      "====> Epoch: 386 Average loss: 1.6382\n",
      "====> Epoch: 388 Average loss: 1.5559\n",
      "====> Epoch: 390 Average loss: 1.5924\n",
      "====> Epoch: 392 Average loss: 1.5866\n",
      "====> Epoch: 394 Average loss: 1.5850\n",
      "====> Epoch: 396 Average loss: 1.5971\n",
      "====> Epoch: 398 Average loss: 1.5616\n",
      "====> Epoch: 400 Average loss: 1.5166\n",
      "====> Epoch: 402 Average loss: 1.6034\n",
      "====> Epoch: 404 Average loss: 1.6655\n",
      "====> Epoch: 406 Average loss: 1.5679\n",
      "====> Epoch: 408 Average loss: 1.5684\n",
      "====> Epoch: 410 Average loss: 1.6482\n",
      "====> Epoch: 412 Average loss: 1.5255\n",
      "====> Epoch: 414 Average loss: 1.6795\n",
      "====> Epoch: 416 Average loss: 1.5970\n",
      "====> Epoch: 418 Average loss: 1.5954\n",
      "====> Epoch: 420 Average loss: 1.6144\n",
      "====> Epoch: 422 Average loss: 1.5673\n",
      "====> Epoch: 424 Average loss: 1.5709\n",
      "====> Epoch: 426 Average loss: 1.5420\n",
      "====> Epoch: 428 Average loss: 1.5704\n",
      "====> Epoch: 430 Average loss: 1.6226\n",
      "====> Epoch: 432 Average loss: 1.6277\n",
      "====> Epoch: 434 Average loss: 1.6099\n",
      "====> Epoch: 436 Average loss: 1.6025\n",
      "====> Epoch: 438 Average loss: 1.5722\n",
      "====> Epoch: 440 Average loss: 1.5962\n",
      "====> Epoch: 442 Average loss: 1.5586\n",
      "====> Epoch: 444 Average loss: 1.6346\n",
      "====> Epoch: 446 Average loss: 1.5992\n",
      "====> Epoch: 448 Average loss: 1.6168\n",
      "====> Epoch: 450 Average loss: 1.6088\n",
      "====> Epoch: 452 Average loss: 1.5965\n",
      "====> Epoch: 454 Average loss: 1.5574\n",
      "====> Epoch: 456 Average loss: 1.5223\n",
      "====> Epoch: 458 Average loss: 1.6044\n",
      "====> Epoch: 460 Average loss: 1.5259\n",
      "====> Epoch: 462 Average loss: 1.5732\n",
      "====> Epoch: 464 Average loss: 1.6604\n",
      "====> Epoch: 466 Average loss: 1.5619\n",
      "====> Epoch: 468 Average loss: 1.5650\n",
      "====> Epoch: 470 Average loss: 1.5542\n",
      "====> Epoch: 472 Average loss: 1.5644\n",
      "====> Epoch: 474 Average loss: 1.5957\n",
      "====> Epoch: 476 Average loss: 1.6060\n",
      "====> Epoch: 478 Average loss: 1.5943\n",
      "====> Epoch: 480 Average loss: 1.5723\n",
      "====> Epoch: 482 Average loss: 1.6049\n",
      "====> Epoch: 484 Average loss: 1.5699\n",
      "====> Epoch: 486 Average loss: 1.5516\n",
      "====> Epoch: 488 Average loss: 1.5892\n",
      "====> Epoch: 490 Average loss: 1.5733\n",
      "====> Epoch: 492 Average loss: 1.5745\n",
      "====> Epoch: 494 Average loss: 1.6626\n",
      "====> Epoch: 496 Average loss: 1.5294\n",
      "====> Epoch: 498 Average loss: 1.5491\n",
      "====> Epoch: 500 Average loss: 1.5825\n",
      "====> Epoch: 502 Average loss: 1.5527\n",
      "====> Epoch: 504 Average loss: 1.5718\n",
      "====> Epoch: 506 Average loss: 1.5464\n",
      "====> Epoch: 508 Average loss: 1.6258\n",
      "====> Epoch: 510 Average loss: 1.6331\n",
      "====> Epoch: 512 Average loss: 1.5889\n",
      "====> Epoch: 514 Average loss: 1.5539\n",
      "====> Epoch: 516 Average loss: 1.5528\n",
      "====> Epoch: 518 Average loss: 1.6016\n",
      "====> Epoch: 520 Average loss: 1.5663\n",
      "====> Epoch: 522 Average loss: 1.5454\n",
      "====> Epoch: 524 Average loss: 1.6191\n",
      "====> Epoch: 526 Average loss: 1.5492\n",
      "====> Epoch: 528 Average loss: 1.5939\n",
      "====> Epoch: 530 Average loss: 1.6157\n",
      "====> Epoch: 532 Average loss: 1.5643\n",
      "====> Epoch: 534 Average loss: 1.5753\n",
      "====> Epoch: 536 Average loss: 1.5753\n",
      "====> Epoch: 538 Average loss: 1.5958\n",
      "====> Epoch: 540 Average loss: 1.5886\n",
      "====> Epoch: 542 Average loss: 1.6092\n",
      "====> Epoch: 544 Average loss: 1.5980\n",
      "====> Epoch: 546 Average loss: 1.5479\n",
      "====> Epoch: 548 Average loss: 1.5537\n",
      "====> Epoch: 550 Average loss: 1.6201\n",
      "====> Epoch: 552 Average loss: 1.5900\n",
      "====> Epoch: 554 Average loss: 1.6126\n",
      "====> Epoch: 556 Average loss: 1.6312\n",
      "====> Epoch: 558 Average loss: 1.5738\n",
      "====> Epoch: 560 Average loss: 1.5588\n",
      "====> Epoch: 562 Average loss: 1.6128\n",
      "====> Epoch: 564 Average loss: 1.6201\n",
      "====> Epoch: 566 Average loss: 1.6393\n",
      "====> Epoch: 568 Average loss: 1.5792\n",
      "====> Epoch: 570 Average loss: 1.5654\n",
      "====> Epoch: 572 Average loss: 1.5973\n",
      "====> Epoch: 574 Average loss: 1.5490\n",
      "====> Epoch: 576 Average loss: 1.6132\n",
      "====> Epoch: 578 Average loss: 1.5356\n",
      "====> Epoch: 580 Average loss: 1.6200\n",
      "====> Epoch: 582 Average loss: 1.6157\n",
      "====> Epoch: 584 Average loss: 1.5836\n",
      "====> Epoch: 586 Average loss: 1.5633\n",
      "====> Epoch: 588 Average loss: 1.6346\n",
      "====> Epoch: 590 Average loss: 1.5885\n",
      "====> Epoch: 592 Average loss: 1.6032\n",
      "====> Epoch: 594 Average loss: 1.5872\n",
      "====> Epoch: 596 Average loss: 1.5983\n",
      "====> Epoch: 598 Average loss: 1.6338\n",
      "====> Epoch: 600 Average loss: 1.5792\n",
      "====> Epoch: 602 Average loss: 1.6254\n",
      "====> Epoch: 604 Average loss: 1.5965\n",
      "====> Epoch: 606 Average loss: 1.5455\n",
      "====> Epoch: 608 Average loss: 1.6547\n",
      "====> Epoch: 610 Average loss: 1.5694\n",
      "====> Epoch: 612 Average loss: 1.6501\n",
      "====> Epoch: 614 Average loss: 1.6282\n",
      "====> Epoch: 616 Average loss: 1.5536\n",
      "====> Epoch: 618 Average loss: 1.5818\n",
      "====> Epoch: 620 Average loss: 1.6066\n",
      "====> Epoch: 622 Average loss: 1.5578\n",
      "====> Epoch: 624 Average loss: 1.6360\n",
      "====> Epoch: 626 Average loss: 1.5853\n",
      "====> Epoch: 628 Average loss: 1.5927\n",
      "====> Epoch: 630 Average loss: 1.6538\n",
      "====> Epoch: 632 Average loss: 1.5521\n",
      "====> Epoch: 634 Average loss: 1.5926\n",
      "====> Epoch: 636 Average loss: 1.5688\n",
      "====> Epoch: 638 Average loss: 1.5306\n",
      "====> Epoch: 640 Average loss: 1.5924\n",
      "====> Epoch: 642 Average loss: 1.6195\n",
      "====> Epoch: 644 Average loss: 1.5732\n",
      "====> Epoch: 646 Average loss: 1.5858\n",
      "====> Epoch: 648 Average loss: 1.5811\n",
      "====> Epoch: 650 Average loss: 1.5867\n",
      "====> Epoch: 652 Average loss: 1.6450\n",
      "====> Epoch: 654 Average loss: 1.5979\n",
      "====> Epoch: 656 Average loss: 1.6099\n",
      "====> Epoch: 658 Average loss: 1.6201\n",
      "====> Epoch: 660 Average loss: 1.6002\n",
      "====> Epoch: 662 Average loss: 1.5566\n",
      "====> Epoch: 664 Average loss: 1.6018\n",
      "====> Epoch: 666 Average loss: 1.5892\n",
      "====> Epoch: 668 Average loss: 1.5771\n",
      "====> Epoch: 670 Average loss: 1.6391\n",
      "====> Epoch: 672 Average loss: 1.6050\n",
      "====> Epoch: 674 Average loss: 1.6410\n",
      "====> Epoch: 676 Average loss: 1.5963\n",
      "====> Epoch: 678 Average loss: 1.5543\n",
      "====> Epoch: 680 Average loss: 1.5657\n",
      "====> Epoch: 682 Average loss: 1.5614\n",
      "====> Epoch: 684 Average loss: 1.5437\n",
      "====> Epoch: 686 Average loss: 1.5570\n",
      "====> Epoch: 688 Average loss: 1.5696\n",
      "====> Epoch: 690 Average loss: 1.5854\n",
      "====> Epoch: 692 Average loss: 1.5897\n",
      "====> Epoch: 694 Average loss: 1.5648\n",
      "====> Epoch: 696 Average loss: 1.6264\n",
      "====> Epoch: 698 Average loss: 1.6182\n",
      "====> Epoch: 700 Average loss: 1.5476\n",
      "====> Epoch: 702 Average loss: 1.5476\n",
      "====> Epoch: 704 Average loss: 1.5962\n",
      "====> Epoch: 706 Average loss: 1.5555\n",
      "====> Epoch: 708 Average loss: 1.5171\n",
      "====> Epoch: 710 Average loss: 1.5337\n",
      "====> Epoch: 712 Average loss: 1.5705\n",
      "====> Epoch: 714 Average loss: 1.6066\n",
      "====> Epoch: 716 Average loss: 1.5226\n",
      "====> Epoch: 718 Average loss: 1.5938\n",
      "====> Epoch: 720 Average loss: 1.6192\n",
      "====> Epoch: 722 Average loss: 1.5472\n",
      "====> Epoch: 724 Average loss: 1.6228\n",
      "====> Epoch: 726 Average loss: 1.5641\n",
      "====> Epoch: 728 Average loss: 1.5614\n",
      "====> Epoch: 730 Average loss: 1.5946\n",
      "====> Epoch: 732 Average loss: 1.5542\n",
      "====> Epoch: 734 Average loss: 1.5566\n",
      "====> Epoch: 736 Average loss: 1.5416\n",
      "====> Epoch: 738 Average loss: 1.6105\n",
      "====> Epoch: 740 Average loss: 1.5343\n",
      "====> Epoch: 742 Average loss: 1.5455\n",
      "====> Epoch: 744 Average loss: 1.5887\n",
      "====> Epoch: 746 Average loss: 1.5754\n",
      "====> Epoch: 748 Average loss: 1.5766\n",
      "====> Epoch: 750 Average loss: 1.5699\n",
      "====> Epoch: 752 Average loss: 1.6082\n",
      "====> Epoch: 754 Average loss: 1.6265\n",
      "====> Epoch: 756 Average loss: 1.6074\n",
      "====> Epoch: 758 Average loss: 1.5495\n",
      "====> Epoch: 760 Average loss: 1.6153\n",
      "====> Epoch: 762 Average loss: 1.6358\n",
      "====> Epoch: 764 Average loss: 1.5628\n",
      "====> Epoch: 766 Average loss: 1.5979\n",
      "====> Epoch: 768 Average loss: 1.5348\n",
      "====> Epoch: 770 Average loss: 1.6097\n",
      "====> Epoch: 772 Average loss: 1.5765\n",
      "====> Epoch: 774 Average loss: 1.5041\n",
      "====> Epoch: 776 Average loss: 1.4991\n",
      "====> Epoch: 778 Average loss: 1.5426\n",
      "====> Epoch: 780 Average loss: 1.5849\n",
      "====> Epoch: 782 Average loss: 1.6324\n",
      "====> Epoch: 784 Average loss: 1.5478\n",
      "====> Epoch: 786 Average loss: 1.5439\n",
      "====> Epoch: 788 Average loss: 1.5748\n",
      "====> Epoch: 790 Average loss: 1.5586\n",
      "====> Epoch: 792 Average loss: 1.5205\n",
      "====> Epoch: 794 Average loss: 1.5787\n",
      "====> Epoch: 796 Average loss: 1.5296\n",
      "====> Epoch: 798 Average loss: 1.5829\n",
      "====> Epoch: 800 Average loss: 1.5321\n",
      "====> Epoch: 802 Average loss: 1.5912\n",
      "====> Epoch: 804 Average loss: 1.5753\n",
      "====> Epoch: 806 Average loss: 1.6210\n",
      "====> Epoch: 808 Average loss: 1.5587\n",
      "====> Epoch: 810 Average loss: 1.5113\n",
      "====> Epoch: 812 Average loss: 1.5142\n",
      "====> Epoch: 814 Average loss: 1.5699\n",
      "====> Epoch: 816 Average loss: 1.5298\n",
      "====> Epoch: 818 Average loss: 1.6239\n",
      "====> Epoch: 820 Average loss: 1.5779\n",
      "====> Epoch: 822 Average loss: 1.5905\n",
      "====> Epoch: 824 Average loss: 1.5232\n",
      "====> Epoch: 826 Average loss: 1.4977\n",
      "====> Epoch: 828 Average loss: 1.5760\n",
      "====> Epoch: 830 Average loss: 1.5913\n",
      "====> Epoch: 832 Average loss: 1.5484\n",
      "====> Epoch: 834 Average loss: 1.6626\n",
      "====> Epoch: 836 Average loss: 1.5553\n",
      "====> Epoch: 838 Average loss: 1.5183\n",
      "====> Epoch: 840 Average loss: 1.5840\n",
      "====> Epoch: 842 Average loss: 1.5116\n",
      "====> Epoch: 844 Average loss: 1.5794\n",
      "====> Epoch: 846 Average loss: 1.5318\n",
      "====> Epoch: 848 Average loss: 1.5412\n",
      "====> Epoch: 850 Average loss: 1.5977\n",
      "====> Epoch: 852 Average loss: 1.6192\n",
      "====> Epoch: 854 Average loss: 1.6105\n",
      "====> Epoch: 856 Average loss: 1.5609\n",
      "====> Epoch: 858 Average loss: 1.5629\n",
      "====> Epoch: 860 Average loss: 1.5040\n",
      "====> Epoch: 862 Average loss: 1.5360\n",
      "====> Epoch: 864 Average loss: 1.5910\n",
      "====> Epoch: 866 Average loss: 1.5406\n",
      "====> Epoch: 868 Average loss: 1.5739\n",
      "====> Epoch: 870 Average loss: 1.5973\n",
      "====> Epoch: 872 Average loss: 1.5610\n",
      "====> Epoch: 874 Average loss: 1.5592\n",
      "====> Epoch: 876 Average loss: 1.5415\n",
      "====> Epoch: 878 Average loss: 1.5926\n",
      "====> Epoch: 880 Average loss: 1.5645\n",
      "====> Epoch: 882 Average loss: 1.5261\n",
      "====> Epoch: 884 Average loss: 1.5824\n",
      "====> Epoch: 886 Average loss: 1.6081\n",
      "====> Epoch: 888 Average loss: 1.5296\n",
      "====> Epoch: 890 Average loss: 1.5447\n",
      "====> Epoch: 892 Average loss: 1.5507\n",
      "====> Epoch: 894 Average loss: 1.5542\n",
      "====> Epoch: 896 Average loss: 1.5784\n",
      "====> Epoch: 898 Average loss: 1.6048\n",
      "====> Epoch: 900 Average loss: 1.5529\n",
      "====> Epoch: 902 Average loss: 1.5883\n",
      "====> Epoch: 904 Average loss: 1.5420\n",
      "====> Epoch: 906 Average loss: 1.5518\n",
      "====> Epoch: 908 Average loss: 1.5791\n",
      "====> Epoch: 910 Average loss: 1.5609\n",
      "====> Epoch: 912 Average loss: 1.5061\n",
      "====> Epoch: 914 Average loss: 1.5629\n",
      "====> Epoch: 916 Average loss: 1.5694\n",
      "====> Epoch: 918 Average loss: 1.5451\n",
      "====> Epoch: 920 Average loss: 1.5500\n",
      "====> Epoch: 922 Average loss: 1.6084\n",
      "====> Epoch: 924 Average loss: 1.5292\n",
      "====> Epoch: 926 Average loss: 1.5559\n",
      "====> Epoch: 928 Average loss: 1.6206\n",
      "====> Epoch: 930 Average loss: 1.5494\n",
      "====> Epoch: 932 Average loss: 1.5556\n",
      "====> Epoch: 934 Average loss: 1.5868\n",
      "====> Epoch: 936 Average loss: 1.5615\n",
      "====> Epoch: 938 Average loss: 1.5189\n",
      "====> Epoch: 940 Average loss: 1.5322\n",
      "====> Epoch: 942 Average loss: 1.5323\n",
      "====> Epoch: 944 Average loss: 1.6128\n",
      "====> Epoch: 946 Average loss: 1.5555\n",
      "====> Epoch: 948 Average loss: 1.5983\n",
      "====> Epoch: 950 Average loss: 1.5964\n",
      "====> Epoch: 952 Average loss: 1.6073\n",
      "====> Epoch: 954 Average loss: 1.5657\n",
      "====> Epoch: 956 Average loss: 1.5953\n",
      "====> Epoch: 958 Average loss: 1.5428\n",
      "====> Epoch: 960 Average loss: 1.5527\n",
      "====> Epoch: 962 Average loss: 1.5547\n",
      "====> Epoch: 964 Average loss: 1.5244\n",
      "====> Epoch: 966 Average loss: 1.6352\n",
      "====> Epoch: 968 Average loss: 1.5600\n",
      "====> Epoch: 970 Average loss: 1.5811\n",
      "====> Epoch: 972 Average loss: 1.5695\n",
      "====> Epoch: 974 Average loss: 1.5637\n",
      "====> Epoch: 976 Average loss: 1.6125\n",
      "====> Epoch: 978 Average loss: 1.6151\n",
      "====> Epoch: 980 Average loss: 1.5729\n",
      "====> Epoch: 982 Average loss: 1.5889\n",
      "====> Epoch: 984 Average loss: 1.5166\n",
      "====> Epoch: 986 Average loss: 1.5099\n",
      "====> Epoch: 988 Average loss: 1.5395\n",
      "====> Epoch: 990 Average loss: 1.5317\n",
      "====> Epoch: 992 Average loss: 1.5898\n",
      "====> Epoch: 994 Average loss: 1.5662\n",
      "====> Epoch: 996 Average loss: 1.5412\n",
      "====> Epoch: 998 Average loss: 1.5651\n",
      "====> Epoch: 1000 Average loss: 1.5579\n"
     ]
    }
   ],
   "source": [
    "model_c2, standardizer_c2 = train_encoder(\"client_n_data/diabetes_clients/client_2.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_client_n_for_diabetes_ds(model_c2, 'client_n_data/diabetes_clients/client_2.csv', 'client_n_data/latent/client_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 7.9732\n",
      "====> Epoch: 4 Average loss: 7.4029\n",
      "====> Epoch: 6 Average loss: 6.6994\n",
      "====> Epoch: 8 Average loss: 5.3698\n",
      "====> Epoch: 10 Average loss: 4.8490\n",
      "====> Epoch: 12 Average loss: 4.4077\n",
      "====> Epoch: 14 Average loss: 4.1814\n",
      "====> Epoch: 16 Average loss: 4.0216\n",
      "====> Epoch: 18 Average loss: 3.9864\n",
      "====> Epoch: 20 Average loss: 3.9269\n",
      "====> Epoch: 22 Average loss: 3.8874\n",
      "====> Epoch: 24 Average loss: 3.7569\n",
      "====> Epoch: 26 Average loss: 3.7931\n",
      "====> Epoch: 28 Average loss: 3.7413\n",
      "====> Epoch: 30 Average loss: 3.7414\n",
      "====> Epoch: 32 Average loss: 3.6490\n",
      "====> Epoch: 34 Average loss: 3.6011\n",
      "====> Epoch: 36 Average loss: 3.6879\n",
      "====> Epoch: 38 Average loss: 3.6091\n",
      "====> Epoch: 40 Average loss: 3.5919\n",
      "====> Epoch: 42 Average loss: 3.5029\n",
      "====> Epoch: 44 Average loss: 3.5298\n",
      "====> Epoch: 46 Average loss: 3.4747\n",
      "====> Epoch: 48 Average loss: 3.4911\n",
      "====> Epoch: 50 Average loss: 3.5061\n",
      "====> Epoch: 52 Average loss: 3.4882\n",
      "====> Epoch: 54 Average loss: 3.4882\n",
      "====> Epoch: 56 Average loss: 3.4231\n",
      "====> Epoch: 58 Average loss: 3.4703\n",
      "====> Epoch: 60 Average loss: 3.4802\n",
      "====> Epoch: 62 Average loss: 3.4063\n",
      "====> Epoch: 64 Average loss: 3.4602\n",
      "====> Epoch: 66 Average loss: 3.4241\n",
      "====> Epoch: 68 Average loss: 3.4695\n",
      "====> Epoch: 70 Average loss: 3.4018\n",
      "====> Epoch: 72 Average loss: 3.3962\n",
      "====> Epoch: 74 Average loss: 3.3732\n",
      "====> Epoch: 76 Average loss: 3.3913\n",
      "====> Epoch: 78 Average loss: 3.3888\n",
      "====> Epoch: 80 Average loss: 3.2977\n",
      "====> Epoch: 82 Average loss: 3.3902\n",
      "====> Epoch: 84 Average loss: 3.4131\n",
      "====> Epoch: 86 Average loss: 3.3480\n",
      "====> Epoch: 88 Average loss: 3.4293\n",
      "====> Epoch: 90 Average loss: 3.2864\n",
      "====> Epoch: 92 Average loss: 3.3025\n",
      "====> Epoch: 94 Average loss: 3.3193\n",
      "====> Epoch: 96 Average loss: 3.3605\n",
      "====> Epoch: 98 Average loss: 3.4305\n",
      "====> Epoch: 100 Average loss: 3.3251\n",
      "====> Epoch: 102 Average loss: 3.4103\n",
      "====> Epoch: 104 Average loss: 3.3025\n",
      "====> Epoch: 106 Average loss: 3.3297\n",
      "====> Epoch: 108 Average loss: 3.2919\n",
      "====> Epoch: 110 Average loss: 3.3713\n",
      "====> Epoch: 112 Average loss: 3.3496\n",
      "====> Epoch: 114 Average loss: 3.3700\n",
      "====> Epoch: 116 Average loss: 3.3084\n",
      "====> Epoch: 118 Average loss: 3.3965\n",
      "====> Epoch: 120 Average loss: 3.3542\n",
      "====> Epoch: 122 Average loss: 3.2446\n",
      "====> Epoch: 124 Average loss: 3.3130\n",
      "====> Epoch: 126 Average loss: 3.3480\n",
      "====> Epoch: 128 Average loss: 3.2386\n",
      "====> Epoch: 130 Average loss: 3.3690\n",
      "====> Epoch: 132 Average loss: 3.2287\n",
      "====> Epoch: 134 Average loss: 3.2170\n",
      "====> Epoch: 136 Average loss: 3.3132\n",
      "====> Epoch: 138 Average loss: 3.2630\n",
      "====> Epoch: 140 Average loss: 3.2369\n",
      "====> Epoch: 142 Average loss: 3.2032\n",
      "====> Epoch: 144 Average loss: 3.2374\n",
      "====> Epoch: 146 Average loss: 3.2519\n",
      "====> Epoch: 148 Average loss: 3.3106\n",
      "====> Epoch: 150 Average loss: 3.2933\n",
      "====> Epoch: 152 Average loss: 3.2464\n",
      "====> Epoch: 154 Average loss: 3.2439\n",
      "====> Epoch: 156 Average loss: 3.2638\n",
      "====> Epoch: 158 Average loss: 3.2639\n",
      "====> Epoch: 160 Average loss: 3.2005\n",
      "====> Epoch: 162 Average loss: 3.1943\n",
      "====> Epoch: 164 Average loss: 3.2915\n",
      "====> Epoch: 166 Average loss: 3.2934\n",
      "====> Epoch: 168 Average loss: 3.2311\n",
      "====> Epoch: 170 Average loss: 3.2725\n",
      "====> Epoch: 172 Average loss: 3.3105\n",
      "====> Epoch: 174 Average loss: 3.1723\n",
      "====> Epoch: 176 Average loss: 3.2515\n",
      "====> Epoch: 178 Average loss: 3.2974\n",
      "====> Epoch: 180 Average loss: 3.2289\n",
      "====> Epoch: 182 Average loss: 3.2030\n",
      "====> Epoch: 184 Average loss: 3.2705\n",
      "====> Epoch: 186 Average loss: 3.2364\n",
      "====> Epoch: 188 Average loss: 3.3040\n",
      "====> Epoch: 190 Average loss: 3.2212\n",
      "====> Epoch: 192 Average loss: 3.3054\n",
      "====> Epoch: 194 Average loss: 3.1912\n",
      "====> Epoch: 196 Average loss: 3.1626\n",
      "====> Epoch: 198 Average loss: 3.2295\n",
      "====> Epoch: 200 Average loss: 3.2045\n",
      "====> Epoch: 202 Average loss: 3.2055\n",
      "====> Epoch: 204 Average loss: 3.2756\n",
      "====> Epoch: 206 Average loss: 3.2227\n",
      "====> Epoch: 208 Average loss: 3.1990\n",
      "====> Epoch: 210 Average loss: 3.2309\n",
      "====> Epoch: 212 Average loss: 3.1900\n",
      "====> Epoch: 214 Average loss: 3.2284\n",
      "====> Epoch: 216 Average loss: 3.1642\n",
      "====> Epoch: 218 Average loss: 3.1942\n",
      "====> Epoch: 220 Average loss: 3.2701\n",
      "====> Epoch: 222 Average loss: 3.2392\n",
      "====> Epoch: 224 Average loss: 3.2678\n",
      "====> Epoch: 226 Average loss: 3.1859\n",
      "====> Epoch: 228 Average loss: 3.2437\n",
      "====> Epoch: 230 Average loss: 3.2386\n",
      "====> Epoch: 232 Average loss: 3.2355\n",
      "====> Epoch: 234 Average loss: 3.1471\n",
      "====> Epoch: 236 Average loss: 3.2245\n",
      "====> Epoch: 238 Average loss: 3.2210\n",
      "====> Epoch: 240 Average loss: 3.2476\n",
      "====> Epoch: 242 Average loss: 3.1744\n",
      "====> Epoch: 244 Average loss: 3.1837\n",
      "====> Epoch: 246 Average loss: 3.2185\n",
      "====> Epoch: 248 Average loss: 3.1811\n",
      "====> Epoch: 250 Average loss: 3.2708\n",
      "====> Epoch: 252 Average loss: 3.2638\n",
      "====> Epoch: 254 Average loss: 3.2518\n",
      "====> Epoch: 256 Average loss: 3.2733\n",
      "====> Epoch: 258 Average loss: 3.2741\n",
      "====> Epoch: 260 Average loss: 3.2151\n",
      "====> Epoch: 262 Average loss: 3.2299\n",
      "====> Epoch: 264 Average loss: 3.2037\n",
      "====> Epoch: 266 Average loss: 3.1971\n",
      "====> Epoch: 268 Average loss: 3.2682\n",
      "====> Epoch: 270 Average loss: 3.2150\n",
      "====> Epoch: 272 Average loss: 3.2420\n",
      "====> Epoch: 274 Average loss: 3.1682\n",
      "====> Epoch: 276 Average loss: 3.2177\n",
      "====> Epoch: 278 Average loss: 3.1570\n",
      "====> Epoch: 280 Average loss: 3.2421\n",
      "====> Epoch: 282 Average loss: 3.2047\n",
      "====> Epoch: 284 Average loss: 3.2114\n",
      "====> Epoch: 286 Average loss: 3.2268\n",
      "====> Epoch: 288 Average loss: 3.2214\n",
      "====> Epoch: 290 Average loss: 3.2499\n",
      "====> Epoch: 292 Average loss: 3.1688\n",
      "====> Epoch: 294 Average loss: 3.1797\n",
      "====> Epoch: 296 Average loss: 3.2160\n",
      "====> Epoch: 298 Average loss: 3.1865\n",
      "====> Epoch: 300 Average loss: 3.1568\n",
      "====> Epoch: 302 Average loss: 3.1908\n",
      "====> Epoch: 304 Average loss: 3.2717\n",
      "====> Epoch: 306 Average loss: 3.1996\n",
      "====> Epoch: 308 Average loss: 3.2318\n",
      "====> Epoch: 310 Average loss: 3.1883\n",
      "====> Epoch: 312 Average loss: 3.2625\n",
      "====> Epoch: 314 Average loss: 3.1750\n",
      "====> Epoch: 316 Average loss: 3.1871\n",
      "====> Epoch: 318 Average loss: 3.1931\n",
      "====> Epoch: 320 Average loss: 3.1706\n",
      "====> Epoch: 322 Average loss: 3.1657\n",
      "====> Epoch: 324 Average loss: 3.1799\n",
      "====> Epoch: 326 Average loss: 3.1827\n",
      "====> Epoch: 328 Average loss: 3.1648\n",
      "====> Epoch: 330 Average loss: 3.1709\n",
      "====> Epoch: 332 Average loss: 3.2676\n",
      "====> Epoch: 334 Average loss: 3.1923\n",
      "====> Epoch: 336 Average loss: 3.2353\n",
      "====> Epoch: 338 Average loss: 3.1589\n",
      "====> Epoch: 340 Average loss: 3.1411\n",
      "====> Epoch: 342 Average loss: 3.2206\n",
      "====> Epoch: 344 Average loss: 3.2350\n",
      "====> Epoch: 346 Average loss: 3.2027\n",
      "====> Epoch: 348 Average loss: 3.2342\n",
      "====> Epoch: 350 Average loss: 3.1786\n",
      "====> Epoch: 352 Average loss: 3.1723\n",
      "====> Epoch: 354 Average loss: 3.2361\n",
      "====> Epoch: 356 Average loss: 3.2617\n",
      "====> Epoch: 358 Average loss: 3.1849\n",
      "====> Epoch: 360 Average loss: 3.1816\n",
      "====> Epoch: 362 Average loss: 3.2560\n",
      "====> Epoch: 364 Average loss: 3.2048\n",
      "====> Epoch: 366 Average loss: 3.2191\n",
      "====> Epoch: 368 Average loss: 3.1889\n",
      "====> Epoch: 370 Average loss: 3.1645\n",
      "====> Epoch: 372 Average loss: 3.1604\n",
      "====> Epoch: 374 Average loss: 3.1696\n",
      "====> Epoch: 376 Average loss: 3.1989\n",
      "====> Epoch: 378 Average loss: 3.2231\n",
      "====> Epoch: 380 Average loss: 3.1877\n",
      "====> Epoch: 382 Average loss: 3.1596\n",
      "====> Epoch: 384 Average loss: 3.1738\n",
      "====> Epoch: 386 Average loss: 3.1695\n",
      "====> Epoch: 388 Average loss: 3.1423\n",
      "====> Epoch: 390 Average loss: 3.2442\n",
      "====> Epoch: 392 Average loss: 3.2362\n",
      "====> Epoch: 394 Average loss: 3.1580\n",
      "====> Epoch: 396 Average loss: 3.2696\n",
      "====> Epoch: 398 Average loss: 3.1522\n",
      "====> Epoch: 400 Average loss: 3.0777\n",
      "====> Epoch: 402 Average loss: 3.1948\n",
      "====> Epoch: 404 Average loss: 3.2339\n",
      "====> Epoch: 406 Average loss: 3.1663\n",
      "====> Epoch: 408 Average loss: 3.2229\n",
      "====> Epoch: 410 Average loss: 3.1716\n",
      "====> Epoch: 412 Average loss: 3.2316\n",
      "====> Epoch: 414 Average loss: 3.2306\n",
      "====> Epoch: 416 Average loss: 3.1122\n",
      "====> Epoch: 418 Average loss: 3.1420\n",
      "====> Epoch: 420 Average loss: 3.1791\n",
      "====> Epoch: 422 Average loss: 3.1643\n",
      "====> Epoch: 424 Average loss: 3.1244\n",
      "====> Epoch: 426 Average loss: 3.1462\n",
      "====> Epoch: 428 Average loss: 3.1559\n",
      "====> Epoch: 430 Average loss: 3.1770\n",
      "====> Epoch: 432 Average loss: 3.2407\n",
      "====> Epoch: 434 Average loss: 3.2145\n",
      "====> Epoch: 436 Average loss: 3.1925\n",
      "====> Epoch: 438 Average loss: 3.1426\n",
      "====> Epoch: 440 Average loss: 3.1858\n",
      "====> Epoch: 442 Average loss: 3.2324\n",
      "====> Epoch: 444 Average loss: 3.1493\n",
      "====> Epoch: 446 Average loss: 3.1497\n",
      "====> Epoch: 448 Average loss: 3.1680\n",
      "====> Epoch: 450 Average loss: 3.1290\n",
      "====> Epoch: 452 Average loss: 3.2240\n",
      "====> Epoch: 454 Average loss: 3.1904\n",
      "====> Epoch: 456 Average loss: 3.1915\n",
      "====> Epoch: 458 Average loss: 3.2461\n",
      "====> Epoch: 460 Average loss: 3.1129\n",
      "====> Epoch: 462 Average loss: 3.1795\n",
      "====> Epoch: 464 Average loss: 3.1824\n",
      "====> Epoch: 466 Average loss: 3.2346\n",
      "====> Epoch: 468 Average loss: 3.2329\n",
      "====> Epoch: 470 Average loss: 3.1468\n",
      "====> Epoch: 472 Average loss: 3.2062\n",
      "====> Epoch: 474 Average loss: 3.2098\n",
      "====> Epoch: 476 Average loss: 3.2010\n",
      "====> Epoch: 478 Average loss: 3.2037\n",
      "====> Epoch: 480 Average loss: 3.1636\n",
      "====> Epoch: 482 Average loss: 3.2366\n",
      "====> Epoch: 484 Average loss: 3.1574\n",
      "====> Epoch: 486 Average loss: 3.1047\n",
      "====> Epoch: 488 Average loss: 3.2235\n",
      "====> Epoch: 490 Average loss: 3.1039\n",
      "====> Epoch: 492 Average loss: 3.1822\n",
      "====> Epoch: 494 Average loss: 3.1728\n",
      "====> Epoch: 496 Average loss: 3.2197\n",
      "====> Epoch: 498 Average loss: 3.1906\n",
      "====> Epoch: 500 Average loss: 3.1642\n",
      "====> Epoch: 502 Average loss: 3.1087\n",
      "====> Epoch: 504 Average loss: 3.1744\n",
      "====> Epoch: 506 Average loss: 3.2097\n",
      "====> Epoch: 508 Average loss: 3.2026\n",
      "====> Epoch: 510 Average loss: 3.2067\n",
      "====> Epoch: 512 Average loss: 3.1683\n",
      "====> Epoch: 514 Average loss: 3.1189\n",
      "====> Epoch: 516 Average loss: 3.2015\n",
      "====> Epoch: 518 Average loss: 3.1692\n",
      "====> Epoch: 520 Average loss: 3.2007\n",
      "====> Epoch: 522 Average loss: 3.1211\n",
      "====> Epoch: 524 Average loss: 3.1214\n",
      "====> Epoch: 526 Average loss: 3.1770\n",
      "====> Epoch: 528 Average loss: 3.1701\n",
      "====> Epoch: 530 Average loss: 3.1496\n",
      "====> Epoch: 532 Average loss: 3.1713\n",
      "====> Epoch: 534 Average loss: 3.1828\n",
      "====> Epoch: 536 Average loss: 3.2047\n",
      "====> Epoch: 538 Average loss: 3.1673\n",
      "====> Epoch: 540 Average loss: 3.1552\n",
      "====> Epoch: 542 Average loss: 3.2149\n",
      "====> Epoch: 544 Average loss: 3.1535\n",
      "====> Epoch: 546 Average loss: 3.2058\n",
      "====> Epoch: 548 Average loss: 3.1933\n",
      "====> Epoch: 550 Average loss: 3.1573\n",
      "====> Epoch: 552 Average loss: 3.1650\n",
      "====> Epoch: 554 Average loss: 3.1212\n",
      "====> Epoch: 556 Average loss: 3.1799\n",
      "====> Epoch: 558 Average loss: 3.1746\n",
      "====> Epoch: 560 Average loss: 3.1983\n",
      "====> Epoch: 562 Average loss: 3.1598\n",
      "====> Epoch: 564 Average loss: 3.1586\n",
      "====> Epoch: 566 Average loss: 3.2204\n",
      "====> Epoch: 568 Average loss: 3.2035\n",
      "====> Epoch: 570 Average loss: 3.1267\n",
      "====> Epoch: 572 Average loss: 3.2111\n",
      "====> Epoch: 574 Average loss: 3.1780\n",
      "====> Epoch: 576 Average loss: 3.1937\n",
      "====> Epoch: 578 Average loss: 3.1679\n",
      "====> Epoch: 580 Average loss: 3.1628\n",
      "====> Epoch: 582 Average loss: 3.1842\n",
      "====> Epoch: 584 Average loss: 3.1982\n",
      "====> Epoch: 586 Average loss: 3.1531\n",
      "====> Epoch: 588 Average loss: 3.2400\n",
      "====> Epoch: 590 Average loss: 3.1762\n",
      "====> Epoch: 592 Average loss: 3.1661\n",
      "====> Epoch: 594 Average loss: 3.1995\n",
      "====> Epoch: 596 Average loss: 3.1195\n",
      "====> Epoch: 598 Average loss: 3.2014\n",
      "====> Epoch: 600 Average loss: 3.1536\n",
      "====> Epoch: 602 Average loss: 3.1949\n",
      "====> Epoch: 604 Average loss: 3.1511\n",
      "====> Epoch: 606 Average loss: 3.1107\n",
      "====> Epoch: 608 Average loss: 3.1865\n",
      "====> Epoch: 610 Average loss: 3.1917\n",
      "====> Epoch: 612 Average loss: 3.1278\n",
      "====> Epoch: 614 Average loss: 3.1836\n",
      "====> Epoch: 616 Average loss: 3.1460\n",
      "====> Epoch: 618 Average loss: 3.1435\n",
      "====> Epoch: 620 Average loss: 3.1607\n",
      "====> Epoch: 622 Average loss: 3.1547\n",
      "====> Epoch: 624 Average loss: 3.1675\n",
      "====> Epoch: 626 Average loss: 3.1721\n",
      "====> Epoch: 628 Average loss: 3.2003\n",
      "====> Epoch: 630 Average loss: 3.1357\n",
      "====> Epoch: 632 Average loss: 3.1534\n",
      "====> Epoch: 634 Average loss: 3.2291\n",
      "====> Epoch: 636 Average loss: 3.1436\n",
      "====> Epoch: 638 Average loss: 3.0706\n",
      "====> Epoch: 640 Average loss: 3.1053\n",
      "====> Epoch: 642 Average loss: 3.1878\n",
      "====> Epoch: 644 Average loss: 3.1730\n",
      "====> Epoch: 646 Average loss: 3.1775\n",
      "====> Epoch: 648 Average loss: 3.2024\n",
      "====> Epoch: 650 Average loss: 3.1942\n",
      "====> Epoch: 652 Average loss: 3.1190\n",
      "====> Epoch: 654 Average loss: 3.1491\n",
      "====> Epoch: 656 Average loss: 3.1095\n",
      "====> Epoch: 658 Average loss: 3.1558\n",
      "====> Epoch: 660 Average loss: 3.1616\n",
      "====> Epoch: 662 Average loss: 3.1540\n",
      "====> Epoch: 664 Average loss: 3.0743\n",
      "====> Epoch: 666 Average loss: 3.1318\n",
      "====> Epoch: 668 Average loss: 3.1176\n",
      "====> Epoch: 670 Average loss: 3.1915\n",
      "====> Epoch: 672 Average loss: 3.1562\n",
      "====> Epoch: 674 Average loss: 3.1888\n",
      "====> Epoch: 676 Average loss: 3.1391\n",
      "====> Epoch: 678 Average loss: 3.1488\n",
      "====> Epoch: 680 Average loss: 3.1355\n",
      "====> Epoch: 682 Average loss: 3.1927\n",
      "====> Epoch: 684 Average loss: 3.1847\n",
      "====> Epoch: 686 Average loss: 3.1843\n",
      "====> Epoch: 688 Average loss: 3.1351\n",
      "====> Epoch: 690 Average loss: 3.1952\n",
      "====> Epoch: 692 Average loss: 3.1500\n",
      "====> Epoch: 694 Average loss: 3.1327\n",
      "====> Epoch: 696 Average loss: 3.0927\n",
      "====> Epoch: 698 Average loss: 3.1602\n",
      "====> Epoch: 700 Average loss: 3.0894\n",
      "====> Epoch: 702 Average loss: 3.2098\n",
      "====> Epoch: 704 Average loss: 3.1740\n",
      "====> Epoch: 706 Average loss: 3.1728\n",
      "====> Epoch: 708 Average loss: 3.1696\n",
      "====> Epoch: 710 Average loss: 3.1710\n",
      "====> Epoch: 712 Average loss: 3.1164\n",
      "====> Epoch: 714 Average loss: 3.1693\n",
      "====> Epoch: 716 Average loss: 3.1861\n",
      "====> Epoch: 718 Average loss: 3.1149\n",
      "====> Epoch: 720 Average loss: 3.1257\n",
      "====> Epoch: 722 Average loss: 3.1584\n",
      "====> Epoch: 724 Average loss: 3.1082\n",
      "====> Epoch: 726 Average loss: 3.1762\n",
      "====> Epoch: 728 Average loss: 3.1615\n",
      "====> Epoch: 730 Average loss: 3.1818\n",
      "====> Epoch: 732 Average loss: 3.1818\n",
      "====> Epoch: 734 Average loss: 3.1308\n",
      "====> Epoch: 736 Average loss: 3.1731\n",
      "====> Epoch: 738 Average loss: 3.2083\n",
      "====> Epoch: 740 Average loss: 3.2320\n",
      "====> Epoch: 742 Average loss: 3.1950\n",
      "====> Epoch: 744 Average loss: 3.1726\n",
      "====> Epoch: 746 Average loss: 3.1807\n",
      "====> Epoch: 748 Average loss: 3.1393\n",
      "====> Epoch: 750 Average loss: 3.1978\n",
      "====> Epoch: 752 Average loss: 3.1321\n",
      "====> Epoch: 754 Average loss: 3.1720\n",
      "====> Epoch: 756 Average loss: 3.1630\n",
      "====> Epoch: 758 Average loss: 3.1747\n",
      "====> Epoch: 760 Average loss: 3.1346\n",
      "====> Epoch: 762 Average loss: 3.1690\n",
      "====> Epoch: 764 Average loss: 3.1322\n",
      "====> Epoch: 766 Average loss: 3.2498\n",
      "====> Epoch: 768 Average loss: 3.1558\n",
      "====> Epoch: 770 Average loss: 3.1611\n",
      "====> Epoch: 772 Average loss: 3.1908\n",
      "====> Epoch: 774 Average loss: 3.1647\n",
      "====> Epoch: 776 Average loss: 3.1952\n",
      "====> Epoch: 778 Average loss: 3.1726\n",
      "====> Epoch: 780 Average loss: 3.1632\n",
      "====> Epoch: 782 Average loss: 3.2031\n",
      "====> Epoch: 784 Average loss: 3.1414\n",
      "====> Epoch: 786 Average loss: 3.2103\n",
      "====> Epoch: 788 Average loss: 3.1381\n",
      "====> Epoch: 790 Average loss: 3.1715\n",
      "====> Epoch: 792 Average loss: 3.1180\n",
      "====> Epoch: 794 Average loss: 3.1388\n",
      "====> Epoch: 796 Average loss: 3.2129\n",
      "====> Epoch: 798 Average loss: 3.1577\n",
      "====> Epoch: 800 Average loss: 3.1027\n",
      "====> Epoch: 802 Average loss: 3.1205\n",
      "====> Epoch: 804 Average loss: 3.1428\n",
      "====> Epoch: 806 Average loss: 3.1322\n",
      "====> Epoch: 808 Average loss: 3.1887\n",
      "====> Epoch: 810 Average loss: 3.1300\n",
      "====> Epoch: 812 Average loss: 3.1879\n",
      "====> Epoch: 814 Average loss: 3.1336\n",
      "====> Epoch: 816 Average loss: 3.1302\n",
      "====> Epoch: 818 Average loss: 3.1058\n",
      "====> Epoch: 820 Average loss: 3.1704\n",
      "====> Epoch: 822 Average loss: 3.2820\n",
      "====> Epoch: 824 Average loss: 3.1587\n",
      "====> Epoch: 826 Average loss: 3.1620\n",
      "====> Epoch: 828 Average loss: 3.1520\n",
      "====> Epoch: 830 Average loss: 3.1099\n",
      "====> Epoch: 832 Average loss: 3.1244\n",
      "====> Epoch: 834 Average loss: 3.1331\n",
      "====> Epoch: 836 Average loss: 3.1921\n",
      "====> Epoch: 838 Average loss: 3.1133\n",
      "====> Epoch: 840 Average loss: 3.1190\n",
      "====> Epoch: 842 Average loss: 3.1816\n",
      "====> Epoch: 844 Average loss: 3.1383\n",
      "====> Epoch: 846 Average loss: 3.1569\n",
      "====> Epoch: 848 Average loss: 3.1234\n",
      "====> Epoch: 850 Average loss: 3.1567\n",
      "====> Epoch: 852 Average loss: 3.1744\n",
      "====> Epoch: 854 Average loss: 3.1316\n",
      "====> Epoch: 856 Average loss: 3.2371\n",
      "====> Epoch: 858 Average loss: 3.1361\n",
      "====> Epoch: 860 Average loss: 3.1371\n",
      "====> Epoch: 862 Average loss: 3.0784\n",
      "====> Epoch: 864 Average loss: 3.1686\n",
      "====> Epoch: 866 Average loss: 3.0946\n",
      "====> Epoch: 868 Average loss: 3.1918\n",
      "====> Epoch: 870 Average loss: 3.1926\n",
      "====> Epoch: 872 Average loss: 3.1979\n",
      "====> Epoch: 874 Average loss: 3.1516\n",
      "====> Epoch: 876 Average loss: 3.0764\n",
      "====> Epoch: 878 Average loss: 3.1393\n",
      "====> Epoch: 880 Average loss: 3.1660\n",
      "====> Epoch: 882 Average loss: 3.0950\n",
      "====> Epoch: 884 Average loss: 3.1449\n",
      "====> Epoch: 886 Average loss: 3.2072\n",
      "====> Epoch: 888 Average loss: 3.1716\n",
      "====> Epoch: 890 Average loss: 3.1921\n",
      "====> Epoch: 892 Average loss: 3.1052\n",
      "====> Epoch: 894 Average loss: 3.1320\n",
      "====> Epoch: 896 Average loss: 3.1906\n",
      "====> Epoch: 898 Average loss: 3.1207\n",
      "====> Epoch: 900 Average loss: 3.2196\n",
      "====> Epoch: 902 Average loss: 3.1353\n",
      "====> Epoch: 904 Average loss: 3.1448\n",
      "====> Epoch: 906 Average loss: 3.1186\n",
      "====> Epoch: 908 Average loss: 3.1608\n",
      "====> Epoch: 910 Average loss: 3.2768\n",
      "====> Epoch: 912 Average loss: 3.1645\n",
      "====> Epoch: 914 Average loss: 3.1492\n",
      "====> Epoch: 916 Average loss: 3.1421\n",
      "====> Epoch: 918 Average loss: 3.1588\n",
      "====> Epoch: 920 Average loss: 3.0762\n",
      "====> Epoch: 922 Average loss: 3.1641\n",
      "====> Epoch: 924 Average loss: 3.1504\n",
      "====> Epoch: 926 Average loss: 3.1158\n",
      "====> Epoch: 928 Average loss: 3.1685\n",
      "====> Epoch: 930 Average loss: 3.0967\n",
      "====> Epoch: 932 Average loss: 3.1457\n",
      "====> Epoch: 934 Average loss: 3.0747\n",
      "====> Epoch: 936 Average loss: 3.1027\n",
      "====> Epoch: 938 Average loss: 3.0964\n",
      "====> Epoch: 940 Average loss: 3.1708\n",
      "====> Epoch: 942 Average loss: 3.1794\n",
      "====> Epoch: 944 Average loss: 3.1861\n",
      "====> Epoch: 946 Average loss: 3.1403\n",
      "====> Epoch: 948 Average loss: 3.1131\n",
      "====> Epoch: 950 Average loss: 3.1336\n",
      "====> Epoch: 952 Average loss: 3.1143\n",
      "====> Epoch: 954 Average loss: 3.1266\n",
      "====> Epoch: 956 Average loss: 3.1336\n",
      "====> Epoch: 958 Average loss: 3.0914\n",
      "====> Epoch: 960 Average loss: 3.1099\n",
      "====> Epoch: 962 Average loss: 3.1463\n",
      "====> Epoch: 964 Average loss: 3.1105\n",
      "====> Epoch: 966 Average loss: 3.1010\n",
      "====> Epoch: 968 Average loss: 3.1916\n",
      "====> Epoch: 970 Average loss: 3.1769\n",
      "====> Epoch: 972 Average loss: 3.1087\n",
      "====> Epoch: 974 Average loss: 3.1560\n",
      "====> Epoch: 976 Average loss: 3.1214\n",
      "====> Epoch: 978 Average loss: 3.1186\n",
      "====> Epoch: 980 Average loss: 3.1392\n",
      "====> Epoch: 982 Average loss: 3.1389\n",
      "====> Epoch: 984 Average loss: 3.1721\n",
      "====> Epoch: 986 Average loss: 3.1175\n",
      "====> Epoch: 988 Average loss: 3.1619\n",
      "====> Epoch: 990 Average loss: 3.0809\n",
      "====> Epoch: 992 Average loss: 3.1932\n",
      "====> Epoch: 994 Average loss: 3.1087\n",
      "====> Epoch: 996 Average loss: 3.1475\n",
      "====> Epoch: 998 Average loss: 3.1345\n",
      "====> Epoch: 1000 Average loss: 3.0646\n"
     ]
    }
   ],
   "source": [
    "model_c3, standardizer_c3 = train_encoder(\"client_n_data/diabetes_clients/client_3.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_client_n_for_diabetes_ds(model_c3, 'client_n_data/diabetes_clients/client_3.csv', 'client_n_data/latent/client_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Client Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client 1 Reconstruction Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_generated_latent = np.load(\"syn_data/syn_latent/diabetes generated/X_num_unnorm.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 9)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_generated_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_1_generated_latent = diabetes_generated_latent[:, 0:3]\n",
    "np.save('syn_data/syn_latent/diabetes generated/X_c1_unnorm.npy', client_1_generated_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_2_generated_latent = diabetes_generated_latent[:, 3:6]\n",
    "np.save('syn_data/syn_latent/diabetes generated/X_c2_unnorm.npy', client_2_generated_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_3_generated_latent =  diabetes_generated_latent[:, 6:9]\n",
    "np.save('syn_data/syn_latent/diabetes generated/X_c3_unnorm.npy', client_3_generated_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preg', 'plas'] []\n"
     ]
    }
   ],
   "source": [
    "x_client_1_syn = reconstruction_of_latent(model_c1, \"syn_data/syn_latent/diabetes generated/X_c1_unnorm.npy\", \"syn_data/syn_latent/diabetes generated/y_train.npy\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2)"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_client_1_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 2)"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_diabetes_c1_real = load_diabetes_data(\"client_n_data/diabetes_clients/client_1.csv\", \",\")\n",
    "x_diabetes_c1_real = x_diabetes_c1_real[1].inverse_transform(x_diabetes_c1_real[0].cpu().detach().numpy())\n",
    "x_diabetes_c1_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.9113609494611784\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_1_syn[:768], x_diabetes_c1_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3285598678261618"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(x_client_1_syn[:768], x_diabetes_c1_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client 2 Reconstruction Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pres', 'skin'] []\n"
     ]
    }
   ],
   "source": [
    "x_client_2_syn = reconstruction_of_latent(model_c2, \"syn_data/syn_latent/diabetes generated/X_c2_unnorm.npy\", \"syn_data/syn_latent/diabetes generated/y_train.npy\", standardizer_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 2)"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_diabetes_c2_real = load_diabetes_data(\"client_n_data/diabetes_clients/client_2.csv\", \",\")\n",
    "x_diabetes_c2_real = x_diabetes_c2_real[1].inverse_transform(x_diabetes_c2_real[0].cpu().detach().numpy())\n",
    "x_diabetes_c2_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8878641888259153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4493680841694591"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resemblance_measure(x_diabetes_c2_real, x_client_2_syn[:768])\n",
    "jensen_shannon_similarity(x_diabetes_c2_real, x_client_2_syn[:768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Client 3 resemblance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insu', 'mass', 'pedi', 'age'] []\n"
     ]
    }
   ],
   "source": [
    "x_client_3_syn = reconstruction_of_latent(model_c3, \"syn_data/syn_latent/diabetes generated/X_c3_unnorm.npy\", \"syn_data/syn_latent/diabetes generated/y_train.npy\", standardizer_c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 4)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_diabetes_c3_real = load_diabetes_data(\"client_n_data/diabetes_clients/client_3.csv\", \",\")\n",
    "x_diabetes_c3_real = x_diabetes_c3_real[1].inverse_transform(x_diabetes_c3_real[0].cpu().detach().numpy())\n",
    "x_diabetes_c3_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.6834076393911033\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_3_syn[:768], x_diabetes_c3_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3817520677806493"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(x_client_3_syn[:768], x_diabetes_c3_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Data Client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>92697</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>92037</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>93023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>90034</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>92612</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  \\\n",
       "0        1   25           1      49     91107       4    1.6          1   \n",
       "1        2   45          19      34     90089       3    1.5          1   \n",
       "2        3   39          15      11     94720       1    1.0          1   \n",
       "3        4   35           9     100     94112       1    2.7          2   \n",
       "4        5   35           8      45     91330       4    1.0          2   \n",
       "...    ...  ...         ...     ...       ...     ...    ...        ...   \n",
       "4995  4996   29           3      40     92697       1    1.9          3   \n",
       "4996  4997   30           4      15     92037       4    0.4          1   \n",
       "4997  4998   63          39      24     93023       2    0.3          3   \n",
       "4998  4999   65          40      49     90034       3    0.5          2   \n",
       "4999  5000   28           4      83     92612       3    0.8          1   \n",
       "\n",
       "      Mortgage  Personal Loan  Securities Account  CD Account  Online  \\\n",
       "0            0              0                   1           0       0   \n",
       "1            0              0                   1           0       0   \n",
       "2            0              0                   0           0       0   \n",
       "3            0              0                   0           0       0   \n",
       "4            0              0                   0           0       0   \n",
       "...        ...            ...                 ...         ...     ...   \n",
       "4995         0              0                   0           0       1   \n",
       "4996        85              0                   0           0       1   \n",
       "4997         0              0                   0           0       0   \n",
       "4998         0              0                   0           0       1   \n",
       "4999         0              0                   0           0       1   \n",
       "\n",
       "      CreditCard  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "4995           0  \n",
       "4996           0  \n",
       "4997           0  \n",
       "4998           0  \n",
       "4999           1  \n",
       "\n",
       "[5000 rows x 14 columns]"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_data_all_client = pd.read_csv(\"Data/bank.csv\")\n",
    "loan_data_all_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = loan_data_all_client.iloc[:, -1:]\n",
    "df_client_1 = loan_data_all_client.iloc[:, :4]\n",
    "df_client_2 = loan_data_all_client.iloc[:, 4:8]\n",
    "df_client_3 = loan_data_all_client.iloc[:, 8:13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_1 = pd.concat([df_client_1, df_label], axis=1)\n",
    "df_client_1.to_csv(\"client_n_data/bank_clients/bank_1.csv\", index=False)\n",
    "df_client_2 = pd.concat([df_client_2, df_label], axis=1)\n",
    "df_client_2.to_csv(\"client_n_data/bank_clients/bank_2.csv\", index=False)\n",
    "df_client_3 = pd.concat([df_client_3, df_label], axis=1)\n",
    "df_client_3.to_csv(\"client_n_data/bank_clients/bank_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 4.9692\n",
      "====> Epoch: 4 Average loss: 3.9265\n",
      "====> Epoch: 6 Average loss: 3.4615\n",
      "====> Epoch: 8 Average loss: 3.3442\n",
      "====> Epoch: 10 Average loss: 3.2889\n",
      "====> Epoch: 12 Average loss: 3.2613\n",
      "====> Epoch: 14 Average loss: 3.2300\n",
      "====> Epoch: 16 Average loss: 3.1994\n",
      "====> Epoch: 18 Average loss: 3.1300\n",
      "====> Epoch: 20 Average loss: 3.1381\n",
      "====> Epoch: 22 Average loss: 3.0808\n",
      "====> Epoch: 24 Average loss: 3.0939\n",
      "====> Epoch: 26 Average loss: 3.0972\n",
      "====> Epoch: 28 Average loss: 3.0837\n",
      "====> Epoch: 30 Average loss: 3.0672\n",
      "====> Epoch: 32 Average loss: 3.0700\n",
      "====> Epoch: 34 Average loss: 3.0944\n",
      "====> Epoch: 36 Average loss: 3.0786\n",
      "====> Epoch: 38 Average loss: 3.0689\n",
      "====> Epoch: 40 Average loss: 3.0782\n",
      "====> Epoch: 42 Average loss: 3.0759\n",
      "====> Epoch: 44 Average loss: 3.0483\n",
      "====> Epoch: 46 Average loss: 3.0619\n",
      "====> Epoch: 48 Average loss: 3.0768\n",
      "====> Epoch: 50 Average loss: 3.0688\n",
      "====> Epoch: 52 Average loss: 3.0540\n",
      "====> Epoch: 54 Average loss: 3.0623\n",
      "====> Epoch: 56 Average loss: 3.0592\n",
      "====> Epoch: 58 Average loss: 3.0611\n",
      "====> Epoch: 60 Average loss: 3.0525\n",
      "====> Epoch: 62 Average loss: 3.0677\n",
      "====> Epoch: 64 Average loss: 3.0500\n",
      "====> Epoch: 66 Average loss: 3.0292\n",
      "====> Epoch: 68 Average loss: 3.0528\n",
      "====> Epoch: 70 Average loss: 3.0409\n",
      "====> Epoch: 72 Average loss: 3.0674\n",
      "====> Epoch: 74 Average loss: 3.0607\n",
      "====> Epoch: 76 Average loss: 3.0598\n",
      "====> Epoch: 78 Average loss: 3.0617\n",
      "====> Epoch: 80 Average loss: 3.0571\n",
      "====> Epoch: 82 Average loss: 3.0666\n",
      "====> Epoch: 84 Average loss: 3.0645\n",
      "====> Epoch: 86 Average loss: 3.0687\n",
      "====> Epoch: 88 Average loss: 3.0478\n",
      "====> Epoch: 90 Average loss: 3.0317\n",
      "====> Epoch: 92 Average loss: 3.0958\n",
      "====> Epoch: 94 Average loss: 3.0674\n",
      "====> Epoch: 96 Average loss: 3.0671\n",
      "====> Epoch: 98 Average loss: 3.0848\n",
      "====> Epoch: 100 Average loss: 3.0607\n",
      "====> Epoch: 102 Average loss: 3.0550\n",
      "====> Epoch: 104 Average loss: 3.0738\n",
      "====> Epoch: 106 Average loss: 3.0523\n",
      "====> Epoch: 108 Average loss: 3.0602\n",
      "====> Epoch: 110 Average loss: 3.0635\n",
      "====> Epoch: 112 Average loss: 3.0802\n",
      "====> Epoch: 114 Average loss: 3.0497\n",
      "====> Epoch: 116 Average loss: 3.0677\n",
      "====> Epoch: 118 Average loss: 3.0488\n",
      "====> Epoch: 120 Average loss: 3.0675\n",
      "====> Epoch: 122 Average loss: 3.0646\n",
      "====> Epoch: 124 Average loss: 3.0887\n",
      "====> Epoch: 126 Average loss: 3.0861\n",
      "====> Epoch: 128 Average loss: 3.0601\n",
      "====> Epoch: 130 Average loss: 3.0440\n",
      "====> Epoch: 132 Average loss: 3.0535\n",
      "====> Epoch: 134 Average loss: 3.0324\n",
      "====> Epoch: 136 Average loss: 3.0674\n",
      "====> Epoch: 138 Average loss: 3.0664\n",
      "====> Epoch: 140 Average loss: 3.0683\n",
      "====> Epoch: 142 Average loss: 3.0460\n",
      "====> Epoch: 144 Average loss: 3.0654\n",
      "====> Epoch: 146 Average loss: 3.0861\n",
      "====> Epoch: 148 Average loss: 3.0431\n",
      "====> Epoch: 150 Average loss: 3.0474\n",
      "====> Epoch: 152 Average loss: 3.0406\n",
      "====> Epoch: 154 Average loss: 3.0644\n",
      "====> Epoch: 156 Average loss: 3.0488\n",
      "====> Epoch: 158 Average loss: 3.0518\n",
      "====> Epoch: 160 Average loss: 3.0637\n",
      "====> Epoch: 162 Average loss: 3.0798\n",
      "====> Epoch: 164 Average loss: 3.0541\n",
      "====> Epoch: 166 Average loss: 3.0820\n",
      "====> Epoch: 168 Average loss: 3.0711\n",
      "====> Epoch: 170 Average loss: 3.0562\n",
      "====> Epoch: 172 Average loss: 3.0498\n",
      "====> Epoch: 174 Average loss: 3.0368\n",
      "====> Epoch: 176 Average loss: 3.0524\n",
      "====> Epoch: 178 Average loss: 3.0650\n",
      "====> Epoch: 180 Average loss: 3.0613\n",
      "====> Epoch: 182 Average loss: 3.0343\n",
      "====> Epoch: 184 Average loss: 3.0394\n",
      "====> Epoch: 186 Average loss: 3.0697\n",
      "====> Epoch: 188 Average loss: 3.0322\n",
      "====> Epoch: 190 Average loss: 3.0608\n",
      "====> Epoch: 192 Average loss: 3.0721\n",
      "====> Epoch: 194 Average loss: 3.0466\n",
      "====> Epoch: 196 Average loss: 3.0435\n",
      "====> Epoch: 198 Average loss: 3.0527\n",
      "====> Epoch: 200 Average loss: 3.0520\n",
      "====> Epoch: 202 Average loss: 3.0590\n",
      "====> Epoch: 204 Average loss: 3.0547\n",
      "====> Epoch: 206 Average loss: 3.0374\n",
      "====> Epoch: 208 Average loss: 3.0524\n",
      "====> Epoch: 210 Average loss: 3.0697\n",
      "====> Epoch: 212 Average loss: 3.0743\n",
      "====> Epoch: 214 Average loss: 3.0573\n",
      "====> Epoch: 216 Average loss: 3.0598\n",
      "====> Epoch: 218 Average loss: 3.0779\n",
      "====> Epoch: 220 Average loss: 3.0555\n",
      "====> Epoch: 222 Average loss: 3.0710\n",
      "====> Epoch: 224 Average loss: 3.0469\n",
      "====> Epoch: 226 Average loss: 3.0939\n",
      "====> Epoch: 228 Average loss: 3.0765\n",
      "====> Epoch: 230 Average loss: 3.0625\n",
      "====> Epoch: 232 Average loss: 3.0755\n",
      "====> Epoch: 234 Average loss: 3.0645\n",
      "====> Epoch: 236 Average loss: 3.0501\n",
      "====> Epoch: 238 Average loss: 3.0853\n",
      "====> Epoch: 240 Average loss: 3.0437\n",
      "====> Epoch: 242 Average loss: 3.0651\n",
      "====> Epoch: 244 Average loss: 3.0647\n",
      "====> Epoch: 246 Average loss: 3.0407\n",
      "====> Epoch: 248 Average loss: 3.0343\n",
      "====> Epoch: 250 Average loss: 3.1036\n",
      "====> Epoch: 252 Average loss: 3.0583\n",
      "====> Epoch: 254 Average loss: 3.0490\n",
      "====> Epoch: 256 Average loss: 3.0940\n",
      "====> Epoch: 258 Average loss: 3.0340\n",
      "====> Epoch: 260 Average loss: 3.0463\n",
      "====> Epoch: 262 Average loss: 3.0413\n",
      "====> Epoch: 264 Average loss: 3.0588\n",
      "====> Epoch: 266 Average loss: 3.0700\n",
      "====> Epoch: 268 Average loss: 3.0714\n",
      "====> Epoch: 270 Average loss: 3.0553\n",
      "====> Epoch: 272 Average loss: 3.0436\n",
      "====> Epoch: 274 Average loss: 3.0541\n",
      "====> Epoch: 276 Average loss: 3.0467\n",
      "====> Epoch: 278 Average loss: 3.0614\n",
      "====> Epoch: 280 Average loss: 3.0577\n",
      "====> Epoch: 282 Average loss: 3.0663\n",
      "====> Epoch: 284 Average loss: 3.0627\n",
      "====> Epoch: 286 Average loss: 3.0720\n",
      "====> Epoch: 288 Average loss: 3.0771\n",
      "====> Epoch: 290 Average loss: 3.0490\n",
      "====> Epoch: 292 Average loss: 3.0576\n",
      "====> Epoch: 294 Average loss: 3.0557\n",
      "====> Epoch: 296 Average loss: 3.0619\n",
      "====> Epoch: 298 Average loss: 3.0398\n",
      "====> Epoch: 300 Average loss: 3.0475\n",
      "====> Epoch: 302 Average loss: 3.0757\n",
      "====> Epoch: 304 Average loss: 3.0489\n",
      "====> Epoch: 306 Average loss: 3.0629\n",
      "====> Epoch: 308 Average loss: 3.0268\n",
      "====> Epoch: 310 Average loss: 3.0533\n",
      "====> Epoch: 312 Average loss: 3.0417\n",
      "====> Epoch: 314 Average loss: 3.0597\n",
      "====> Epoch: 316 Average loss: 3.0586\n",
      "====> Epoch: 318 Average loss: 3.0577\n",
      "====> Epoch: 320 Average loss: 3.0801\n",
      "====> Epoch: 322 Average loss: 3.0331\n",
      "====> Epoch: 324 Average loss: 3.0412\n",
      "====> Epoch: 326 Average loss: 3.0405\n",
      "====> Epoch: 328 Average loss: 3.0656\n",
      "====> Epoch: 330 Average loss: 3.0697\n",
      "====> Epoch: 332 Average loss: 3.0500\n",
      "====> Epoch: 334 Average loss: 3.0441\n",
      "====> Epoch: 336 Average loss: 3.0654\n",
      "====> Epoch: 338 Average loss: 3.0572\n",
      "====> Epoch: 340 Average loss: 3.0483\n",
      "====> Epoch: 342 Average loss: 3.0481\n",
      "====> Epoch: 344 Average loss: 3.0840\n",
      "====> Epoch: 346 Average loss: 3.0758\n",
      "====> Epoch: 348 Average loss: 3.0379\n",
      "====> Epoch: 350 Average loss: 3.0499\n",
      "====> Epoch: 352 Average loss: 3.0529\n",
      "====> Epoch: 354 Average loss: 3.0900\n",
      "====> Epoch: 356 Average loss: 3.0540\n",
      "====> Epoch: 358 Average loss: 3.0607\n",
      "====> Epoch: 360 Average loss: 3.0510\n",
      "====> Epoch: 362 Average loss: 3.0356\n",
      "====> Epoch: 364 Average loss: 3.0565\n",
      "====> Epoch: 366 Average loss: 3.0766\n",
      "====> Epoch: 368 Average loss: 3.0506\n",
      "====> Epoch: 370 Average loss: 3.0578\n",
      "====> Epoch: 372 Average loss: 3.0474\n",
      "====> Epoch: 374 Average loss: 3.0638\n",
      "====> Epoch: 376 Average loss: 3.0816\n",
      "====> Epoch: 378 Average loss: 3.0596\n",
      "====> Epoch: 380 Average loss: 3.0735\n",
      "====> Epoch: 382 Average loss: 3.0593\n",
      "====> Epoch: 384 Average loss: 3.0592\n",
      "====> Epoch: 386 Average loss: 3.0471\n",
      "====> Epoch: 388 Average loss: 3.0509\n",
      "====> Epoch: 390 Average loss: 3.0183\n",
      "====> Epoch: 392 Average loss: 3.0360\n",
      "====> Epoch: 394 Average loss: 3.0645\n",
      "====> Epoch: 396 Average loss: 3.0439\n",
      "====> Epoch: 398 Average loss: 3.0622\n",
      "====> Epoch: 400 Average loss: 3.0685\n",
      "====> Epoch: 402 Average loss: 3.0440\n",
      "====> Epoch: 404 Average loss: 3.0571\n",
      "====> Epoch: 406 Average loss: 3.0555\n",
      "====> Epoch: 408 Average loss: 3.0433\n",
      "====> Epoch: 410 Average loss: 3.0386\n",
      "====> Epoch: 412 Average loss: 3.1062\n",
      "====> Epoch: 414 Average loss: 3.0328\n",
      "====> Epoch: 416 Average loss: 3.0493\n",
      "====> Epoch: 418 Average loss: 3.0409\n",
      "====> Epoch: 420 Average loss: 3.0639\n",
      "====> Epoch: 422 Average loss: 3.0709\n",
      "====> Epoch: 424 Average loss: 3.0905\n",
      "====> Epoch: 426 Average loss: 3.0712\n",
      "====> Epoch: 428 Average loss: 3.0587\n",
      "====> Epoch: 430 Average loss: 3.0745\n",
      "====> Epoch: 432 Average loss: 3.0392\n",
      "====> Epoch: 434 Average loss: 3.0525\n",
      "====> Epoch: 436 Average loss: 3.0697\n",
      "====> Epoch: 438 Average loss: 3.0555\n",
      "====> Epoch: 440 Average loss: 3.0670\n",
      "====> Epoch: 442 Average loss: 3.0381\n",
      "====> Epoch: 444 Average loss: 3.0805\n",
      "====> Epoch: 446 Average loss: 3.0570\n",
      "====> Epoch: 448 Average loss: 3.0619\n",
      "====> Epoch: 450 Average loss: 3.0361\n",
      "====> Epoch: 452 Average loss: 3.0664\n",
      "====> Epoch: 454 Average loss: 3.0569\n",
      "====> Epoch: 456 Average loss: 3.0287\n",
      "====> Epoch: 458 Average loss: 3.0526\n",
      "====> Epoch: 460 Average loss: 3.0508\n",
      "====> Epoch: 462 Average loss: 3.0714\n",
      "====> Epoch: 464 Average loss: 3.0508\n",
      "====> Epoch: 466 Average loss: 3.0653\n",
      "====> Epoch: 468 Average loss: 3.0547\n",
      "====> Epoch: 470 Average loss: 3.0503\n",
      "====> Epoch: 472 Average loss: 3.0590\n",
      "====> Epoch: 474 Average loss: 3.0359\n",
      "====> Epoch: 476 Average loss: 3.0765\n",
      "====> Epoch: 478 Average loss: 3.0589\n",
      "====> Epoch: 480 Average loss: 3.0512\n",
      "====> Epoch: 482 Average loss: 3.0404\n",
      "====> Epoch: 484 Average loss: 3.0617\n",
      "====> Epoch: 486 Average loss: 3.0439\n",
      "====> Epoch: 488 Average loss: 3.0782\n",
      "====> Epoch: 490 Average loss: 3.0364\n",
      "====> Epoch: 492 Average loss: 3.0546\n",
      "====> Epoch: 494 Average loss: 3.0716\n",
      "====> Epoch: 496 Average loss: 3.0584\n",
      "====> Epoch: 498 Average loss: 3.0394\n",
      "====> Epoch: 500 Average loss: 3.0166\n",
      "====> Epoch: 502 Average loss: 3.0649\n",
      "====> Epoch: 504 Average loss: 3.0702\n",
      "====> Epoch: 506 Average loss: 3.0474\n",
      "====> Epoch: 508 Average loss: 3.0863\n",
      "====> Epoch: 510 Average loss: 3.0381\n",
      "====> Epoch: 512 Average loss: 3.0547\n",
      "====> Epoch: 514 Average loss: 3.0440\n",
      "====> Epoch: 516 Average loss: 3.0571\n",
      "====> Epoch: 518 Average loss: 3.0693\n",
      "====> Epoch: 520 Average loss: 3.0810\n",
      "====> Epoch: 522 Average loss: 3.0812\n",
      "====> Epoch: 524 Average loss: 3.0440\n",
      "====> Epoch: 526 Average loss: 3.0413\n",
      "====> Epoch: 528 Average loss: 3.0328\n",
      "====> Epoch: 530 Average loss: 3.0693\n",
      "====> Epoch: 532 Average loss: 3.0435\n",
      "====> Epoch: 534 Average loss: 3.0530\n",
      "====> Epoch: 536 Average loss: 3.0550\n",
      "====> Epoch: 538 Average loss: 3.0575\n",
      "====> Epoch: 540 Average loss: 3.0584\n",
      "====> Epoch: 542 Average loss: 3.0451\n",
      "====> Epoch: 544 Average loss: 3.0366\n",
      "====> Epoch: 546 Average loss: 3.0554\n",
      "====> Epoch: 548 Average loss: 3.0827\n",
      "====> Epoch: 550 Average loss: 3.0472\n",
      "====> Epoch: 552 Average loss: 3.0586\n",
      "====> Epoch: 554 Average loss: 3.0633\n",
      "====> Epoch: 556 Average loss: 3.0495\n",
      "====> Epoch: 558 Average loss: 3.0801\n",
      "====> Epoch: 560 Average loss: 3.0545\n",
      "====> Epoch: 562 Average loss: 3.0739\n",
      "====> Epoch: 564 Average loss: 3.0700\n",
      "====> Epoch: 566 Average loss: 3.0817\n",
      "====> Epoch: 568 Average loss: 3.0616\n",
      "====> Epoch: 570 Average loss: 3.0858\n",
      "====> Epoch: 572 Average loss: 3.0462\n",
      "====> Epoch: 574 Average loss: 3.0508\n",
      "====> Epoch: 576 Average loss: 3.0266\n",
      "====> Epoch: 578 Average loss: 3.0894\n",
      "====> Epoch: 580 Average loss: 3.0648\n",
      "====> Epoch: 582 Average loss: 3.0599\n",
      "====> Epoch: 584 Average loss: 3.0528\n",
      "====> Epoch: 586 Average loss: 3.0376\n",
      "====> Epoch: 588 Average loss: 3.0778\n",
      "====> Epoch: 590 Average loss: 3.0480\n",
      "====> Epoch: 592 Average loss: 3.0527\n",
      "====> Epoch: 594 Average loss: 3.0725\n",
      "====> Epoch: 596 Average loss: 3.0547\n",
      "====> Epoch: 598 Average loss: 3.0596\n",
      "====> Epoch: 600 Average loss: 3.0507\n",
      "====> Epoch: 602 Average loss: 3.0712\n",
      "====> Epoch: 604 Average loss: 3.0529\n",
      "====> Epoch: 606 Average loss: 3.0535\n",
      "====> Epoch: 608 Average loss: 3.0663\n",
      "====> Epoch: 610 Average loss: 3.0738\n",
      "====> Epoch: 612 Average loss: 3.0504\n",
      "====> Epoch: 614 Average loss: 3.0402\n",
      "====> Epoch: 616 Average loss: 3.0365\n",
      "====> Epoch: 618 Average loss: 3.0626\n",
      "====> Epoch: 620 Average loss: 3.0486\n",
      "====> Epoch: 622 Average loss: 3.0731\n",
      "====> Epoch: 624 Average loss: 3.0307\n",
      "====> Epoch: 626 Average loss: 3.0536\n",
      "====> Epoch: 628 Average loss: 3.0722\n",
      "====> Epoch: 630 Average loss: 3.0561\n",
      "====> Epoch: 632 Average loss: 3.0664\n",
      "====> Epoch: 634 Average loss: 3.0413\n",
      "====> Epoch: 636 Average loss: 3.0772\n",
      "====> Epoch: 638 Average loss: 3.0429\n",
      "====> Epoch: 640 Average loss: 3.0501\n",
      "====> Epoch: 642 Average loss: 3.0756\n",
      "====> Epoch: 644 Average loss: 3.0675\n",
      "====> Epoch: 646 Average loss: 3.0588\n",
      "====> Epoch: 648 Average loss: 3.0584\n",
      "====> Epoch: 650 Average loss: 3.0410\n",
      "====> Epoch: 652 Average loss: 3.0643\n",
      "====> Epoch: 654 Average loss: 3.0741\n",
      "====> Epoch: 656 Average loss: 3.0320\n",
      "====> Epoch: 658 Average loss: 3.0467\n",
      "====> Epoch: 660 Average loss: 3.0665\n",
      "====> Epoch: 662 Average loss: 3.0608\n",
      "====> Epoch: 664 Average loss: 3.0261\n",
      "====> Epoch: 666 Average loss: 3.0454\n",
      "====> Epoch: 668 Average loss: 3.0276\n",
      "====> Epoch: 670 Average loss: 3.0965\n",
      "====> Epoch: 672 Average loss: 3.0573\n",
      "====> Epoch: 674 Average loss: 3.0652\n",
      "====> Epoch: 676 Average loss: 3.0388\n",
      "====> Epoch: 678 Average loss: 3.0836\n",
      "====> Epoch: 680 Average loss: 3.0396\n",
      "====> Epoch: 682 Average loss: 3.0566\n",
      "====> Epoch: 684 Average loss: 3.0690\n",
      "====> Epoch: 686 Average loss: 3.0446\n",
      "====> Epoch: 688 Average loss: 3.0336\n",
      "====> Epoch: 690 Average loss: 3.0414\n",
      "====> Epoch: 692 Average loss: 3.0546\n",
      "====> Epoch: 694 Average loss: 3.0874\n",
      "====> Epoch: 696 Average loss: 3.0378\n",
      "====> Epoch: 698 Average loss: 3.0424\n",
      "====> Epoch: 700 Average loss: 3.0540\n",
      "====> Epoch: 702 Average loss: 3.0692\n",
      "====> Epoch: 704 Average loss: 3.0484\n",
      "====> Epoch: 706 Average loss: 3.0635\n",
      "====> Epoch: 708 Average loss: 3.0478\n",
      "====> Epoch: 710 Average loss: 3.0500\n",
      "====> Epoch: 712 Average loss: 3.0383\n",
      "====> Epoch: 714 Average loss: 3.0437\n",
      "====> Epoch: 716 Average loss: 3.0567\n",
      "====> Epoch: 718 Average loss: 3.0356\n",
      "====> Epoch: 720 Average loss: 3.0623\n",
      "====> Epoch: 722 Average loss: 3.0741\n",
      "====> Epoch: 724 Average loss: 3.0586\n",
      "====> Epoch: 726 Average loss: 3.0608\n",
      "====> Epoch: 728 Average loss: 3.0335\n",
      "====> Epoch: 730 Average loss: 3.0541\n",
      "====> Epoch: 732 Average loss: 3.0376\n",
      "====> Epoch: 734 Average loss: 3.0651\n",
      "====> Epoch: 736 Average loss: 3.0489\n",
      "====> Epoch: 738 Average loss: 3.0410\n",
      "====> Epoch: 740 Average loss: 3.0633\n",
      "====> Epoch: 742 Average loss: 3.0651\n",
      "====> Epoch: 744 Average loss: 3.0497\n",
      "====> Epoch: 746 Average loss: 3.0361\n",
      "====> Epoch: 748 Average loss: 3.0537\n",
      "====> Epoch: 750 Average loss: 3.0379\n",
      "====> Epoch: 752 Average loss: 3.0439\n",
      "====> Epoch: 754 Average loss: 3.0588\n",
      "====> Epoch: 756 Average loss: 3.0333\n",
      "====> Epoch: 758 Average loss: 3.0718\n",
      "====> Epoch: 760 Average loss: 3.0645\n",
      "====> Epoch: 762 Average loss: 3.0577\n",
      "====> Epoch: 764 Average loss: 3.0646\n",
      "====> Epoch: 766 Average loss: 3.0673\n",
      "====> Epoch: 768 Average loss: 3.0660\n",
      "====> Epoch: 770 Average loss: 3.0504\n",
      "====> Epoch: 772 Average loss: 3.0639\n",
      "====> Epoch: 774 Average loss: 3.0501\n",
      "====> Epoch: 776 Average loss: 3.0721\n",
      "====> Epoch: 778 Average loss: 3.0529\n",
      "====> Epoch: 780 Average loss: 3.0380\n",
      "====> Epoch: 782 Average loss: 3.0667\n",
      "====> Epoch: 784 Average loss: 3.0294\n",
      "====> Epoch: 786 Average loss: 3.0485\n",
      "====> Epoch: 788 Average loss: 3.0668\n",
      "====> Epoch: 790 Average loss: 3.0539\n",
      "====> Epoch: 792 Average loss: 3.0523\n",
      "====> Epoch: 794 Average loss: 3.0582\n",
      "====> Epoch: 796 Average loss: 3.0743\n",
      "====> Epoch: 798 Average loss: 3.0608\n",
      "====> Epoch: 800 Average loss: 3.0671\n",
      "====> Epoch: 802 Average loss: 3.0409\n",
      "====> Epoch: 804 Average loss: 3.0698\n",
      "====> Epoch: 806 Average loss: 3.0456\n",
      "====> Epoch: 808 Average loss: 3.0349\n",
      "====> Epoch: 810 Average loss: 3.0342\n",
      "====> Epoch: 812 Average loss: 3.0469\n",
      "====> Epoch: 814 Average loss: 3.0491\n",
      "====> Epoch: 816 Average loss: 3.0726\n",
      "====> Epoch: 818 Average loss: 3.0212\n",
      "====> Epoch: 820 Average loss: 3.0625\n",
      "====> Epoch: 822 Average loss: 3.0493\n",
      "====> Epoch: 824 Average loss: 3.0559\n",
      "====> Epoch: 826 Average loss: 3.0840\n",
      "====> Epoch: 828 Average loss: 3.0558\n",
      "====> Epoch: 830 Average loss: 3.0548\n",
      "====> Epoch: 832 Average loss: 3.0545\n",
      "====> Epoch: 834 Average loss: 3.0474\n",
      "====> Epoch: 836 Average loss: 3.0665\n",
      "====> Epoch: 838 Average loss: 3.0663\n",
      "====> Epoch: 840 Average loss: 3.0662\n",
      "====> Epoch: 842 Average loss: 3.0621\n",
      "====> Epoch: 844 Average loss: 3.0703\n",
      "====> Epoch: 846 Average loss: 3.0588\n",
      "====> Epoch: 848 Average loss: 3.0539\n",
      "====> Epoch: 850 Average loss: 3.0456\n",
      "====> Epoch: 852 Average loss: 3.0387\n",
      "====> Epoch: 854 Average loss: 3.0811\n",
      "====> Epoch: 856 Average loss: 3.0729\n",
      "====> Epoch: 858 Average loss: 3.0502\n",
      "====> Epoch: 860 Average loss: 3.0671\n",
      "====> Epoch: 862 Average loss: 3.0381\n",
      "====> Epoch: 864 Average loss: 3.0687\n",
      "====> Epoch: 866 Average loss: 3.0526\n",
      "====> Epoch: 868 Average loss: 3.0694\n",
      "====> Epoch: 870 Average loss: 3.0372\n",
      "====> Epoch: 872 Average loss: 3.0504\n",
      "====> Epoch: 874 Average loss: 3.0596\n",
      "====> Epoch: 876 Average loss: 3.0833\n",
      "====> Epoch: 878 Average loss: 3.0093\n",
      "====> Epoch: 880 Average loss: 3.0314\n",
      "====> Epoch: 882 Average loss: 3.0410\n",
      "====> Epoch: 884 Average loss: 3.0729\n",
      "====> Epoch: 886 Average loss: 3.0609\n",
      "====> Epoch: 888 Average loss: 3.0732\n",
      "====> Epoch: 890 Average loss: 3.0617\n",
      "====> Epoch: 892 Average loss: 3.0419\n",
      "====> Epoch: 894 Average loss: 3.0508\n",
      "====> Epoch: 896 Average loss: 3.0376\n",
      "====> Epoch: 898 Average loss: 3.0633\n",
      "====> Epoch: 900 Average loss: 3.0507\n",
      "====> Epoch: 902 Average loss: 3.0461\n",
      "====> Epoch: 904 Average loss: 3.0647\n",
      "====> Epoch: 906 Average loss: 3.0437\n",
      "====> Epoch: 908 Average loss: 3.0722\n",
      "====> Epoch: 910 Average loss: 3.0763\n",
      "====> Epoch: 912 Average loss: 3.0464\n",
      "====> Epoch: 914 Average loss: 3.0521\n",
      "====> Epoch: 916 Average loss: 3.0656\n",
      "====> Epoch: 918 Average loss: 3.0638\n",
      "====> Epoch: 920 Average loss: 3.0417\n",
      "====> Epoch: 922 Average loss: 3.0397\n",
      "====> Epoch: 924 Average loss: 3.0498\n",
      "====> Epoch: 926 Average loss: 3.0371\n",
      "====> Epoch: 928 Average loss: 3.0414\n",
      "====> Epoch: 930 Average loss: 3.0276\n",
      "====> Epoch: 932 Average loss: 3.0693\n",
      "====> Epoch: 934 Average loss: 3.0616\n",
      "====> Epoch: 936 Average loss: 3.0424\n",
      "====> Epoch: 938 Average loss: 3.0683\n",
      "====> Epoch: 940 Average loss: 3.0620\n",
      "====> Epoch: 942 Average loss: 3.0415\n",
      "====> Epoch: 944 Average loss: 3.0680\n",
      "====> Epoch: 946 Average loss: 3.0563\n",
      "====> Epoch: 948 Average loss: 3.0282\n",
      "====> Epoch: 950 Average loss: 3.0346\n",
      "====> Epoch: 952 Average loss: 3.0448\n",
      "====> Epoch: 954 Average loss: 3.0506\n",
      "====> Epoch: 956 Average loss: 3.0586\n",
      "====> Epoch: 958 Average loss: 3.0440\n",
      "====> Epoch: 960 Average loss: 3.0317\n",
      "====> Epoch: 962 Average loss: 3.0435\n",
      "====> Epoch: 964 Average loss: 3.0349\n",
      "====> Epoch: 966 Average loss: 3.0549\n",
      "====> Epoch: 968 Average loss: 3.0428\n",
      "====> Epoch: 970 Average loss: 3.0511\n",
      "====> Epoch: 972 Average loss: 3.0861\n",
      "====> Epoch: 974 Average loss: 3.0529\n",
      "====> Epoch: 976 Average loss: 3.0757\n",
      "====> Epoch: 978 Average loss: 3.0526\n",
      "====> Epoch: 980 Average loss: 3.0413\n",
      "====> Epoch: 982 Average loss: 3.0494\n",
      "====> Epoch: 984 Average loss: 3.0558\n",
      "====> Epoch: 986 Average loss: 3.0558\n",
      "====> Epoch: 988 Average loss: 3.0357\n",
      "====> Epoch: 990 Average loss: 3.0389\n",
      "====> Epoch: 992 Average loss: 3.0496\n",
      "====> Epoch: 994 Average loss: 3.0499\n",
      "====> Epoch: 996 Average loss: 3.0590\n",
      "====> Epoch: 998 Average loss: 3.0564\n",
      "====> Epoch: 1000 Average loss: 3.0596\n"
     ]
    }
   ],
   "source": [
    "model_b1, dataset_b1 = train_encoder(\"client_n_data/bank_clients/bank_1.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 4.5884\n",
      "====> Epoch: 4 Average loss: 3.7007\n",
      "====> Epoch: 6 Average loss: 3.5828\n",
      "====> Epoch: 8 Average loss: 3.5160\n",
      "====> Epoch: 10 Average loss: 3.5145\n",
      "====> Epoch: 12 Average loss: 3.4839\n",
      "====> Epoch: 14 Average loss: 3.4773\n",
      "====> Epoch: 16 Average loss: 3.4263\n",
      "====> Epoch: 18 Average loss: 3.4767\n",
      "====> Epoch: 20 Average loss: 3.4550\n",
      "====> Epoch: 22 Average loss: 3.4763\n",
      "====> Epoch: 24 Average loss: 3.4427\n",
      "====> Epoch: 26 Average loss: 3.4420\n",
      "====> Epoch: 28 Average loss: 3.4286\n",
      "====> Epoch: 30 Average loss: 3.4584\n",
      "====> Epoch: 32 Average loss: 3.4476\n",
      "====> Epoch: 34 Average loss: 3.4403\n",
      "====> Epoch: 36 Average loss: 3.4115\n",
      "====> Epoch: 38 Average loss: 3.4350\n",
      "====> Epoch: 40 Average loss: 3.4089\n",
      "====> Epoch: 42 Average loss: 3.4242\n",
      "====> Epoch: 44 Average loss: 3.4247\n",
      "====> Epoch: 46 Average loss: 3.4150\n",
      "====> Epoch: 48 Average loss: 3.4159\n",
      "====> Epoch: 50 Average loss: 3.4147\n",
      "====> Epoch: 52 Average loss: 3.4004\n",
      "====> Epoch: 54 Average loss: 3.4001\n",
      "====> Epoch: 56 Average loss: 3.4024\n",
      "====> Epoch: 58 Average loss: 3.4203\n",
      "====> Epoch: 60 Average loss: 3.4217\n",
      "====> Epoch: 62 Average loss: 3.4150\n",
      "====> Epoch: 64 Average loss: 3.4292\n",
      "====> Epoch: 66 Average loss: 3.4343\n",
      "====> Epoch: 68 Average loss: 3.4129\n",
      "====> Epoch: 70 Average loss: 3.4150\n",
      "====> Epoch: 72 Average loss: 3.4172\n",
      "====> Epoch: 74 Average loss: 3.3907\n",
      "====> Epoch: 76 Average loss: 3.4304\n",
      "====> Epoch: 78 Average loss: 3.4312\n",
      "====> Epoch: 80 Average loss: 3.4029\n",
      "====> Epoch: 82 Average loss: 3.4224\n",
      "====> Epoch: 84 Average loss: 3.4131\n",
      "====> Epoch: 86 Average loss: 3.3956\n",
      "====> Epoch: 88 Average loss: 3.3901\n",
      "====> Epoch: 90 Average loss: 3.4229\n",
      "====> Epoch: 92 Average loss: 3.3946\n",
      "====> Epoch: 94 Average loss: 3.3891\n",
      "====> Epoch: 96 Average loss: 3.4280\n",
      "====> Epoch: 98 Average loss: 3.4153\n",
      "====> Epoch: 100 Average loss: 3.4028\n",
      "====> Epoch: 102 Average loss: 3.4004\n",
      "====> Epoch: 104 Average loss: 3.4342\n",
      "====> Epoch: 106 Average loss: 3.4193\n",
      "====> Epoch: 108 Average loss: 3.3955\n",
      "====> Epoch: 110 Average loss: 3.4093\n",
      "====> Epoch: 112 Average loss: 3.4044\n",
      "====> Epoch: 114 Average loss: 3.4078\n",
      "====> Epoch: 116 Average loss: 3.4168\n",
      "====> Epoch: 118 Average loss: 3.4032\n",
      "====> Epoch: 120 Average loss: 3.3982\n",
      "====> Epoch: 122 Average loss: 3.4020\n",
      "====> Epoch: 124 Average loss: 3.3911\n",
      "====> Epoch: 126 Average loss: 3.4096\n",
      "====> Epoch: 128 Average loss: 3.4043\n",
      "====> Epoch: 130 Average loss: 3.4034\n",
      "====> Epoch: 132 Average loss: 3.4292\n",
      "====> Epoch: 134 Average loss: 3.4247\n",
      "====> Epoch: 136 Average loss: 3.4028\n",
      "====> Epoch: 138 Average loss: 3.4245\n",
      "====> Epoch: 140 Average loss: 3.3918\n",
      "====> Epoch: 142 Average loss: 3.4148\n",
      "====> Epoch: 144 Average loss: 3.4107\n",
      "====> Epoch: 146 Average loss: 3.3777\n",
      "====> Epoch: 148 Average loss: 3.4022\n",
      "====> Epoch: 150 Average loss: 3.3907\n",
      "====> Epoch: 152 Average loss: 3.3947\n",
      "====> Epoch: 154 Average loss: 3.3926\n",
      "====> Epoch: 156 Average loss: 3.3829\n",
      "====> Epoch: 158 Average loss: 3.4002\n",
      "====> Epoch: 160 Average loss: 3.3808\n",
      "====> Epoch: 162 Average loss: 3.4245\n",
      "====> Epoch: 164 Average loss: 3.4056\n",
      "====> Epoch: 166 Average loss: 3.3977\n",
      "====> Epoch: 168 Average loss: 3.4087\n",
      "====> Epoch: 170 Average loss: 3.4025\n",
      "====> Epoch: 172 Average loss: 3.3916\n",
      "====> Epoch: 174 Average loss: 3.4052\n",
      "====> Epoch: 176 Average loss: 3.3709\n",
      "====> Epoch: 178 Average loss: 3.3819\n",
      "====> Epoch: 180 Average loss: 3.3763\n",
      "====> Epoch: 182 Average loss: 3.4324\n",
      "====> Epoch: 184 Average loss: 3.3795\n",
      "====> Epoch: 186 Average loss: 3.4124\n",
      "====> Epoch: 188 Average loss: 3.4059\n",
      "====> Epoch: 190 Average loss: 3.3900\n",
      "====> Epoch: 192 Average loss: 3.3916\n",
      "====> Epoch: 194 Average loss: 3.3715\n",
      "====> Epoch: 196 Average loss: 3.3789\n",
      "====> Epoch: 198 Average loss: 3.4092\n",
      "====> Epoch: 200 Average loss: 3.3840\n",
      "====> Epoch: 202 Average loss: 3.4124\n",
      "====> Epoch: 204 Average loss: 3.3979\n",
      "====> Epoch: 206 Average loss: 3.4182\n",
      "====> Epoch: 208 Average loss: 3.3847\n",
      "====> Epoch: 210 Average loss: 3.4245\n",
      "====> Epoch: 212 Average loss: 3.4024\n",
      "====> Epoch: 214 Average loss: 3.3965\n",
      "====> Epoch: 216 Average loss: 3.3953\n",
      "====> Epoch: 218 Average loss: 3.3966\n",
      "====> Epoch: 220 Average loss: 3.4009\n",
      "====> Epoch: 222 Average loss: 3.3913\n",
      "====> Epoch: 224 Average loss: 3.3961\n",
      "====> Epoch: 226 Average loss: 3.4009\n",
      "====> Epoch: 228 Average loss: 3.3817\n",
      "====> Epoch: 230 Average loss: 3.3820\n",
      "====> Epoch: 232 Average loss: 3.3847\n",
      "====> Epoch: 234 Average loss: 3.3837\n",
      "====> Epoch: 236 Average loss: 3.3874\n",
      "====> Epoch: 238 Average loss: 3.3980\n",
      "====> Epoch: 240 Average loss: 3.4173\n",
      "====> Epoch: 242 Average loss: 3.3862\n",
      "====> Epoch: 244 Average loss: 3.3905\n",
      "====> Epoch: 246 Average loss: 3.3850\n",
      "====> Epoch: 248 Average loss: 3.4139\n",
      "====> Epoch: 250 Average loss: 3.3953\n",
      "====> Epoch: 252 Average loss: 3.3929\n",
      "====> Epoch: 254 Average loss: 3.3948\n",
      "====> Epoch: 256 Average loss: 3.3816\n",
      "====> Epoch: 258 Average loss: 3.4032\n",
      "====> Epoch: 260 Average loss: 3.3974\n",
      "====> Epoch: 262 Average loss: 3.3993\n",
      "====> Epoch: 264 Average loss: 3.3888\n",
      "====> Epoch: 266 Average loss: 3.4147\n",
      "====> Epoch: 268 Average loss: 3.3903\n",
      "====> Epoch: 270 Average loss: 3.3960\n",
      "====> Epoch: 272 Average loss: 3.3938\n",
      "====> Epoch: 274 Average loss: 3.4083\n",
      "====> Epoch: 276 Average loss: 3.3983\n",
      "====> Epoch: 278 Average loss: 3.3949\n",
      "====> Epoch: 280 Average loss: 3.3989\n",
      "====> Epoch: 282 Average loss: 3.4010\n",
      "====> Epoch: 284 Average loss: 3.3700\n",
      "====> Epoch: 286 Average loss: 3.4322\n",
      "====> Epoch: 288 Average loss: 3.3480\n",
      "====> Epoch: 290 Average loss: 3.3943\n",
      "====> Epoch: 292 Average loss: 3.4012\n",
      "====> Epoch: 294 Average loss: 3.3934\n",
      "====> Epoch: 296 Average loss: 3.4177\n",
      "====> Epoch: 298 Average loss: 3.4011\n",
      "====> Epoch: 300 Average loss: 3.4032\n",
      "====> Epoch: 302 Average loss: 3.3634\n",
      "====> Epoch: 304 Average loss: 3.3842\n",
      "====> Epoch: 306 Average loss: 3.3777\n",
      "====> Epoch: 308 Average loss: 3.3946\n",
      "====> Epoch: 310 Average loss: 3.3767\n",
      "====> Epoch: 312 Average loss: 3.4190\n",
      "====> Epoch: 314 Average loss: 3.3849\n",
      "====> Epoch: 316 Average loss: 3.3983\n",
      "====> Epoch: 318 Average loss: 3.3917\n",
      "====> Epoch: 320 Average loss: 3.4045\n",
      "====> Epoch: 322 Average loss: 3.3514\n",
      "====> Epoch: 324 Average loss: 3.3643\n",
      "====> Epoch: 326 Average loss: 3.3910\n",
      "====> Epoch: 328 Average loss: 3.3955\n",
      "====> Epoch: 330 Average loss: 3.3820\n",
      "====> Epoch: 332 Average loss: 3.3934\n",
      "====> Epoch: 334 Average loss: 3.3991\n",
      "====> Epoch: 336 Average loss: 3.3914\n",
      "====> Epoch: 338 Average loss: 3.3942\n",
      "====> Epoch: 340 Average loss: 3.3731\n",
      "====> Epoch: 342 Average loss: 3.3954\n",
      "====> Epoch: 344 Average loss: 3.4009\n",
      "====> Epoch: 346 Average loss: 3.4073\n",
      "====> Epoch: 348 Average loss: 3.4128\n",
      "====> Epoch: 350 Average loss: 3.3823\n",
      "====> Epoch: 352 Average loss: 3.4159\n",
      "====> Epoch: 354 Average loss: 3.4167\n",
      "====> Epoch: 356 Average loss: 3.3840\n",
      "====> Epoch: 358 Average loss: 3.3873\n",
      "====> Epoch: 360 Average loss: 3.4093\n",
      "====> Epoch: 362 Average loss: 3.3835\n",
      "====> Epoch: 364 Average loss: 3.3709\n",
      "====> Epoch: 366 Average loss: 3.3886\n",
      "====> Epoch: 368 Average loss: 3.4027\n",
      "====> Epoch: 370 Average loss: 3.3797\n",
      "====> Epoch: 372 Average loss: 3.3655\n",
      "====> Epoch: 374 Average loss: 3.3612\n",
      "====> Epoch: 376 Average loss: 3.3904\n",
      "====> Epoch: 378 Average loss: 3.3699\n",
      "====> Epoch: 380 Average loss: 3.3967\n",
      "====> Epoch: 382 Average loss: 3.3797\n",
      "====> Epoch: 384 Average loss: 3.3845\n",
      "====> Epoch: 386 Average loss: 3.4178\n",
      "====> Epoch: 388 Average loss: 3.3922\n",
      "====> Epoch: 390 Average loss: 3.4032\n",
      "====> Epoch: 392 Average loss: 3.3869\n",
      "====> Epoch: 394 Average loss: 3.3748\n",
      "====> Epoch: 396 Average loss: 3.3794\n",
      "====> Epoch: 398 Average loss: 3.3900\n",
      "====> Epoch: 400 Average loss: 3.3929\n",
      "====> Epoch: 402 Average loss: 3.3523\n",
      "====> Epoch: 404 Average loss: 3.3878\n",
      "====> Epoch: 406 Average loss: 3.3968\n",
      "====> Epoch: 408 Average loss: 3.3952\n",
      "====> Epoch: 410 Average loss: 3.3895\n",
      "====> Epoch: 412 Average loss: 3.3655\n",
      "====> Epoch: 414 Average loss: 3.3913\n",
      "====> Epoch: 416 Average loss: 3.3709\n",
      "====> Epoch: 418 Average loss: 3.3538\n",
      "====> Epoch: 420 Average loss: 3.3847\n",
      "====> Epoch: 422 Average loss: 3.3865\n",
      "====> Epoch: 424 Average loss: 3.3635\n",
      "====> Epoch: 426 Average loss: 3.3864\n",
      "====> Epoch: 428 Average loss: 3.4113\n",
      "====> Epoch: 430 Average loss: 3.3679\n",
      "====> Epoch: 432 Average loss: 3.3956\n",
      "====> Epoch: 434 Average loss: 3.3651\n",
      "====> Epoch: 436 Average loss: 3.3944\n",
      "====> Epoch: 438 Average loss: 3.3870\n",
      "====> Epoch: 440 Average loss: 3.3578\n",
      "====> Epoch: 442 Average loss: 3.3701\n",
      "====> Epoch: 444 Average loss: 3.3750\n",
      "====> Epoch: 446 Average loss: 3.4033\n",
      "====> Epoch: 448 Average loss: 3.3721\n",
      "====> Epoch: 450 Average loss: 3.3654\n",
      "====> Epoch: 452 Average loss: 3.3995\n",
      "====> Epoch: 454 Average loss: 3.3682\n",
      "====> Epoch: 456 Average loss: 3.3646\n",
      "====> Epoch: 458 Average loss: 3.3725\n",
      "====> Epoch: 460 Average loss: 3.3625\n",
      "====> Epoch: 462 Average loss: 3.3552\n",
      "====> Epoch: 464 Average loss: 3.3851\n",
      "====> Epoch: 466 Average loss: 3.3937\n",
      "====> Epoch: 468 Average loss: 3.3854\n",
      "====> Epoch: 470 Average loss: 3.3822\n",
      "====> Epoch: 472 Average loss: 3.3603\n",
      "====> Epoch: 474 Average loss: 3.3659\n",
      "====> Epoch: 476 Average loss: 3.3821\n",
      "====> Epoch: 478 Average loss: 3.4070\n",
      "====> Epoch: 480 Average loss: 3.3624\n",
      "====> Epoch: 482 Average loss: 3.3741\n",
      "====> Epoch: 484 Average loss: 3.4089\n",
      "====> Epoch: 486 Average loss: 3.3947\n",
      "====> Epoch: 488 Average loss: 3.3611\n",
      "====> Epoch: 490 Average loss: 3.3904\n",
      "====> Epoch: 492 Average loss: 3.3684\n",
      "====> Epoch: 494 Average loss: 3.3639\n",
      "====> Epoch: 496 Average loss: 3.3959\n",
      "====> Epoch: 498 Average loss: 3.3803\n",
      "====> Epoch: 500 Average loss: 3.3837\n",
      "====> Epoch: 502 Average loss: 3.3615\n",
      "====> Epoch: 504 Average loss: 3.3879\n",
      "====> Epoch: 506 Average loss: 3.3447\n",
      "====> Epoch: 508 Average loss: 3.3551\n",
      "====> Epoch: 510 Average loss: 3.3967\n",
      "====> Epoch: 512 Average loss: 3.3534\n",
      "====> Epoch: 514 Average loss: 3.3428\n",
      "====> Epoch: 516 Average loss: 3.3588\n",
      "====> Epoch: 518 Average loss: 3.3835\n",
      "====> Epoch: 520 Average loss: 3.3768\n",
      "====> Epoch: 522 Average loss: 3.3709\n",
      "====> Epoch: 524 Average loss: 3.4014\n",
      "====> Epoch: 526 Average loss: 3.3711\n",
      "====> Epoch: 528 Average loss: 3.3401\n",
      "====> Epoch: 530 Average loss: 3.3816\n",
      "====> Epoch: 532 Average loss: 3.3947\n",
      "====> Epoch: 534 Average loss: 3.3559\n",
      "====> Epoch: 536 Average loss: 3.3690\n",
      "====> Epoch: 538 Average loss: 3.3781\n",
      "====> Epoch: 540 Average loss: 3.3994\n",
      "====> Epoch: 542 Average loss: 3.3797\n",
      "====> Epoch: 544 Average loss: 3.3685\n",
      "====> Epoch: 546 Average loss: 3.3714\n",
      "====> Epoch: 548 Average loss: 3.3861\n",
      "====> Epoch: 550 Average loss: 3.3603\n",
      "====> Epoch: 552 Average loss: 3.3648\n",
      "====> Epoch: 554 Average loss: 3.3625\n",
      "====> Epoch: 556 Average loss: 3.3714\n",
      "====> Epoch: 558 Average loss: 3.3812\n",
      "====> Epoch: 560 Average loss: 3.3789\n",
      "====> Epoch: 562 Average loss: 3.3915\n",
      "====> Epoch: 564 Average loss: 3.3769\n",
      "====> Epoch: 566 Average loss: 3.3794\n",
      "====> Epoch: 568 Average loss: 3.3722\n",
      "====> Epoch: 570 Average loss: 3.3386\n",
      "====> Epoch: 572 Average loss: 3.3730\n",
      "====> Epoch: 574 Average loss: 3.3758\n",
      "====> Epoch: 576 Average loss: 3.4156\n",
      "====> Epoch: 578 Average loss: 3.3721\n",
      "====> Epoch: 580 Average loss: 3.3795\n",
      "====> Epoch: 582 Average loss: 3.3761\n",
      "====> Epoch: 584 Average loss: 3.3711\n",
      "====> Epoch: 586 Average loss: 3.3776\n",
      "====> Epoch: 588 Average loss: 3.3702\n",
      "====> Epoch: 590 Average loss: 3.3744\n",
      "====> Epoch: 592 Average loss: 3.3863\n",
      "====> Epoch: 594 Average loss: 3.3732\n",
      "====> Epoch: 596 Average loss: 3.3684\n",
      "====> Epoch: 598 Average loss: 3.3760\n",
      "====> Epoch: 600 Average loss: 3.3843\n",
      "====> Epoch: 602 Average loss: 3.3952\n",
      "====> Epoch: 604 Average loss: 3.3457\n",
      "====> Epoch: 606 Average loss: 3.3572\n",
      "====> Epoch: 608 Average loss: 3.3566\n",
      "====> Epoch: 610 Average loss: 3.3576\n",
      "====> Epoch: 612 Average loss: 3.3567\n",
      "====> Epoch: 614 Average loss: 3.3638\n",
      "====> Epoch: 616 Average loss: 3.3777\n",
      "====> Epoch: 618 Average loss: 3.3789\n",
      "====> Epoch: 620 Average loss: 3.3658\n",
      "====> Epoch: 622 Average loss: 3.3804\n",
      "====> Epoch: 624 Average loss: 3.4140\n",
      "====> Epoch: 626 Average loss: 3.3848\n",
      "====> Epoch: 628 Average loss: 3.3433\n",
      "====> Epoch: 630 Average loss: 3.3832\n",
      "====> Epoch: 632 Average loss: 3.3461\n",
      "====> Epoch: 634 Average loss: 3.3447\n",
      "====> Epoch: 636 Average loss: 3.3692\n",
      "====> Epoch: 638 Average loss: 3.3548\n",
      "====> Epoch: 640 Average loss: 3.3736\n",
      "====> Epoch: 642 Average loss: 3.4028\n",
      "====> Epoch: 644 Average loss: 3.3737\n",
      "====> Epoch: 646 Average loss: 3.3344\n",
      "====> Epoch: 648 Average loss: 3.3682\n",
      "====> Epoch: 650 Average loss: 3.3550\n",
      "====> Epoch: 652 Average loss: 3.3967\n",
      "====> Epoch: 654 Average loss: 3.3848\n",
      "====> Epoch: 656 Average loss: 3.3682\n",
      "====> Epoch: 658 Average loss: 3.3455\n",
      "====> Epoch: 660 Average loss: 3.3751\n",
      "====> Epoch: 662 Average loss: 3.3774\n",
      "====> Epoch: 664 Average loss: 3.3682\n",
      "====> Epoch: 666 Average loss: 3.3506\n",
      "====> Epoch: 668 Average loss: 3.3923\n",
      "====> Epoch: 670 Average loss: 3.3658\n",
      "====> Epoch: 672 Average loss: 3.3858\n",
      "====> Epoch: 674 Average loss: 3.3563\n",
      "====> Epoch: 676 Average loss: 3.3434\n",
      "====> Epoch: 678 Average loss: 3.3594\n",
      "====> Epoch: 680 Average loss: 3.3550\n",
      "====> Epoch: 682 Average loss: 3.3780\n",
      "====> Epoch: 684 Average loss: 3.3828\n",
      "====> Epoch: 686 Average loss: 3.3716\n",
      "====> Epoch: 688 Average loss: 3.3790\n",
      "====> Epoch: 690 Average loss: 3.3681\n",
      "====> Epoch: 692 Average loss: 3.3855\n",
      "====> Epoch: 694 Average loss: 3.3574\n",
      "====> Epoch: 696 Average loss: 3.3797\n",
      "====> Epoch: 698 Average loss: 3.3898\n",
      "====> Epoch: 700 Average loss: 3.3649\n",
      "====> Epoch: 702 Average loss: 3.3645\n",
      "====> Epoch: 704 Average loss: 3.3719\n",
      "====> Epoch: 706 Average loss: 3.4006\n",
      "====> Epoch: 708 Average loss: 3.3697\n",
      "====> Epoch: 710 Average loss: 3.3462\n",
      "====> Epoch: 712 Average loss: 3.3802\n",
      "====> Epoch: 714 Average loss: 3.3722\n",
      "====> Epoch: 716 Average loss: 3.3806\n",
      "====> Epoch: 718 Average loss: 3.3891\n",
      "====> Epoch: 720 Average loss: 3.3433\n",
      "====> Epoch: 722 Average loss: 3.4096\n",
      "====> Epoch: 724 Average loss: 3.3496\n",
      "====> Epoch: 726 Average loss: 3.3684\n",
      "====> Epoch: 728 Average loss: 3.3687\n",
      "====> Epoch: 730 Average loss: 3.3621\n",
      "====> Epoch: 732 Average loss: 3.3769\n",
      "====> Epoch: 734 Average loss: 3.3779\n",
      "====> Epoch: 736 Average loss: 3.3635\n",
      "====> Epoch: 738 Average loss: 3.3555\n",
      "====> Epoch: 740 Average loss: 3.3746\n",
      "====> Epoch: 742 Average loss: 3.3672\n",
      "====> Epoch: 744 Average loss: 3.3810\n",
      "====> Epoch: 746 Average loss: 3.3516\n",
      "====> Epoch: 748 Average loss: 3.3496\n",
      "====> Epoch: 750 Average loss: 3.3635\n",
      "====> Epoch: 752 Average loss: 3.3616\n",
      "====> Epoch: 754 Average loss: 3.3790\n",
      "====> Epoch: 756 Average loss: 3.3583\n",
      "====> Epoch: 758 Average loss: 3.3425\n",
      "====> Epoch: 760 Average loss: 3.3361\n",
      "====> Epoch: 762 Average loss: 3.3822\n",
      "====> Epoch: 764 Average loss: 3.3507\n",
      "====> Epoch: 766 Average loss: 3.3709\n",
      "====> Epoch: 768 Average loss: 3.3923\n",
      "====> Epoch: 770 Average loss: 3.3538\n",
      "====> Epoch: 772 Average loss: 3.3479\n",
      "====> Epoch: 774 Average loss: 3.3449\n",
      "====> Epoch: 776 Average loss: 3.3710\n",
      "====> Epoch: 778 Average loss: 3.3536\n",
      "====> Epoch: 780 Average loss: 3.3515\n",
      "====> Epoch: 782 Average loss: 3.3486\n",
      "====> Epoch: 784 Average loss: 3.3862\n",
      "====> Epoch: 786 Average loss: 3.3765\n",
      "====> Epoch: 788 Average loss: 3.3779\n",
      "====> Epoch: 790 Average loss: 3.3508\n",
      "====> Epoch: 792 Average loss: 3.3541\n",
      "====> Epoch: 794 Average loss: 3.3701\n",
      "====> Epoch: 796 Average loss: 3.3827\n",
      "====> Epoch: 798 Average loss: 3.3646\n",
      "====> Epoch: 800 Average loss: 3.3971\n",
      "====> Epoch: 802 Average loss: 3.3674\n",
      "====> Epoch: 804 Average loss: 3.3641\n",
      "====> Epoch: 806 Average loss: 3.3774\n",
      "====> Epoch: 808 Average loss: 3.3487\n",
      "====> Epoch: 810 Average loss: 3.3579\n",
      "====> Epoch: 812 Average loss: 3.3616\n",
      "====> Epoch: 814 Average loss: 3.3387\n",
      "====> Epoch: 816 Average loss: 3.3754\n",
      "====> Epoch: 818 Average loss: 3.3650\n",
      "====> Epoch: 820 Average loss: 3.4015\n",
      "====> Epoch: 822 Average loss: 3.3503\n",
      "====> Epoch: 824 Average loss: 3.3624\n",
      "====> Epoch: 826 Average loss: 3.3770\n",
      "====> Epoch: 828 Average loss: 3.3495\n",
      "====> Epoch: 830 Average loss: 3.3510\n",
      "====> Epoch: 832 Average loss: 3.3564\n",
      "====> Epoch: 834 Average loss: 3.3804\n",
      "====> Epoch: 836 Average loss: 3.3498\n",
      "====> Epoch: 838 Average loss: 3.3542\n",
      "====> Epoch: 840 Average loss: 3.3424\n",
      "====> Epoch: 842 Average loss: 3.3676\n",
      "====> Epoch: 844 Average loss: 3.3588\n",
      "====> Epoch: 846 Average loss: 3.3542\n",
      "====> Epoch: 848 Average loss: 3.3617\n",
      "====> Epoch: 850 Average loss: 3.3436\n",
      "====> Epoch: 852 Average loss: 3.3903\n",
      "====> Epoch: 854 Average loss: 3.3770\n",
      "====> Epoch: 856 Average loss: 3.3694\n",
      "====> Epoch: 858 Average loss: 3.3704\n",
      "====> Epoch: 860 Average loss: 3.3740\n",
      "====> Epoch: 862 Average loss: 3.3342\n",
      "====> Epoch: 864 Average loss: 3.3474\n",
      "====> Epoch: 866 Average loss: 3.3653\n",
      "====> Epoch: 868 Average loss: 3.3535\n",
      "====> Epoch: 870 Average loss: 3.3681\n",
      "====> Epoch: 872 Average loss: 3.3690\n",
      "====> Epoch: 874 Average loss: 3.3725\n",
      "====> Epoch: 876 Average loss: 3.3746\n",
      "====> Epoch: 878 Average loss: 3.3465\n",
      "====> Epoch: 880 Average loss: 3.3728\n",
      "====> Epoch: 882 Average loss: 3.3443\n",
      "====> Epoch: 884 Average loss: 3.3422\n",
      "====> Epoch: 886 Average loss: 3.3703\n",
      "====> Epoch: 888 Average loss: 3.3583\n",
      "====> Epoch: 890 Average loss: 3.3540\n",
      "====> Epoch: 892 Average loss: 3.3406\n",
      "====> Epoch: 894 Average loss: 3.3726\n",
      "====> Epoch: 896 Average loss: 3.3607\n",
      "====> Epoch: 898 Average loss: 3.3766\n",
      "====> Epoch: 900 Average loss: 3.3621\n",
      "====> Epoch: 902 Average loss: 3.3477\n",
      "====> Epoch: 904 Average loss: 3.3655\n",
      "====> Epoch: 906 Average loss: 3.3684\n",
      "====> Epoch: 908 Average loss: 3.3516\n",
      "====> Epoch: 910 Average loss: 3.3692\n",
      "====> Epoch: 912 Average loss: 3.3432\n",
      "====> Epoch: 914 Average loss: 3.3667\n",
      "====> Epoch: 916 Average loss: 3.3607\n",
      "====> Epoch: 918 Average loss: 3.3456\n",
      "====> Epoch: 920 Average loss: 3.3856\n",
      "====> Epoch: 922 Average loss: 3.3530\n",
      "====> Epoch: 924 Average loss: 3.3470\n",
      "====> Epoch: 926 Average loss: 3.3462\n",
      "====> Epoch: 928 Average loss: 3.3416\n",
      "====> Epoch: 930 Average loss: 3.3596\n",
      "====> Epoch: 932 Average loss: 3.3700\n",
      "====> Epoch: 934 Average loss: 3.3567\n",
      "====> Epoch: 936 Average loss: 3.3437\n",
      "====> Epoch: 938 Average loss: 3.3371\n",
      "====> Epoch: 940 Average loss: 3.3520\n",
      "====> Epoch: 942 Average loss: 3.3569\n",
      "====> Epoch: 944 Average loss: 3.3401\n",
      "====> Epoch: 946 Average loss: 3.3530\n",
      "====> Epoch: 948 Average loss: 3.3528\n",
      "====> Epoch: 950 Average loss: 3.3680\n",
      "====> Epoch: 952 Average loss: 3.3811\n",
      "====> Epoch: 954 Average loss: 3.3518\n",
      "====> Epoch: 956 Average loss: 3.3680\n",
      "====> Epoch: 958 Average loss: 3.3545\n",
      "====> Epoch: 960 Average loss: 3.3571\n",
      "====> Epoch: 962 Average loss: 3.3813\n",
      "====> Epoch: 964 Average loss: 3.3316\n",
      "====> Epoch: 966 Average loss: 3.3676\n",
      "====> Epoch: 968 Average loss: 3.3841\n",
      "====> Epoch: 970 Average loss: 3.3710\n",
      "====> Epoch: 972 Average loss: 3.3477\n",
      "====> Epoch: 974 Average loss: 3.3690\n",
      "====> Epoch: 976 Average loss: 3.3612\n",
      "====> Epoch: 978 Average loss: 3.3716\n",
      "====> Epoch: 980 Average loss: 3.3636\n",
      "====> Epoch: 982 Average loss: 3.3566\n",
      "====> Epoch: 984 Average loss: 3.3572\n",
      "====> Epoch: 986 Average loss: 3.3483\n",
      "====> Epoch: 988 Average loss: 3.3530\n",
      "====> Epoch: 990 Average loss: 3.3436\n",
      "====> Epoch: 992 Average loss: 3.3699\n",
      "====> Epoch: 994 Average loss: 3.3393\n",
      "====> Epoch: 996 Average loss: 3.3611\n",
      "====> Epoch: 998 Average loss: 3.3584\n",
      "====> Epoch: 1000 Average loss: 3.3361\n"
     ]
    }
   ],
   "source": [
    "model_b2, dataset_b2 = train_encoder(\"client_n_data/bank_clients/bank_2.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 3.8010\n",
      "====> Epoch: 4 Average loss: 2.3006\n",
      "====> Epoch: 6 Average loss: 1.6693\n",
      "====> Epoch: 8 Average loss: 1.4318\n",
      "====> Epoch: 10 Average loss: 1.3062\n",
      "====> Epoch: 12 Average loss: 1.2543\n",
      "====> Epoch: 14 Average loss: 1.2379\n",
      "====> Epoch: 16 Average loss: 1.2586\n",
      "====> Epoch: 18 Average loss: 1.2152\n",
      "====> Epoch: 20 Average loss: 1.2382\n",
      "====> Epoch: 22 Average loss: 1.2347\n",
      "====> Epoch: 24 Average loss: 1.2300\n",
      "====> Epoch: 26 Average loss: 1.2194\n",
      "====> Epoch: 28 Average loss: 1.2488\n",
      "====> Epoch: 30 Average loss: 1.2294\n",
      "====> Epoch: 32 Average loss: 1.2362\n",
      "====> Epoch: 34 Average loss: 1.2366\n",
      "====> Epoch: 36 Average loss: 1.2583\n",
      "====> Epoch: 38 Average loss: 1.2294\n",
      "====> Epoch: 40 Average loss: 1.2523\n",
      "====> Epoch: 42 Average loss: 1.2345\n",
      "====> Epoch: 44 Average loss: 1.2336\n",
      "====> Epoch: 46 Average loss: 1.2346\n",
      "====> Epoch: 48 Average loss: 1.2261\n",
      "====> Epoch: 50 Average loss: 1.2371\n",
      "====> Epoch: 52 Average loss: 1.2403\n",
      "====> Epoch: 54 Average loss: 1.2358\n",
      "====> Epoch: 56 Average loss: 1.2394\n",
      "====> Epoch: 58 Average loss: 1.2454\n",
      "====> Epoch: 60 Average loss: 1.2364\n",
      "====> Epoch: 62 Average loss: 1.2386\n",
      "====> Epoch: 64 Average loss: 1.2502\n",
      "====> Epoch: 66 Average loss: 1.2250\n",
      "====> Epoch: 68 Average loss: 1.2165\n",
      "====> Epoch: 70 Average loss: 1.2241\n",
      "====> Epoch: 72 Average loss: 1.2207\n",
      "====> Epoch: 74 Average loss: 1.2338\n",
      "====> Epoch: 76 Average loss: 1.2279\n",
      "====> Epoch: 78 Average loss: 1.2378\n",
      "====> Epoch: 80 Average loss: 1.2269\n",
      "====> Epoch: 82 Average loss: 1.2424\n",
      "====> Epoch: 84 Average loss: 1.2354\n",
      "====> Epoch: 86 Average loss: 1.2348\n",
      "====> Epoch: 88 Average loss: 1.2134\n",
      "====> Epoch: 90 Average loss: 1.2465\n",
      "====> Epoch: 92 Average loss: 1.2437\n",
      "====> Epoch: 94 Average loss: 1.2339\n",
      "====> Epoch: 96 Average loss: 1.2250\n",
      "====> Epoch: 98 Average loss: 1.2379\n",
      "====> Epoch: 100 Average loss: 1.2398\n",
      "====> Epoch: 102 Average loss: 1.2571\n",
      "====> Epoch: 104 Average loss: 1.2443\n",
      "====> Epoch: 106 Average loss: 1.2415\n",
      "====> Epoch: 108 Average loss: 1.2352\n",
      "====> Epoch: 110 Average loss: 1.2274\n",
      "====> Epoch: 112 Average loss: 1.2200\n",
      "====> Epoch: 114 Average loss: 1.2312\n",
      "====> Epoch: 116 Average loss: 1.2348\n",
      "====> Epoch: 118 Average loss: 1.2416\n",
      "====> Epoch: 120 Average loss: 1.2268\n",
      "====> Epoch: 122 Average loss: 1.2159\n",
      "====> Epoch: 124 Average loss: 1.2183\n",
      "====> Epoch: 126 Average loss: 1.2467\n",
      "====> Epoch: 128 Average loss: 1.2560\n",
      "====> Epoch: 130 Average loss: 1.2243\n",
      "====> Epoch: 132 Average loss: 1.2279\n",
      "====> Epoch: 134 Average loss: 1.2361\n",
      "====> Epoch: 136 Average loss: 1.2297\n",
      "====> Epoch: 138 Average loss: 1.2365\n",
      "====> Epoch: 140 Average loss: 1.2331\n",
      "====> Epoch: 142 Average loss: 1.2277\n",
      "====> Epoch: 144 Average loss: 1.2229\n",
      "====> Epoch: 146 Average loss: 1.2230\n",
      "====> Epoch: 148 Average loss: 1.2364\n",
      "====> Epoch: 150 Average loss: 1.2409\n",
      "====> Epoch: 152 Average loss: 1.2343\n",
      "====> Epoch: 154 Average loss: 1.2345\n",
      "====> Epoch: 156 Average loss: 1.2371\n",
      "====> Epoch: 158 Average loss: 1.2437\n",
      "====> Epoch: 160 Average loss: 1.2301\n",
      "====> Epoch: 162 Average loss: 1.2482\n",
      "====> Epoch: 164 Average loss: 1.2260\n",
      "====> Epoch: 166 Average loss: 1.2343\n",
      "====> Epoch: 168 Average loss: 1.2180\n",
      "====> Epoch: 170 Average loss: 1.2212\n",
      "====> Epoch: 172 Average loss: 1.2315\n",
      "====> Epoch: 174 Average loss: 1.2188\n",
      "====> Epoch: 176 Average loss: 1.2336\n",
      "====> Epoch: 178 Average loss: 1.2335\n",
      "====> Epoch: 180 Average loss: 1.2432\n",
      "====> Epoch: 182 Average loss: 1.2462\n",
      "====> Epoch: 184 Average loss: 1.2314\n",
      "====> Epoch: 186 Average loss: 1.2323\n",
      "====> Epoch: 188 Average loss: 1.2260\n",
      "====> Epoch: 190 Average loss: 1.2247\n",
      "====> Epoch: 192 Average loss: 1.2238\n",
      "====> Epoch: 194 Average loss: 1.2277\n",
      "====> Epoch: 196 Average loss: 1.2353\n",
      "====> Epoch: 198 Average loss: 1.2340\n",
      "====> Epoch: 200 Average loss: 1.2313\n",
      "====> Epoch: 202 Average loss: 1.2424\n",
      "====> Epoch: 204 Average loss: 1.2350\n",
      "====> Epoch: 206 Average loss: 1.2219\n",
      "====> Epoch: 208 Average loss: 1.2301\n",
      "====> Epoch: 210 Average loss: 1.2233\n",
      "====> Epoch: 212 Average loss: 1.2310\n",
      "====> Epoch: 214 Average loss: 1.2341\n",
      "====> Epoch: 216 Average loss: 1.2226\n",
      "====> Epoch: 218 Average loss: 1.2226\n",
      "====> Epoch: 220 Average loss: 1.2327\n",
      "====> Epoch: 222 Average loss: 1.2268\n",
      "====> Epoch: 224 Average loss: 1.2234\n",
      "====> Epoch: 226 Average loss: 1.2240\n",
      "====> Epoch: 228 Average loss: 1.2096\n",
      "====> Epoch: 230 Average loss: 1.2264\n",
      "====> Epoch: 232 Average loss: 1.2159\n",
      "====> Epoch: 234 Average loss: 1.2256\n",
      "====> Epoch: 236 Average loss: 1.2236\n",
      "====> Epoch: 238 Average loss: 1.2275\n",
      "====> Epoch: 240 Average loss: 1.2405\n",
      "====> Epoch: 242 Average loss: 1.2270\n",
      "====> Epoch: 244 Average loss: 1.2264\n",
      "====> Epoch: 246 Average loss: 1.2132\n",
      "====> Epoch: 248 Average loss: 1.2242\n",
      "====> Epoch: 250 Average loss: 1.2232\n",
      "====> Epoch: 252 Average loss: 1.2202\n",
      "====> Epoch: 254 Average loss: 1.2405\n",
      "====> Epoch: 256 Average loss: 1.2361\n",
      "====> Epoch: 258 Average loss: 1.2334\n",
      "====> Epoch: 260 Average loss: 1.2220\n",
      "====> Epoch: 262 Average loss: 1.2523\n",
      "====> Epoch: 264 Average loss: 1.2305\n",
      "====> Epoch: 266 Average loss: 1.2356\n",
      "====> Epoch: 268 Average loss: 1.2226\n",
      "====> Epoch: 270 Average loss: 1.2206\n",
      "====> Epoch: 272 Average loss: 1.2489\n",
      "====> Epoch: 274 Average loss: 1.2305\n",
      "====> Epoch: 276 Average loss: 1.2263\n",
      "====> Epoch: 278 Average loss: 1.2197\n",
      "====> Epoch: 280 Average loss: 1.2265\n",
      "====> Epoch: 282 Average loss: 1.2090\n",
      "====> Epoch: 284 Average loss: 1.2358\n",
      "====> Epoch: 286 Average loss: 1.2115\n",
      "====> Epoch: 288 Average loss: 1.2245\n",
      "====> Epoch: 290 Average loss: 1.2219\n",
      "====> Epoch: 292 Average loss: 1.2278\n",
      "====> Epoch: 294 Average loss: 1.2259\n",
      "====> Epoch: 296 Average loss: 1.2320\n",
      "====> Epoch: 298 Average loss: 1.2494\n",
      "====> Epoch: 300 Average loss: 1.2383\n",
      "====> Epoch: 302 Average loss: 1.2217\n",
      "====> Epoch: 304 Average loss: 1.2376\n",
      "====> Epoch: 306 Average loss: 1.2383\n",
      "====> Epoch: 308 Average loss: 1.2303\n",
      "====> Epoch: 310 Average loss: 1.2220\n",
      "====> Epoch: 312 Average loss: 1.2284\n",
      "====> Epoch: 314 Average loss: 1.2390\n",
      "====> Epoch: 316 Average loss: 1.2211\n",
      "====> Epoch: 318 Average loss: 1.2371\n",
      "====> Epoch: 320 Average loss: 1.2321\n",
      "====> Epoch: 322 Average loss: 1.2271\n",
      "====> Epoch: 324 Average loss: 1.2287\n",
      "====> Epoch: 326 Average loss: 1.2237\n",
      "====> Epoch: 328 Average loss: 1.2188\n",
      "====> Epoch: 330 Average loss: 1.2300\n",
      "====> Epoch: 332 Average loss: 1.2283\n",
      "====> Epoch: 334 Average loss: 1.2228\n",
      "====> Epoch: 336 Average loss: 1.2146\n",
      "====> Epoch: 338 Average loss: 1.2318\n",
      "====> Epoch: 340 Average loss: 1.2296\n",
      "====> Epoch: 342 Average loss: 1.2515\n",
      "====> Epoch: 344 Average loss: 1.2303\n",
      "====> Epoch: 346 Average loss: 1.2432\n",
      "====> Epoch: 348 Average loss: 1.2295\n",
      "====> Epoch: 350 Average loss: 1.2132\n",
      "====> Epoch: 352 Average loss: 1.2256\n",
      "====> Epoch: 354 Average loss: 1.2320\n",
      "====> Epoch: 356 Average loss: 1.2195\n",
      "====> Epoch: 358 Average loss: 1.2179\n",
      "====> Epoch: 360 Average loss: 1.2297\n",
      "====> Epoch: 362 Average loss: 1.2284\n",
      "====> Epoch: 364 Average loss: 1.2381\n",
      "====> Epoch: 366 Average loss: 1.2459\n",
      "====> Epoch: 368 Average loss: 1.2324\n",
      "====> Epoch: 370 Average loss: 1.2435\n",
      "====> Epoch: 372 Average loss: 1.2059\n",
      "====> Epoch: 374 Average loss: 1.2281\n",
      "====> Epoch: 376 Average loss: 1.2237\n",
      "====> Epoch: 378 Average loss: 1.2440\n",
      "====> Epoch: 380 Average loss: 1.2196\n",
      "====> Epoch: 382 Average loss: 1.2476\n",
      "====> Epoch: 384 Average loss: 1.2315\n",
      "====> Epoch: 386 Average loss: 1.2329\n",
      "====> Epoch: 388 Average loss: 1.2336\n",
      "====> Epoch: 390 Average loss: 1.2313\n",
      "====> Epoch: 392 Average loss: 1.2326\n",
      "====> Epoch: 394 Average loss: 1.2018\n",
      "====> Epoch: 396 Average loss: 1.2274\n",
      "====> Epoch: 398 Average loss: 1.2146\n",
      "====> Epoch: 400 Average loss: 1.2432\n",
      "====> Epoch: 402 Average loss: 1.2098\n",
      "====> Epoch: 404 Average loss: 1.2268\n",
      "====> Epoch: 406 Average loss: 1.2407\n",
      "====> Epoch: 408 Average loss: 1.2356\n",
      "====> Epoch: 410 Average loss: 1.2236\n",
      "====> Epoch: 412 Average loss: 1.2358\n",
      "====> Epoch: 414 Average loss: 1.2483\n",
      "====> Epoch: 416 Average loss: 1.2090\n",
      "====> Epoch: 418 Average loss: 1.2437\n",
      "====> Epoch: 420 Average loss: 1.2367\n",
      "====> Epoch: 422 Average loss: 1.2335\n",
      "====> Epoch: 424 Average loss: 1.2288\n",
      "====> Epoch: 426 Average loss: 1.2240\n",
      "====> Epoch: 428 Average loss: 1.2278\n",
      "====> Epoch: 430 Average loss: 1.2329\n",
      "====> Epoch: 432 Average loss: 1.2423\n",
      "====> Epoch: 434 Average loss: 1.2425\n",
      "====> Epoch: 436 Average loss: 1.2316\n",
      "====> Epoch: 438 Average loss: 1.2231\n",
      "====> Epoch: 440 Average loss: 1.2244\n",
      "====> Epoch: 442 Average loss: 1.2246\n",
      "====> Epoch: 444 Average loss: 1.2212\n",
      "====> Epoch: 446 Average loss: 1.2424\n",
      "====> Epoch: 448 Average loss: 1.2250\n",
      "====> Epoch: 450 Average loss: 1.2355\n",
      "====> Epoch: 452 Average loss: 1.2133\n",
      "====> Epoch: 454 Average loss: 1.2219\n",
      "====> Epoch: 456 Average loss: 1.2191\n",
      "====> Epoch: 458 Average loss: 1.2351\n",
      "====> Epoch: 460 Average loss: 1.2342\n",
      "====> Epoch: 462 Average loss: 1.2182\n",
      "====> Epoch: 464 Average loss: 1.2312\n",
      "====> Epoch: 466 Average loss: 1.2245\n",
      "====> Epoch: 468 Average loss: 1.2407\n",
      "====> Epoch: 470 Average loss: 1.2507\n",
      "====> Epoch: 472 Average loss: 1.2333\n",
      "====> Epoch: 474 Average loss: 1.2156\n",
      "====> Epoch: 476 Average loss: 1.2193\n",
      "====> Epoch: 478 Average loss: 1.2297\n",
      "====> Epoch: 480 Average loss: 1.2351\n",
      "====> Epoch: 482 Average loss: 1.2294\n",
      "====> Epoch: 484 Average loss: 1.2449\n",
      "====> Epoch: 486 Average loss: 1.2275\n",
      "====> Epoch: 488 Average loss: 1.2240\n",
      "====> Epoch: 490 Average loss: 1.2213\n",
      "====> Epoch: 492 Average loss: 1.2235\n",
      "====> Epoch: 494 Average loss: 1.2373\n",
      "====> Epoch: 496 Average loss: 1.2186\n",
      "====> Epoch: 498 Average loss: 1.2309\n",
      "====> Epoch: 500 Average loss: 1.2234\n",
      "====> Epoch: 502 Average loss: 1.2379\n",
      "====> Epoch: 504 Average loss: 1.2236\n",
      "====> Epoch: 506 Average loss: 1.2159\n",
      "====> Epoch: 508 Average loss: 1.2265\n",
      "====> Epoch: 510 Average loss: 1.2149\n",
      "====> Epoch: 512 Average loss: 1.2290\n",
      "====> Epoch: 514 Average loss: 1.2409\n",
      "====> Epoch: 516 Average loss: 1.2232\n",
      "====> Epoch: 518 Average loss: 1.2358\n",
      "====> Epoch: 520 Average loss: 1.2354\n",
      "====> Epoch: 522 Average loss: 1.2373\n",
      "====> Epoch: 524 Average loss: 1.2221\n",
      "====> Epoch: 526 Average loss: 1.2272\n",
      "====> Epoch: 528 Average loss: 1.2445\n",
      "====> Epoch: 530 Average loss: 1.2210\n",
      "====> Epoch: 532 Average loss: 1.2176\n",
      "====> Epoch: 534 Average loss: 1.2341\n",
      "====> Epoch: 536 Average loss: 1.2406\n",
      "====> Epoch: 538 Average loss: 1.2288\n",
      "====> Epoch: 540 Average loss: 1.2221\n",
      "====> Epoch: 542 Average loss: 1.2246\n",
      "====> Epoch: 544 Average loss: 1.2124\n",
      "====> Epoch: 546 Average loss: 1.2345\n",
      "====> Epoch: 548 Average loss: 1.2108\n",
      "====> Epoch: 550 Average loss: 1.2540\n",
      "====> Epoch: 552 Average loss: 1.2394\n",
      "====> Epoch: 554 Average loss: 1.2312\n",
      "====> Epoch: 556 Average loss: 1.2234\n",
      "====> Epoch: 558 Average loss: 1.2259\n",
      "====> Epoch: 560 Average loss: 1.2229\n",
      "====> Epoch: 562 Average loss: 1.2185\n",
      "====> Epoch: 564 Average loss: 1.2219\n",
      "====> Epoch: 566 Average loss: 1.2286\n",
      "====> Epoch: 568 Average loss: 1.2267\n",
      "====> Epoch: 570 Average loss: 1.2293\n",
      "====> Epoch: 572 Average loss: 1.2323\n",
      "====> Epoch: 574 Average loss: 1.2347\n",
      "====> Epoch: 576 Average loss: 1.2229\n",
      "====> Epoch: 578 Average loss: 1.2276\n",
      "====> Epoch: 580 Average loss: 1.2212\n",
      "====> Epoch: 582 Average loss: 1.2223\n",
      "====> Epoch: 584 Average loss: 1.2178\n",
      "====> Epoch: 586 Average loss: 1.2280\n",
      "====> Epoch: 588 Average loss: 1.2261\n",
      "====> Epoch: 590 Average loss: 1.2148\n",
      "====> Epoch: 592 Average loss: 1.2231\n",
      "====> Epoch: 594 Average loss: 1.2160\n",
      "====> Epoch: 596 Average loss: 1.2407\n",
      "====> Epoch: 598 Average loss: 1.2247\n",
      "====> Epoch: 600 Average loss: 1.2193\n",
      "====> Epoch: 602 Average loss: 1.2364\n",
      "====> Epoch: 604 Average loss: 1.2152\n",
      "====> Epoch: 606 Average loss: 1.2299\n",
      "====> Epoch: 608 Average loss: 1.2129\n",
      "====> Epoch: 610 Average loss: 1.2186\n",
      "====> Epoch: 612 Average loss: 1.2164\n",
      "====> Epoch: 614 Average loss: 1.2310\n",
      "====> Epoch: 616 Average loss: 1.2395\n",
      "====> Epoch: 618 Average loss: 1.2253\n",
      "====> Epoch: 620 Average loss: 1.2251\n",
      "====> Epoch: 622 Average loss: 1.2086\n",
      "====> Epoch: 624 Average loss: 1.2276\n",
      "====> Epoch: 626 Average loss: 1.2279\n",
      "====> Epoch: 628 Average loss: 1.2273\n",
      "====> Epoch: 630 Average loss: 1.2330\n",
      "====> Epoch: 632 Average loss: 1.2313\n",
      "====> Epoch: 634 Average loss: 1.2282\n",
      "====> Epoch: 636 Average loss: 1.2126\n",
      "====> Epoch: 638 Average loss: 1.2315\n",
      "====> Epoch: 640 Average loss: 1.2339\n",
      "====> Epoch: 642 Average loss: 1.2405\n",
      "====> Epoch: 644 Average loss: 1.2419\n",
      "====> Epoch: 646 Average loss: 1.2223\n",
      "====> Epoch: 648 Average loss: 1.2304\n",
      "====> Epoch: 650 Average loss: 1.2132\n",
      "====> Epoch: 652 Average loss: 1.2085\n",
      "====> Epoch: 654 Average loss: 1.2324\n",
      "====> Epoch: 656 Average loss: 1.2283\n",
      "====> Epoch: 658 Average loss: 1.2319\n",
      "====> Epoch: 660 Average loss: 1.2255\n",
      "====> Epoch: 662 Average loss: 1.2056\n",
      "====> Epoch: 664 Average loss: 1.2314\n",
      "====> Epoch: 666 Average loss: 1.2369\n",
      "====> Epoch: 668 Average loss: 1.2305\n",
      "====> Epoch: 670 Average loss: 1.2180\n",
      "====> Epoch: 672 Average loss: 1.2406\n",
      "====> Epoch: 674 Average loss: 1.2351\n",
      "====> Epoch: 676 Average loss: 1.2398\n",
      "====> Epoch: 678 Average loss: 1.2263\n",
      "====> Epoch: 680 Average loss: 1.2137\n",
      "====> Epoch: 682 Average loss: 1.2174\n",
      "====> Epoch: 684 Average loss: 1.2249\n",
      "====> Epoch: 686 Average loss: 1.2457\n",
      "====> Epoch: 688 Average loss: 1.2389\n",
      "====> Epoch: 690 Average loss: 1.2204\n",
      "====> Epoch: 692 Average loss: 1.2339\n",
      "====> Epoch: 694 Average loss: 1.2314\n",
      "====> Epoch: 696 Average loss: 1.2065\n",
      "====> Epoch: 698 Average loss: 1.2293\n",
      "====> Epoch: 700 Average loss: 1.2196\n",
      "====> Epoch: 702 Average loss: 1.2289\n",
      "====> Epoch: 704 Average loss: 1.2222\n",
      "====> Epoch: 706 Average loss: 1.2218\n",
      "====> Epoch: 708 Average loss: 1.2193\n",
      "====> Epoch: 710 Average loss: 1.2213\n",
      "====> Epoch: 712 Average loss: 1.2246\n",
      "====> Epoch: 714 Average loss: 1.2135\n",
      "====> Epoch: 716 Average loss: 1.2215\n",
      "====> Epoch: 718 Average loss: 1.2397\n",
      "====> Epoch: 720 Average loss: 1.2396\n",
      "====> Epoch: 722 Average loss: 1.2407\n",
      "====> Epoch: 724 Average loss: 1.2278\n",
      "====> Epoch: 726 Average loss: 1.2230\n",
      "====> Epoch: 728 Average loss: 1.2435\n",
      "====> Epoch: 730 Average loss: 1.2355\n",
      "====> Epoch: 732 Average loss: 1.2271\n",
      "====> Epoch: 734 Average loss: 1.2361\n",
      "====> Epoch: 736 Average loss: 1.2219\n",
      "====> Epoch: 738 Average loss: 1.2310\n",
      "====> Epoch: 740 Average loss: 1.2271\n",
      "====> Epoch: 742 Average loss: 1.2348\n",
      "====> Epoch: 744 Average loss: 1.2176\n",
      "====> Epoch: 746 Average loss: 1.2320\n",
      "====> Epoch: 748 Average loss: 1.2309\n",
      "====> Epoch: 750 Average loss: 1.2248\n",
      "====> Epoch: 752 Average loss: 1.2356\n",
      "====> Epoch: 754 Average loss: 1.2123\n",
      "====> Epoch: 756 Average loss: 1.2388\n",
      "====> Epoch: 758 Average loss: 1.2258\n",
      "====> Epoch: 760 Average loss: 1.2228\n",
      "====> Epoch: 762 Average loss: 1.2341\n",
      "====> Epoch: 764 Average loss: 1.2137\n",
      "====> Epoch: 766 Average loss: 1.2229\n",
      "====> Epoch: 768 Average loss: 1.2282\n",
      "====> Epoch: 770 Average loss: 1.2184\n",
      "====> Epoch: 772 Average loss: 1.2264\n",
      "====> Epoch: 774 Average loss: 1.2192\n",
      "====> Epoch: 776 Average loss: 1.2077\n",
      "====> Epoch: 778 Average loss: 1.2251\n",
      "====> Epoch: 780 Average loss: 1.2332\n",
      "====> Epoch: 782 Average loss: 1.2394\n",
      "====> Epoch: 784 Average loss: 1.2265\n",
      "====> Epoch: 786 Average loss: 1.2343\n",
      "====> Epoch: 788 Average loss: 1.2352\n",
      "====> Epoch: 790 Average loss: 1.2300\n",
      "====> Epoch: 792 Average loss: 1.2465\n",
      "====> Epoch: 794 Average loss: 1.2246\n",
      "====> Epoch: 796 Average loss: 1.2380\n",
      "====> Epoch: 798 Average loss: 1.2374\n",
      "====> Epoch: 800 Average loss: 1.2174\n",
      "====> Epoch: 802 Average loss: 1.2241\n",
      "====> Epoch: 804 Average loss: 1.2212\n",
      "====> Epoch: 806 Average loss: 1.2430\n",
      "====> Epoch: 808 Average loss: 1.2358\n",
      "====> Epoch: 810 Average loss: 1.2521\n",
      "====> Epoch: 812 Average loss: 1.2304\n",
      "====> Epoch: 814 Average loss: 1.2276\n",
      "====> Epoch: 816 Average loss: 1.2313\n",
      "====> Epoch: 818 Average loss: 1.2361\n",
      "====> Epoch: 820 Average loss: 1.2124\n",
      "====> Epoch: 822 Average loss: 1.2447\n",
      "====> Epoch: 824 Average loss: 1.2213\n",
      "====> Epoch: 826 Average loss: 1.2248\n",
      "====> Epoch: 828 Average loss: 1.2131\n",
      "====> Epoch: 830 Average loss: 1.2273\n",
      "====> Epoch: 832 Average loss: 1.2318\n",
      "====> Epoch: 834 Average loss: 1.2241\n",
      "====> Epoch: 836 Average loss: 1.2291\n",
      "====> Epoch: 838 Average loss: 1.2254\n",
      "====> Epoch: 840 Average loss: 1.2176\n",
      "====> Epoch: 842 Average loss: 1.2201\n",
      "====> Epoch: 844 Average loss: 1.2249\n",
      "====> Epoch: 846 Average loss: 1.2162\n",
      "====> Epoch: 848 Average loss: 1.2222\n",
      "====> Epoch: 850 Average loss: 1.2294\n",
      "====> Epoch: 852 Average loss: 1.2288\n",
      "====> Epoch: 854 Average loss: 1.2361\n",
      "====> Epoch: 856 Average loss: 1.2217\n",
      "====> Epoch: 858 Average loss: 1.2181\n",
      "====> Epoch: 860 Average loss: 1.2376\n",
      "====> Epoch: 862 Average loss: 1.2312\n",
      "====> Epoch: 864 Average loss: 1.2295\n",
      "====> Epoch: 866 Average loss: 1.2253\n",
      "====> Epoch: 868 Average loss: 1.2321\n",
      "====> Epoch: 870 Average loss: 1.2260\n",
      "====> Epoch: 872 Average loss: 1.2311\n",
      "====> Epoch: 874 Average loss: 1.2204\n",
      "====> Epoch: 876 Average loss: 1.2151\n",
      "====> Epoch: 878 Average loss: 1.2303\n",
      "====> Epoch: 880 Average loss: 1.2178\n",
      "====> Epoch: 882 Average loss: 1.2187\n",
      "====> Epoch: 884 Average loss: 1.2276\n",
      "====> Epoch: 886 Average loss: 1.2262\n",
      "====> Epoch: 888 Average loss: 1.2338\n",
      "====> Epoch: 890 Average loss: 1.2188\n",
      "====> Epoch: 892 Average loss: 1.2184\n",
      "====> Epoch: 894 Average loss: 1.2289\n",
      "====> Epoch: 896 Average loss: 1.2362\n",
      "====> Epoch: 898 Average loss: 1.2201\n",
      "====> Epoch: 900 Average loss: 1.2155\n",
      "====> Epoch: 902 Average loss: 1.2417\n",
      "====> Epoch: 904 Average loss: 1.2251\n",
      "====> Epoch: 906 Average loss: 1.2175\n",
      "====> Epoch: 908 Average loss: 1.2312\n",
      "====> Epoch: 910 Average loss: 1.2257\n",
      "====> Epoch: 912 Average loss: 1.2352\n",
      "====> Epoch: 914 Average loss: 1.2328\n",
      "====> Epoch: 916 Average loss: 1.2169\n",
      "====> Epoch: 918 Average loss: 1.2290\n",
      "====> Epoch: 920 Average loss: 1.2264\n",
      "====> Epoch: 922 Average loss: 1.2314\n",
      "====> Epoch: 924 Average loss: 1.2256\n",
      "====> Epoch: 926 Average loss: 1.2202\n",
      "====> Epoch: 928 Average loss: 1.2307\n",
      "====> Epoch: 930 Average loss: 1.2180\n",
      "====> Epoch: 932 Average loss: 1.2302\n",
      "====> Epoch: 934 Average loss: 1.2261\n",
      "====> Epoch: 936 Average loss: 1.2329\n",
      "====> Epoch: 938 Average loss: 1.2357\n",
      "====> Epoch: 940 Average loss: 1.2368\n",
      "====> Epoch: 942 Average loss: 1.2215\n",
      "====> Epoch: 944 Average loss: 1.2186\n",
      "====> Epoch: 946 Average loss: 1.2171\n",
      "====> Epoch: 948 Average loss: 1.2276\n",
      "====> Epoch: 950 Average loss: 1.2182\n",
      "====> Epoch: 952 Average loss: 1.2289\n",
      "====> Epoch: 954 Average loss: 1.2252\n",
      "====> Epoch: 956 Average loss: 1.2207\n",
      "====> Epoch: 958 Average loss: 1.2229\n",
      "====> Epoch: 960 Average loss: 1.2301\n",
      "====> Epoch: 962 Average loss: 1.2305\n",
      "====> Epoch: 964 Average loss: 1.2401\n",
      "====> Epoch: 966 Average loss: 1.2260\n",
      "====> Epoch: 968 Average loss: 1.2312\n",
      "====> Epoch: 970 Average loss: 1.2281\n",
      "====> Epoch: 972 Average loss: 1.2274\n",
      "====> Epoch: 974 Average loss: 1.2356\n",
      "====> Epoch: 976 Average loss: 1.2375\n",
      "====> Epoch: 978 Average loss: 1.2205\n",
      "====> Epoch: 980 Average loss: 1.2324\n",
      "====> Epoch: 982 Average loss: 1.2415\n",
      "====> Epoch: 984 Average loss: 1.2277\n",
      "====> Epoch: 986 Average loss: 1.2280\n",
      "====> Epoch: 988 Average loss: 1.2240\n",
      "====> Epoch: 990 Average loss: 1.2335\n",
      "====> Epoch: 992 Average loss: 1.2213\n",
      "====> Epoch: 994 Average loss: 1.2336\n",
      "====> Epoch: 996 Average loss: 1.2392\n",
      "====> Epoch: 998 Average loss: 1.2252\n",
      "====> Epoch: 1000 Average loss: 1.2362\n"
     ]
    }
   ],
   "source": [
    "model_b3, dataset_b3 = train_encoder(\"client_n_data/bank_clients/bank_3.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_client_n_for_diabetes_ds(model_b1, 'client_n_data/bank_clients/bank_1.csv', 'client_n_data/latent/bank_1.csv')\n",
    "generate_latent_client_n_for_diabetes_ds(model_b2, 'client_n_data/bank_clients/bank_2.csv', 'client_n_data/latent/bank_2.csv')\n",
    "generate_latent_client_n_for_diabetes_ds(model_b3, 'client_n_data/bank_clients/bank_3.csv', 'client_n_data/latent/bank_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio N Client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>99993</td>\n",
       "      <td>19240</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>99995</td>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>99996</td>\n",
       "      <td>19066</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>99998</td>\n",
       "      <td>22431</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>99999</td>\n",
       "      <td>20540</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  \\\n",
       "0          0  18393       2     168    62.0    110     80            1     1   \n",
       "1          1  20228       1     156    85.0    140     90            3     1   \n",
       "2          2  18857       1     165    64.0    130     70            3     1   \n",
       "3          3  17623       2     169    82.0    150    100            1     1   \n",
       "4          4  17474       1     156    56.0    100     60            1     1   \n",
       "...      ...    ...     ...     ...     ...    ...    ...          ...   ...   \n",
       "69995  99993  19240       2     168    76.0    120     80            1     1   \n",
       "69996  99995  22601       1     158   126.0    140     90            2     2   \n",
       "69997  99996  19066       2     183   105.0    180     90            3     1   \n",
       "69998  99998  22431       1     163    72.0    135     80            1     2   \n",
       "69999  99999  20540       1     170    72.0    120     80            2     1   \n",
       "\n",
       "       smoke  alco  active  cardio  \n",
       "0          0     0       1       0  \n",
       "1          0     0       1       1  \n",
       "2          0     0       0       1  \n",
       "3          0     0       1       1  \n",
       "4          0     0       0       0  \n",
       "...      ...   ...     ...     ...  \n",
       "69995      1     0       1       0  \n",
       "69996      0     0       1       1  \n",
       "69997      0     1       0       1  \n",
       "69998      0     0       0       1  \n",
       "69999      0     0       1       0  \n",
       "\n",
       "[70000 rows x 13 columns]"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_data_all_client = pd.read_csv(\"Data/cardio_train.csv\", sep=\";\")\n",
    "cardio_data_all_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = cardio_data_all_client.iloc[:, -1:]\n",
    "df_client_1 = cardio_data_all_client.iloc[:, :4]\n",
    "df_client_2 = cardio_data_all_client.iloc[:, 4:8]\n",
    "df_client_3 = cardio_data_all_client.iloc[:, 8:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_1 = pd.concat([df_client_1, df_label], axis=1)\n",
    "df_client_1.to_csv(\"client_n_data/cardio_clients/cardio_1.csv\", index=False)\n",
    "\n",
    "df_client_2 = pd.concat([df_client_2, df_label], axis=1)\n",
    "df_client_2.to_csv(\"client_n_data/cardio_clients/cardio_2.csv\", index=False)\n",
    "\n",
    "df_client_3 = pd.concat([df_client_3, df_label], axis=1)\n",
    "df_client_3.to_csv(\"client_n_data/cardio_clients/cardio_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 2.9341\n",
      "====> Epoch: 4 Average loss: 2.8984\n",
      "====> Epoch: 6 Average loss: 2.8993\n",
      "====> Epoch: 8 Average loss: 2.8941\n",
      "====> Epoch: 10 Average loss: 2.8949\n",
      "====> Epoch: 12 Average loss: 2.8887\n",
      "====> Epoch: 14 Average loss: 2.8923\n",
      "====> Epoch: 16 Average loss: 2.8933\n",
      "====> Epoch: 18 Average loss: 2.8815\n",
      "====> Epoch: 20 Average loss: 2.8928\n",
      "====> Epoch: 22 Average loss: 2.8910\n",
      "====> Epoch: 24 Average loss: 2.8858\n",
      "====> Epoch: 26 Average loss: 2.8900\n",
      "====> Epoch: 28 Average loss: 2.8879\n",
      "====> Epoch: 30 Average loss: 2.8904\n",
      "====> Epoch: 32 Average loss: 2.8890\n",
      "====> Epoch: 34 Average loss: 2.8867\n",
      "====> Epoch: 36 Average loss: 2.8889\n",
      "====> Epoch: 38 Average loss: 2.8841\n",
      "====> Epoch: 40 Average loss: 2.8829\n",
      "====> Epoch: 42 Average loss: 2.8885\n",
      "====> Epoch: 44 Average loss: 2.8913\n",
      "====> Epoch: 46 Average loss: 2.8864\n",
      "====> Epoch: 48 Average loss: 2.8905\n",
      "====> Epoch: 50 Average loss: 2.8834\n",
      "====> Epoch: 52 Average loss: 2.8849\n",
      "====> Epoch: 54 Average loss: 2.8823\n",
      "====> Epoch: 56 Average loss: 2.8826\n",
      "====> Epoch: 58 Average loss: 2.8849\n",
      "====> Epoch: 60 Average loss: 2.8805\n",
      "====> Epoch: 62 Average loss: 2.8857\n",
      "====> Epoch: 64 Average loss: 2.8835\n",
      "====> Epoch: 66 Average loss: 2.8906\n",
      "====> Epoch: 68 Average loss: 2.8848\n",
      "====> Epoch: 70 Average loss: 2.8845\n",
      "====> Epoch: 72 Average loss: 2.8823\n",
      "====> Epoch: 74 Average loss: 2.8864\n",
      "====> Epoch: 76 Average loss: 2.8852\n",
      "====> Epoch: 78 Average loss: 2.8846\n",
      "====> Epoch: 80 Average loss: 2.8865\n",
      "====> Epoch: 82 Average loss: 2.8871\n",
      "====> Epoch: 84 Average loss: 2.8899\n",
      "====> Epoch: 86 Average loss: 2.8846\n",
      "====> Epoch: 88 Average loss: 2.8867\n",
      "====> Epoch: 90 Average loss: 2.8810\n",
      "====> Epoch: 92 Average loss: 2.8852\n",
      "====> Epoch: 94 Average loss: 2.8798\n",
      "====> Epoch: 96 Average loss: 2.8806\n",
      "====> Epoch: 98 Average loss: 2.8772\n",
      "====> Epoch: 100 Average loss: 2.8812\n",
      "====> Epoch: 102 Average loss: 2.8806\n",
      "====> Epoch: 104 Average loss: 2.8859\n",
      "====> Epoch: 106 Average loss: 2.8807\n",
      "====> Epoch: 108 Average loss: 2.8879\n",
      "====> Epoch: 110 Average loss: 2.8825\n",
      "====> Epoch: 112 Average loss: 2.8858\n",
      "====> Epoch: 114 Average loss: 2.8846\n",
      "====> Epoch: 116 Average loss: 2.8858\n",
      "====> Epoch: 118 Average loss: 2.8848\n",
      "====> Epoch: 120 Average loss: 2.8867\n",
      "====> Epoch: 122 Average loss: 2.8867\n",
      "====> Epoch: 124 Average loss: 2.8833\n",
      "====> Epoch: 126 Average loss: 2.8840\n",
      "====> Epoch: 128 Average loss: 2.8893\n",
      "====> Epoch: 130 Average loss: 2.8858\n",
      "====> Epoch: 132 Average loss: 2.8853\n",
      "====> Epoch: 134 Average loss: 2.8799\n",
      "====> Epoch: 136 Average loss: 2.8855\n",
      "====> Epoch: 138 Average loss: 2.8854\n",
      "====> Epoch: 140 Average loss: 2.8923\n",
      "====> Epoch: 142 Average loss: 2.8864\n",
      "====> Epoch: 144 Average loss: 2.8834\n",
      "====> Epoch: 146 Average loss: 2.8884\n",
      "====> Epoch: 148 Average loss: 2.8904\n",
      "====> Epoch: 150 Average loss: 2.8839\n",
      "====> Epoch: 152 Average loss: 2.8841\n",
      "====> Epoch: 154 Average loss: 2.8833\n",
      "====> Epoch: 156 Average loss: 2.8827\n",
      "====> Epoch: 158 Average loss: 2.8808\n",
      "====> Epoch: 160 Average loss: 2.8882\n",
      "====> Epoch: 162 Average loss: 2.8861\n",
      "====> Epoch: 164 Average loss: 2.8879\n",
      "====> Epoch: 166 Average loss: 2.8834\n",
      "====> Epoch: 168 Average loss: 2.8813\n",
      "====> Epoch: 170 Average loss: 2.8851\n",
      "====> Epoch: 172 Average loss: 2.8833\n",
      "====> Epoch: 174 Average loss: 2.8868\n",
      "====> Epoch: 176 Average loss: 2.8883\n",
      "====> Epoch: 178 Average loss: 2.8841\n",
      "====> Epoch: 180 Average loss: 2.8823\n",
      "====> Epoch: 182 Average loss: 2.8877\n",
      "====> Epoch: 184 Average loss: 2.8858\n",
      "====> Epoch: 186 Average loss: 2.8880\n",
      "====> Epoch: 188 Average loss: 2.8793\n",
      "====> Epoch: 190 Average loss: 2.8924\n",
      "====> Epoch: 192 Average loss: 2.8863\n",
      "====> Epoch: 194 Average loss: 2.8862\n",
      "====> Epoch: 196 Average loss: 2.8806\n",
      "====> Epoch: 198 Average loss: 2.8823\n",
      "====> Epoch: 200 Average loss: 2.8843\n",
      "====> Epoch: 202 Average loss: 2.8821\n",
      "====> Epoch: 204 Average loss: 2.8838\n",
      "====> Epoch: 206 Average loss: 2.8802\n",
      "====> Epoch: 208 Average loss: 2.8867\n",
      "====> Epoch: 210 Average loss: 2.8763\n",
      "====> Epoch: 212 Average loss: 2.8846\n",
      "====> Epoch: 214 Average loss: 2.8879\n",
      "====> Epoch: 216 Average loss: 2.8883\n",
      "====> Epoch: 218 Average loss: 2.8878\n",
      "====> Epoch: 220 Average loss: 2.8784\n",
      "====> Epoch: 222 Average loss: 2.8797\n",
      "====> Epoch: 224 Average loss: 2.8829\n",
      "====> Epoch: 226 Average loss: 2.8870\n",
      "====> Epoch: 228 Average loss: 2.8786\n",
      "====> Epoch: 230 Average loss: 2.8776\n",
      "====> Epoch: 232 Average loss: 2.8867\n",
      "====> Epoch: 234 Average loss: 2.8897\n",
      "====> Epoch: 236 Average loss: 2.8871\n",
      "====> Epoch: 238 Average loss: 2.8780\n",
      "====> Epoch: 240 Average loss: 2.8767\n",
      "====> Epoch: 242 Average loss: 2.8830\n",
      "====> Epoch: 244 Average loss: 2.8864\n",
      "====> Epoch: 246 Average loss: 2.8865\n",
      "====> Epoch: 248 Average loss: 2.8800\n",
      "====> Epoch: 250 Average loss: 2.8790\n",
      "====> Epoch: 252 Average loss: 2.8850\n",
      "====> Epoch: 254 Average loss: 2.8774\n",
      "====> Epoch: 256 Average loss: 2.8792\n",
      "====> Epoch: 258 Average loss: 2.8828\n",
      "====> Epoch: 260 Average loss: 2.8866\n",
      "====> Epoch: 262 Average loss: 2.8799\n",
      "====> Epoch: 264 Average loss: 2.8783\n",
      "====> Epoch: 266 Average loss: 2.8838\n",
      "====> Epoch: 268 Average loss: 2.8844\n",
      "====> Epoch: 270 Average loss: 2.8760\n",
      "====> Epoch: 272 Average loss: 2.8887\n",
      "====> Epoch: 274 Average loss: 2.8831\n",
      "====> Epoch: 276 Average loss: 2.8828\n",
      "====> Epoch: 278 Average loss: 2.8872\n",
      "====> Epoch: 280 Average loss: 2.8800\n",
      "====> Epoch: 282 Average loss: 2.8789\n",
      "====> Epoch: 284 Average loss: 2.8773\n",
      "====> Epoch: 286 Average loss: 2.8857\n",
      "====> Epoch: 288 Average loss: 2.8816\n",
      "====> Epoch: 290 Average loss: 2.8804\n",
      "====> Epoch: 292 Average loss: 2.8891\n",
      "====> Epoch: 294 Average loss: 2.8878\n",
      "====> Epoch: 296 Average loss: 2.8886\n",
      "====> Epoch: 298 Average loss: 2.8883\n",
      "====> Epoch: 300 Average loss: 2.8823\n",
      "====> Epoch: 302 Average loss: 2.8802\n",
      "====> Epoch: 304 Average loss: 2.8792\n",
      "====> Epoch: 306 Average loss: 2.8869\n",
      "====> Epoch: 308 Average loss: 2.8855\n",
      "====> Epoch: 310 Average loss: 2.8820\n",
      "====> Epoch: 312 Average loss: 2.8861\n",
      "====> Epoch: 314 Average loss: 2.8868\n",
      "====> Epoch: 316 Average loss: 2.8848\n",
      "====> Epoch: 318 Average loss: 2.8798\n",
      "====> Epoch: 320 Average loss: 2.8827\n",
      "====> Epoch: 322 Average loss: 2.8800\n",
      "====> Epoch: 324 Average loss: 2.8790\n",
      "====> Epoch: 326 Average loss: 2.8809\n",
      "====> Epoch: 328 Average loss: 2.8876\n",
      "====> Epoch: 330 Average loss: 2.8752\n",
      "====> Epoch: 332 Average loss: 2.8814\n",
      "====> Epoch: 334 Average loss: 2.8786\n",
      "====> Epoch: 336 Average loss: 2.8818\n",
      "====> Epoch: 338 Average loss: 2.8844\n",
      "====> Epoch: 340 Average loss: 2.8834\n",
      "====> Epoch: 342 Average loss: 2.8802\n",
      "====> Epoch: 344 Average loss: 2.8815\n",
      "====> Epoch: 346 Average loss: 2.8820\n",
      "====> Epoch: 348 Average loss: 2.8770\n",
      "====> Epoch: 350 Average loss: 2.8812\n",
      "====> Epoch: 352 Average loss: 2.8868\n",
      "====> Epoch: 354 Average loss: 2.8838\n",
      "====> Epoch: 356 Average loss: 2.8873\n",
      "====> Epoch: 358 Average loss: 2.8818\n",
      "====> Epoch: 360 Average loss: 2.8809\n",
      "====> Epoch: 362 Average loss: 2.8835\n",
      "====> Epoch: 364 Average loss: 2.8849\n",
      "====> Epoch: 366 Average loss: 2.8866\n",
      "====> Epoch: 368 Average loss: 2.8867\n",
      "====> Epoch: 370 Average loss: 2.8830\n",
      "====> Epoch: 372 Average loss: 2.8806\n",
      "====> Epoch: 374 Average loss: 2.8796\n",
      "====> Epoch: 376 Average loss: 2.8842\n",
      "====> Epoch: 378 Average loss: 2.8814\n",
      "====> Epoch: 380 Average loss: 2.8836\n",
      "====> Epoch: 382 Average loss: 2.8823\n",
      "====> Epoch: 384 Average loss: 2.8790\n",
      "====> Epoch: 386 Average loss: 2.8824\n",
      "====> Epoch: 388 Average loss: 2.8813\n",
      "====> Epoch: 390 Average loss: 2.8820\n",
      "====> Epoch: 392 Average loss: 2.8791\n",
      "====> Epoch: 394 Average loss: 2.8814\n",
      "====> Epoch: 396 Average loss: 2.8818\n",
      "====> Epoch: 398 Average loss: 2.8874\n",
      "====> Epoch: 400 Average loss: 2.8825\n",
      "====> Epoch: 402 Average loss: 2.8785\n",
      "====> Epoch: 404 Average loss: 2.8793\n",
      "====> Epoch: 406 Average loss: 2.8815\n",
      "====> Epoch: 408 Average loss: 2.8833\n",
      "====> Epoch: 410 Average loss: 2.8886\n",
      "====> Epoch: 412 Average loss: 2.8866\n",
      "====> Epoch: 414 Average loss: 2.8821\n",
      "====> Epoch: 416 Average loss: 2.8860\n",
      "====> Epoch: 418 Average loss: 2.8801\n",
      "====> Epoch: 420 Average loss: 2.8772\n",
      "====> Epoch: 422 Average loss: 2.8794\n",
      "====> Epoch: 424 Average loss: 2.8822\n",
      "====> Epoch: 426 Average loss: 2.8788\n",
      "====> Epoch: 428 Average loss: 2.8835\n",
      "====> Epoch: 430 Average loss: 2.8848\n",
      "====> Epoch: 432 Average loss: 2.8796\n",
      "====> Epoch: 434 Average loss: 2.8774\n",
      "====> Epoch: 436 Average loss: 2.8814\n",
      "====> Epoch: 438 Average loss: 2.8789\n",
      "====> Epoch: 440 Average loss: 2.8730\n",
      "====> Epoch: 442 Average loss: 2.8771\n",
      "====> Epoch: 444 Average loss: 2.8816\n",
      "====> Epoch: 446 Average loss: 2.8821\n",
      "====> Epoch: 448 Average loss: 2.8874\n",
      "====> Epoch: 450 Average loss: 2.8878\n",
      "====> Epoch: 452 Average loss: 2.8857\n",
      "====> Epoch: 454 Average loss: 2.8760\n",
      "====> Epoch: 456 Average loss: 2.8827\n",
      "====> Epoch: 458 Average loss: 2.8844\n",
      "====> Epoch: 460 Average loss: 2.8845\n",
      "====> Epoch: 462 Average loss: 2.8823\n",
      "====> Epoch: 464 Average loss: 2.8877\n",
      "====> Epoch: 466 Average loss: 2.8808\n",
      "====> Epoch: 468 Average loss: 2.8838\n",
      "====> Epoch: 470 Average loss: 2.8844\n",
      "====> Epoch: 472 Average loss: 2.8796\n",
      "====> Epoch: 474 Average loss: 2.8744\n",
      "====> Epoch: 476 Average loss: 2.8799\n",
      "====> Epoch: 478 Average loss: 2.8764\n",
      "====> Epoch: 480 Average loss: 2.8837\n",
      "====> Epoch: 482 Average loss: 2.8798\n",
      "====> Epoch: 484 Average loss: 2.8848\n",
      "====> Epoch: 486 Average loss: 2.8912\n",
      "====> Epoch: 488 Average loss: 2.8832\n",
      "====> Epoch: 490 Average loss: 2.8829\n",
      "====> Epoch: 492 Average loss: 2.8803\n",
      "====> Epoch: 494 Average loss: 2.8845\n",
      "====> Epoch: 496 Average loss: 2.8842\n",
      "====> Epoch: 498 Average loss: 2.8846\n",
      "====> Epoch: 500 Average loss: 2.8794\n",
      "====> Epoch: 502 Average loss: 2.8861\n",
      "====> Epoch: 504 Average loss: 2.8857\n",
      "====> Epoch: 506 Average loss: 2.8802\n",
      "====> Epoch: 508 Average loss: 2.8840\n",
      "====> Epoch: 510 Average loss: 2.8855\n",
      "====> Epoch: 512 Average loss: 2.8864\n",
      "====> Epoch: 514 Average loss: 2.8794\n",
      "====> Epoch: 516 Average loss: 2.8821\n",
      "====> Epoch: 518 Average loss: 2.8811\n",
      "====> Epoch: 520 Average loss: 2.8801\n",
      "====> Epoch: 522 Average loss: 2.8874\n",
      "====> Epoch: 524 Average loss: 2.8855\n",
      "====> Epoch: 526 Average loss: 2.8765\n",
      "====> Epoch: 528 Average loss: 2.8835\n",
      "====> Epoch: 530 Average loss: 2.8833\n",
      "====> Epoch: 532 Average loss: 2.8852\n",
      "====> Epoch: 534 Average loss: 2.8816\n",
      "====> Epoch: 536 Average loss: 2.8831\n",
      "====> Epoch: 538 Average loss: 2.8847\n",
      "====> Epoch: 540 Average loss: 2.8816\n",
      "====> Epoch: 542 Average loss: 2.8827\n",
      "====> Epoch: 544 Average loss: 2.8849\n",
      "====> Epoch: 546 Average loss: 2.8766\n",
      "====> Epoch: 548 Average loss: 2.8830\n",
      "====> Epoch: 550 Average loss: 2.8803\n",
      "====> Epoch: 552 Average loss: 2.8839\n",
      "====> Epoch: 554 Average loss: 2.8806\n",
      "====> Epoch: 556 Average loss: 2.8813\n",
      "====> Epoch: 558 Average loss: 2.8743\n",
      "====> Epoch: 560 Average loss: 2.8735\n",
      "====> Epoch: 562 Average loss: 2.8789\n",
      "====> Epoch: 564 Average loss: 2.8835\n",
      "====> Epoch: 566 Average loss: 2.8813\n",
      "====> Epoch: 568 Average loss: 2.8767\n",
      "====> Epoch: 570 Average loss: 2.8823\n",
      "====> Epoch: 572 Average loss: 2.8807\n",
      "====> Epoch: 574 Average loss: 2.8810\n",
      "====> Epoch: 576 Average loss: 2.8881\n",
      "====> Epoch: 578 Average loss: 2.8817\n",
      "====> Epoch: 580 Average loss: 2.8857\n",
      "====> Epoch: 582 Average loss: 2.8833\n",
      "====> Epoch: 584 Average loss: 2.8817\n",
      "====> Epoch: 586 Average loss: 2.8820\n",
      "====> Epoch: 588 Average loss: 2.8854\n",
      "====> Epoch: 590 Average loss: 2.8761\n",
      "====> Epoch: 592 Average loss: 2.8781\n",
      "====> Epoch: 594 Average loss: 2.8852\n",
      "====> Epoch: 596 Average loss: 2.8835\n",
      "====> Epoch: 598 Average loss: 2.8846\n",
      "====> Epoch: 600 Average loss: 2.8808\n",
      "====> Epoch: 602 Average loss: 2.8835\n",
      "====> Epoch: 604 Average loss: 2.8836\n",
      "====> Epoch: 606 Average loss: 2.8794\n",
      "====> Epoch: 608 Average loss: 2.8802\n",
      "====> Epoch: 610 Average loss: 2.8860\n",
      "====> Epoch: 612 Average loss: 2.8801\n",
      "====> Epoch: 614 Average loss: 2.8820\n",
      "====> Epoch: 616 Average loss: 2.8801\n",
      "====> Epoch: 618 Average loss: 2.8823\n",
      "====> Epoch: 620 Average loss: 2.8806\n",
      "====> Epoch: 622 Average loss: 2.8837\n",
      "====> Epoch: 624 Average loss: 2.8807\n",
      "====> Epoch: 626 Average loss: 2.8868\n",
      "====> Epoch: 628 Average loss: 2.8809\n",
      "====> Epoch: 630 Average loss: 2.8798\n",
      "====> Epoch: 632 Average loss: 2.8798\n",
      "====> Epoch: 634 Average loss: 2.8856\n",
      "====> Epoch: 636 Average loss: 2.8753\n",
      "====> Epoch: 638 Average loss: 2.8750\n",
      "====> Epoch: 640 Average loss: 2.8791\n",
      "====> Epoch: 642 Average loss: 2.8857\n",
      "====> Epoch: 644 Average loss: 2.8850\n",
      "====> Epoch: 646 Average loss: 2.8750\n",
      "====> Epoch: 648 Average loss: 2.8863\n",
      "====> Epoch: 650 Average loss: 2.8844\n",
      "====> Epoch: 652 Average loss: 2.8798\n",
      "====> Epoch: 654 Average loss: 2.8794\n",
      "====> Epoch: 656 Average loss: 2.8829\n",
      "====> Epoch: 658 Average loss: 2.8891\n",
      "====> Epoch: 660 Average loss: 2.8832\n",
      "====> Epoch: 662 Average loss: 2.8822\n",
      "====> Epoch: 664 Average loss: 2.8825\n",
      "====> Epoch: 666 Average loss: 2.8878\n",
      "====> Epoch: 668 Average loss: 2.8851\n",
      "====> Epoch: 670 Average loss: 2.8796\n",
      "====> Epoch: 672 Average loss: 2.8825\n",
      "====> Epoch: 674 Average loss: 2.8835\n",
      "====> Epoch: 676 Average loss: 2.8806\n",
      "====> Epoch: 678 Average loss: 2.8837\n",
      "====> Epoch: 680 Average loss: 2.8792\n",
      "====> Epoch: 682 Average loss: 2.8777\n",
      "====> Epoch: 684 Average loss: 2.8858\n",
      "====> Epoch: 686 Average loss: 2.8850\n",
      "====> Epoch: 688 Average loss: 2.8791\n",
      "====> Epoch: 690 Average loss: 2.8878\n",
      "====> Epoch: 692 Average loss: 2.8821\n",
      "====> Epoch: 694 Average loss: 2.8809\n",
      "====> Epoch: 696 Average loss: 2.8832\n",
      "====> Epoch: 698 Average loss: 2.8822\n",
      "====> Epoch: 700 Average loss: 2.8817\n",
      "====> Epoch: 702 Average loss: 2.8811\n",
      "====> Epoch: 704 Average loss: 2.8859\n",
      "====> Epoch: 706 Average loss: 2.8799\n",
      "====> Epoch: 708 Average loss: 2.8808\n",
      "====> Epoch: 710 Average loss: 2.8809\n",
      "====> Epoch: 712 Average loss: 2.8810\n",
      "====> Epoch: 714 Average loss: 2.8787\n",
      "====> Epoch: 716 Average loss: 2.8839\n",
      "====> Epoch: 718 Average loss: 2.8776\n",
      "====> Epoch: 720 Average loss: 2.8813\n",
      "====> Epoch: 722 Average loss: 2.8820\n",
      "====> Epoch: 724 Average loss: 2.8789\n",
      "====> Epoch: 726 Average loss: 2.8826\n",
      "====> Epoch: 728 Average loss: 2.8828\n",
      "====> Epoch: 730 Average loss: 2.8788\n",
      "====> Epoch: 732 Average loss: 2.8808\n",
      "====> Epoch: 734 Average loss: 2.8814\n",
      "====> Epoch: 736 Average loss: 2.8797\n",
      "====> Epoch: 738 Average loss: 2.8871\n",
      "====> Epoch: 740 Average loss: 2.8857\n",
      "====> Epoch: 742 Average loss: 2.8848\n",
      "====> Epoch: 744 Average loss: 2.8827\n",
      "====> Epoch: 746 Average loss: 2.8874\n",
      "====> Epoch: 748 Average loss: 2.8854\n",
      "====> Epoch: 750 Average loss: 2.8769\n",
      "====> Epoch: 752 Average loss: 2.8826\n",
      "====> Epoch: 754 Average loss: 2.8788\n",
      "====> Epoch: 756 Average loss: 2.8794\n",
      "====> Epoch: 758 Average loss: 2.8794\n",
      "====> Epoch: 760 Average loss: 2.8851\n",
      "====> Epoch: 762 Average loss: 2.8884\n",
      "====> Epoch: 764 Average loss: 2.8812\n",
      "====> Epoch: 766 Average loss: 2.8879\n",
      "====> Epoch: 768 Average loss: 2.8865\n",
      "====> Epoch: 770 Average loss: 2.8829\n",
      "====> Epoch: 772 Average loss: 2.8801\n",
      "====> Epoch: 774 Average loss: 2.8769\n",
      "====> Epoch: 776 Average loss: 2.8808\n",
      "====> Epoch: 778 Average loss: 2.8846\n",
      "====> Epoch: 780 Average loss: 2.8865\n",
      "====> Epoch: 782 Average loss: 2.8830\n",
      "====> Epoch: 784 Average loss: 2.8876\n",
      "====> Epoch: 786 Average loss: 2.8749\n",
      "====> Epoch: 788 Average loss: 2.8849\n",
      "====> Epoch: 790 Average loss: 2.8852\n",
      "====> Epoch: 792 Average loss: 2.8865\n",
      "====> Epoch: 794 Average loss: 2.8772\n",
      "====> Epoch: 796 Average loss: 2.8804\n",
      "====> Epoch: 798 Average loss: 2.8906\n",
      "====> Epoch: 800 Average loss: 2.8821\n",
      "====> Epoch: 802 Average loss: 2.8894\n",
      "====> Epoch: 804 Average loss: 2.8813\n",
      "====> Epoch: 806 Average loss: 2.8841\n",
      "====> Epoch: 808 Average loss: 2.8792\n",
      "====> Epoch: 810 Average loss: 2.8810\n",
      "====> Epoch: 812 Average loss: 2.8864\n",
      "====> Epoch: 814 Average loss: 2.8855\n",
      "====> Epoch: 816 Average loss: 2.8820\n",
      "====> Epoch: 818 Average loss: 2.8824\n",
      "====> Epoch: 820 Average loss: 2.8861\n",
      "====> Epoch: 822 Average loss: 2.8747\n",
      "====> Epoch: 824 Average loss: 2.8859\n",
      "====> Epoch: 826 Average loss: 2.8872\n",
      "====> Epoch: 828 Average loss: 2.8810\n",
      "====> Epoch: 830 Average loss: 2.8878\n",
      "====> Epoch: 832 Average loss: 2.8801\n",
      "====> Epoch: 834 Average loss: 2.8867\n",
      "====> Epoch: 836 Average loss: 2.8826\n",
      "====> Epoch: 838 Average loss: 2.8794\n",
      "====> Epoch: 840 Average loss: 2.8760\n",
      "====> Epoch: 842 Average loss: 2.8775\n",
      "====> Epoch: 844 Average loss: 2.8861\n",
      "====> Epoch: 846 Average loss: 2.8783\n",
      "====> Epoch: 848 Average loss: 2.8819\n",
      "====> Epoch: 850 Average loss: 2.8857\n",
      "====> Epoch: 852 Average loss: 2.8802\n",
      "====> Epoch: 854 Average loss: 2.8874\n",
      "====> Epoch: 856 Average loss: 2.8834\n",
      "====> Epoch: 858 Average loss: 2.8864\n",
      "====> Epoch: 860 Average loss: 2.8825\n",
      "====> Epoch: 862 Average loss: 2.8829\n",
      "====> Epoch: 864 Average loss: 2.8783\n",
      "====> Epoch: 866 Average loss: 2.8810\n",
      "====> Epoch: 868 Average loss: 2.8829\n",
      "====> Epoch: 870 Average loss: 2.8798\n",
      "====> Epoch: 872 Average loss: 2.8839\n",
      "====> Epoch: 874 Average loss: 2.8831\n",
      "====> Epoch: 876 Average loss: 2.8801\n",
      "====> Epoch: 878 Average loss: 2.8738\n",
      "====> Epoch: 880 Average loss: 2.8810\n",
      "====> Epoch: 882 Average loss: 2.8805\n",
      "====> Epoch: 884 Average loss: 2.8798\n",
      "====> Epoch: 886 Average loss: 2.8776\n",
      "====> Epoch: 888 Average loss: 2.8872\n",
      "====> Epoch: 890 Average loss: 2.8793\n",
      "====> Epoch: 892 Average loss: 2.8801\n",
      "====> Epoch: 894 Average loss: 2.8887\n",
      "====> Epoch: 896 Average loss: 2.8764\n",
      "====> Epoch: 898 Average loss: 2.8845\n",
      "====> Epoch: 900 Average loss: 2.8833\n",
      "====> Epoch: 902 Average loss: 2.8800\n",
      "====> Epoch: 904 Average loss: 2.8856\n",
      "====> Epoch: 906 Average loss: 2.8813\n",
      "====> Epoch: 908 Average loss: 2.8782\n",
      "====> Epoch: 910 Average loss: 2.8752\n",
      "====> Epoch: 912 Average loss: 2.8817\n",
      "====> Epoch: 914 Average loss: 2.8793\n",
      "====> Epoch: 916 Average loss: 2.8822\n",
      "====> Epoch: 918 Average loss: 2.8839\n",
      "====> Epoch: 920 Average loss: 2.8813\n",
      "====> Epoch: 922 Average loss: 2.8778\n",
      "====> Epoch: 924 Average loss: 2.8824\n",
      "====> Epoch: 926 Average loss: 2.8830\n",
      "====> Epoch: 928 Average loss: 2.8839\n",
      "====> Epoch: 930 Average loss: 2.8798\n",
      "====> Epoch: 932 Average loss: 2.8824\n",
      "====> Epoch: 934 Average loss: 2.8758\n",
      "====> Epoch: 936 Average loss: 2.8825\n",
      "====> Epoch: 938 Average loss: 2.8743\n",
      "====> Epoch: 940 Average loss: 2.8842\n",
      "====> Epoch: 942 Average loss: 2.8823\n",
      "====> Epoch: 944 Average loss: 2.8806\n",
      "====> Epoch: 946 Average loss: 2.8852\n",
      "====> Epoch: 948 Average loss: 2.8823\n",
      "====> Epoch: 950 Average loss: 2.8764\n",
      "====> Epoch: 952 Average loss: 2.8767\n",
      "====> Epoch: 954 Average loss: 2.8792\n",
      "====> Epoch: 956 Average loss: 2.8834\n",
      "====> Epoch: 958 Average loss: 2.8804\n",
      "====> Epoch: 960 Average loss: 2.8816\n",
      "====> Epoch: 962 Average loss: 2.8803\n",
      "====> Epoch: 964 Average loss: 2.8787\n",
      "====> Epoch: 966 Average loss: 2.8823\n",
      "====> Epoch: 968 Average loss: 2.8832\n",
      "====> Epoch: 970 Average loss: 2.8778\n",
      "====> Epoch: 972 Average loss: 2.8790\n",
      "====> Epoch: 974 Average loss: 2.8877\n",
      "====> Epoch: 976 Average loss: 2.8773\n",
      "====> Epoch: 978 Average loss: 2.8817\n",
      "====> Epoch: 980 Average loss: 2.8787\n",
      "====> Epoch: 982 Average loss: 2.8813\n",
      "====> Epoch: 984 Average loss: 2.8777\n",
      "====> Epoch: 986 Average loss: 2.8811\n",
      "====> Epoch: 988 Average loss: 2.8872\n",
      "====> Epoch: 990 Average loss: 2.8838\n",
      "====> Epoch: 992 Average loss: 2.8839\n",
      "====> Epoch: 994 Average loss: 2.8824\n",
      "====> Epoch: 996 Average loss: 2.8827\n",
      "====> Epoch: 998 Average loss: 2.8825\n",
      "====> Epoch: 1000 Average loss: 2.8765\n"
     ]
    }
   ],
   "source": [
    "model_car1, standardizer_car1 = train_encoder(\"client_n_data/cardio_clients/cardio_1.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 3.6959\n",
      "====> Epoch: 4 Average loss: 3.5846\n",
      "====> Epoch: 6 Average loss: 3.5395\n",
      "====> Epoch: 8 Average loss: 3.5112\n",
      "====> Epoch: 10 Average loss: 3.5108\n",
      "====> Epoch: 12 Average loss: 3.4936\n",
      "====> Epoch: 14 Average loss: 3.4957\n",
      "====> Epoch: 16 Average loss: 3.5019\n",
      "====> Epoch: 18 Average loss: 3.4933\n",
      "====> Epoch: 20 Average loss: 3.4925\n",
      "====> Epoch: 22 Average loss: 3.4874\n",
      "====> Epoch: 24 Average loss: 3.4898\n",
      "====> Epoch: 26 Average loss: 3.4832\n",
      "====> Epoch: 28 Average loss: 3.4852\n",
      "====> Epoch: 30 Average loss: 3.4868\n",
      "====> Epoch: 32 Average loss: 3.4802\n",
      "====> Epoch: 34 Average loss: 3.4806\n",
      "====> Epoch: 36 Average loss: 3.4865\n",
      "====> Epoch: 38 Average loss: 3.4768\n",
      "====> Epoch: 40 Average loss: 3.4813\n",
      "====> Epoch: 42 Average loss: 3.4817\n",
      "====> Epoch: 44 Average loss: 3.4768\n",
      "====> Epoch: 46 Average loss: 3.4764\n",
      "====> Epoch: 48 Average loss: 3.4842\n",
      "====> Epoch: 50 Average loss: 3.4792\n",
      "====> Epoch: 52 Average loss: 3.4819\n",
      "====> Epoch: 54 Average loss: 3.4775\n",
      "====> Epoch: 56 Average loss: 3.4824\n",
      "====> Epoch: 58 Average loss: 3.4788\n",
      "====> Epoch: 60 Average loss: 3.4816\n",
      "====> Epoch: 62 Average loss: 3.4764\n",
      "====> Epoch: 64 Average loss: 3.4801\n",
      "====> Epoch: 66 Average loss: 3.4721\n",
      "====> Epoch: 68 Average loss: 3.4730\n",
      "====> Epoch: 70 Average loss: 3.4744\n",
      "====> Epoch: 72 Average loss: 3.4740\n",
      "====> Epoch: 74 Average loss: 3.4727\n",
      "====> Epoch: 76 Average loss: 3.4765\n",
      "====> Epoch: 78 Average loss: 3.4784\n",
      "====> Epoch: 80 Average loss: 3.4718\n",
      "====> Epoch: 82 Average loss: 3.4681\n",
      "====> Epoch: 84 Average loss: 3.4746\n",
      "====> Epoch: 86 Average loss: 3.4707\n",
      "====> Epoch: 88 Average loss: 3.4755\n",
      "====> Epoch: 90 Average loss: 3.4732\n",
      "====> Epoch: 92 Average loss: 3.4742\n",
      "====> Epoch: 94 Average loss: 3.4753\n",
      "====> Epoch: 96 Average loss: 3.4681\n",
      "====> Epoch: 98 Average loss: 3.4703\n",
      "====> Epoch: 100 Average loss: 3.4736\n",
      "====> Epoch: 102 Average loss: 3.4675\n",
      "====> Epoch: 104 Average loss: 3.4798\n",
      "====> Epoch: 106 Average loss: 3.4709\n",
      "====> Epoch: 108 Average loss: 3.4742\n",
      "====> Epoch: 110 Average loss: 3.4728\n",
      "====> Epoch: 112 Average loss: 3.4727\n",
      "====> Epoch: 114 Average loss: 3.4707\n",
      "====> Epoch: 116 Average loss: 3.4773\n",
      "====> Epoch: 118 Average loss: 3.4746\n",
      "====> Epoch: 120 Average loss: 3.4707\n",
      "====> Epoch: 122 Average loss: 3.4670\n",
      "====> Epoch: 124 Average loss: 3.4698\n",
      "====> Epoch: 126 Average loss: 3.4743\n",
      "====> Epoch: 128 Average loss: 3.4768\n",
      "====> Epoch: 130 Average loss: 3.4825\n",
      "====> Epoch: 132 Average loss: 3.4710\n",
      "====> Epoch: 134 Average loss: 3.4726\n",
      "====> Epoch: 136 Average loss: 3.4682\n",
      "====> Epoch: 138 Average loss: 3.4724\n",
      "====> Epoch: 140 Average loss: 3.4672\n",
      "====> Epoch: 142 Average loss: 3.4719\n",
      "====> Epoch: 144 Average loss: 3.4720\n",
      "====> Epoch: 146 Average loss: 3.4710\n",
      "====> Epoch: 148 Average loss: 3.4747\n",
      "====> Epoch: 150 Average loss: 3.4680\n",
      "====> Epoch: 152 Average loss: 3.4652\n",
      "====> Epoch: 154 Average loss: 3.4687\n",
      "====> Epoch: 156 Average loss: 3.4685\n",
      "====> Epoch: 158 Average loss: 3.4724\n",
      "====> Epoch: 160 Average loss: 3.4712\n",
      "====> Epoch: 162 Average loss: 3.4721\n",
      "====> Epoch: 164 Average loss: 3.4701\n",
      "====> Epoch: 166 Average loss: 3.4751\n",
      "====> Epoch: 168 Average loss: 3.4693\n",
      "====> Epoch: 170 Average loss: 3.4744\n",
      "====> Epoch: 172 Average loss: 3.4605\n",
      "====> Epoch: 174 Average loss: 3.4651\n",
      "====> Epoch: 176 Average loss: 3.4664\n",
      "====> Epoch: 178 Average loss: 3.4728\n",
      "====> Epoch: 180 Average loss: 3.4703\n",
      "====> Epoch: 182 Average loss: 3.4681\n",
      "====> Epoch: 184 Average loss: 3.4642\n",
      "====> Epoch: 186 Average loss: 3.4654\n",
      "====> Epoch: 188 Average loss: 3.4654\n",
      "====> Epoch: 190 Average loss: 3.4708\n",
      "====> Epoch: 192 Average loss: 3.4686\n",
      "====> Epoch: 194 Average loss: 3.4692\n",
      "====> Epoch: 196 Average loss: 3.4688\n",
      "====> Epoch: 198 Average loss: 3.4610\n",
      "====> Epoch: 200 Average loss: 3.4684\n",
      "====> Epoch: 202 Average loss: 3.4683\n",
      "====> Epoch: 204 Average loss: 3.4659\n",
      "====> Epoch: 206 Average loss: 3.4743\n",
      "====> Epoch: 208 Average loss: 3.4667\n",
      "====> Epoch: 210 Average loss: 3.4729\n",
      "====> Epoch: 212 Average loss: 3.4670\n",
      "====> Epoch: 214 Average loss: 3.4698\n",
      "====> Epoch: 216 Average loss: 3.4664\n",
      "====> Epoch: 218 Average loss: 3.4609\n",
      "====> Epoch: 220 Average loss: 3.4661\n",
      "====> Epoch: 222 Average loss: 3.4685\n",
      "====> Epoch: 224 Average loss: 3.4686\n",
      "====> Epoch: 226 Average loss: 3.4680\n",
      "====> Epoch: 228 Average loss: 3.4700\n",
      "====> Epoch: 230 Average loss: 3.4686\n",
      "====> Epoch: 232 Average loss: 3.4694\n",
      "====> Epoch: 234 Average loss: 3.4691\n",
      "====> Epoch: 236 Average loss: 3.4720\n",
      "====> Epoch: 238 Average loss: 3.4711\n",
      "====> Epoch: 240 Average loss: 3.4706\n",
      "====> Epoch: 242 Average loss: 3.4687\n",
      "====> Epoch: 244 Average loss: 3.4693\n",
      "====> Epoch: 246 Average loss: 3.4681\n",
      "====> Epoch: 248 Average loss: 3.4685\n",
      "====> Epoch: 250 Average loss: 3.4676\n",
      "====> Epoch: 252 Average loss: 3.4665\n",
      "====> Epoch: 254 Average loss: 3.4697\n",
      "====> Epoch: 256 Average loss: 3.4687\n",
      "====> Epoch: 258 Average loss: 3.4734\n",
      "====> Epoch: 260 Average loss: 3.4682\n",
      "====> Epoch: 262 Average loss: 3.4628\n",
      "====> Epoch: 264 Average loss: 3.4608\n",
      "====> Epoch: 266 Average loss: 3.4700\n",
      "====> Epoch: 268 Average loss: 3.4698\n",
      "====> Epoch: 270 Average loss: 3.4646\n",
      "====> Epoch: 272 Average loss: 3.4655\n",
      "====> Epoch: 274 Average loss: 3.4676\n",
      "====> Epoch: 276 Average loss: 3.4656\n",
      "====> Epoch: 278 Average loss: 3.4725\n",
      "====> Epoch: 280 Average loss: 3.4680\n",
      "====> Epoch: 282 Average loss: 3.4655\n",
      "====> Epoch: 284 Average loss: 3.4681\n",
      "====> Epoch: 286 Average loss: 3.4679\n",
      "====> Epoch: 288 Average loss: 3.4680\n",
      "====> Epoch: 290 Average loss: 3.4706\n",
      "====> Epoch: 292 Average loss: 3.4677\n",
      "====> Epoch: 294 Average loss: 3.4650\n",
      "====> Epoch: 296 Average loss: 3.4651\n",
      "====> Epoch: 298 Average loss: 3.4721\n",
      "====> Epoch: 300 Average loss: 3.4718\n",
      "====> Epoch: 302 Average loss: 3.4685\n",
      "====> Epoch: 304 Average loss: 3.4729\n",
      "====> Epoch: 306 Average loss: 3.4700\n",
      "====> Epoch: 308 Average loss: 3.4665\n",
      "====> Epoch: 310 Average loss: 3.4651\n",
      "====> Epoch: 312 Average loss: 3.4642\n",
      "====> Epoch: 314 Average loss: 3.4666\n",
      "====> Epoch: 316 Average loss: 3.4712\n",
      "====> Epoch: 318 Average loss: 3.4638\n",
      "====> Epoch: 320 Average loss: 3.4643\n",
      "====> Epoch: 322 Average loss: 3.4682\n",
      "====> Epoch: 324 Average loss: 3.4705\n",
      "====> Epoch: 326 Average loss: 3.4631\n",
      "====> Epoch: 328 Average loss: 3.4625\n",
      "====> Epoch: 330 Average loss: 3.4731\n",
      "====> Epoch: 332 Average loss: 3.4674\n",
      "====> Epoch: 334 Average loss: 3.4652\n",
      "====> Epoch: 336 Average loss: 3.4638\n",
      "====> Epoch: 338 Average loss: 3.4664\n",
      "====> Epoch: 340 Average loss: 3.4676\n",
      "====> Epoch: 342 Average loss: 3.4731\n",
      "====> Epoch: 344 Average loss: 3.4675\n",
      "====> Epoch: 346 Average loss: 3.4655\n",
      "====> Epoch: 348 Average loss: 3.4713\n",
      "====> Epoch: 350 Average loss: 3.4649\n",
      "====> Epoch: 352 Average loss: 3.4729\n",
      "====> Epoch: 354 Average loss: 3.4644\n",
      "====> Epoch: 356 Average loss: 3.4651\n",
      "====> Epoch: 358 Average loss: 3.4700\n",
      "====> Epoch: 360 Average loss: 3.4711\n",
      "====> Epoch: 362 Average loss: 3.4693\n",
      "====> Epoch: 364 Average loss: 3.4613\n",
      "====> Epoch: 366 Average loss: 3.4601\n",
      "====> Epoch: 368 Average loss: 3.4649\n",
      "====> Epoch: 370 Average loss: 3.4706\n",
      "====> Epoch: 372 Average loss: 3.4651\n",
      "====> Epoch: 374 Average loss: 3.4594\n",
      "====> Epoch: 376 Average loss: 3.4668\n",
      "====> Epoch: 378 Average loss: 3.4634\n",
      "====> Epoch: 380 Average loss: 3.4675\n",
      "====> Epoch: 382 Average loss: 3.4659\n",
      "====> Epoch: 384 Average loss: 3.4670\n",
      "====> Epoch: 386 Average loss: 3.4663\n",
      "====> Epoch: 388 Average loss: 3.4687\n",
      "====> Epoch: 390 Average loss: 3.4626\n",
      "====> Epoch: 392 Average loss: 3.4670\n",
      "====> Epoch: 394 Average loss: 3.4645\n",
      "====> Epoch: 396 Average loss: 3.4628\n",
      "====> Epoch: 398 Average loss: 3.4616\n",
      "====> Epoch: 400 Average loss: 3.4676\n",
      "====> Epoch: 402 Average loss: 3.4673\n",
      "====> Epoch: 404 Average loss: 3.4650\n",
      "====> Epoch: 406 Average loss: 3.4619\n",
      "====> Epoch: 408 Average loss: 3.4672\n",
      "====> Epoch: 410 Average loss: 3.4670\n",
      "====> Epoch: 412 Average loss: 3.4671\n",
      "====> Epoch: 414 Average loss: 3.4664\n",
      "====> Epoch: 416 Average loss: 3.4619\n",
      "====> Epoch: 418 Average loss: 3.4647\n",
      "====> Epoch: 420 Average loss: 3.4593\n",
      "====> Epoch: 422 Average loss: 3.4681\n",
      "====> Epoch: 424 Average loss: 3.4665\n",
      "====> Epoch: 426 Average loss: 3.4614\n",
      "====> Epoch: 428 Average loss: 3.4605\n",
      "====> Epoch: 430 Average loss: 3.4616\n",
      "====> Epoch: 432 Average loss: 3.4622\n",
      "====> Epoch: 434 Average loss: 3.4623\n",
      "====> Epoch: 436 Average loss: 3.4598\n",
      "====> Epoch: 438 Average loss: 3.4612\n",
      "====> Epoch: 440 Average loss: 3.4545\n",
      "====> Epoch: 442 Average loss: 3.4556\n",
      "====> Epoch: 444 Average loss: 3.4627\n",
      "====> Epoch: 446 Average loss: 3.4620\n",
      "====> Epoch: 448 Average loss: 3.4641\n",
      "====> Epoch: 450 Average loss: 3.4629\n",
      "====> Epoch: 452 Average loss: 3.4647\n",
      "====> Epoch: 454 Average loss: 3.4632\n",
      "====> Epoch: 456 Average loss: 3.4589\n",
      "====> Epoch: 458 Average loss: 3.4558\n",
      "====> Epoch: 460 Average loss: 3.4655\n",
      "====> Epoch: 462 Average loss: 3.4701\n",
      "====> Epoch: 464 Average loss: 3.4612\n",
      "====> Epoch: 466 Average loss: 3.4702\n",
      "====> Epoch: 468 Average loss: 3.4630\n",
      "====> Epoch: 470 Average loss: 3.4623\n",
      "====> Epoch: 472 Average loss: 3.4686\n",
      "====> Epoch: 474 Average loss: 3.4657\n",
      "====> Epoch: 476 Average loss: 3.4643\n",
      "====> Epoch: 478 Average loss: 3.4660\n",
      "====> Epoch: 480 Average loss: 3.4632\n",
      "====> Epoch: 482 Average loss: 3.4574\n",
      "====> Epoch: 484 Average loss: 3.4635\n",
      "====> Epoch: 486 Average loss: 3.4650\n",
      "====> Epoch: 488 Average loss: 3.4616\n",
      "====> Epoch: 490 Average loss: 3.4629\n",
      "====> Epoch: 492 Average loss: 3.4650\n",
      "====> Epoch: 494 Average loss: 3.4664\n",
      "====> Epoch: 496 Average loss: 3.4607\n",
      "====> Epoch: 498 Average loss: 3.4590\n",
      "====> Epoch: 500 Average loss: 3.4624\n",
      "====> Epoch: 502 Average loss: 3.4588\n",
      "====> Epoch: 504 Average loss: 3.4612\n",
      "====> Epoch: 506 Average loss: 3.4591\n",
      "====> Epoch: 508 Average loss: 3.4577\n",
      "====> Epoch: 510 Average loss: 3.4619\n",
      "====> Epoch: 512 Average loss: 3.4652\n",
      "====> Epoch: 514 Average loss: 3.4636\n",
      "====> Epoch: 516 Average loss: 3.4670\n",
      "====> Epoch: 518 Average loss: 3.4585\n",
      "====> Epoch: 520 Average loss: 3.4577\n",
      "====> Epoch: 522 Average loss: 3.4646\n",
      "====> Epoch: 524 Average loss: 3.4642\n",
      "====> Epoch: 526 Average loss: 3.4623\n",
      "====> Epoch: 528 Average loss: 3.4587\n",
      "====> Epoch: 530 Average loss: 3.4620\n",
      "====> Epoch: 532 Average loss: 3.4600\n",
      "====> Epoch: 534 Average loss: 3.4604\n",
      "====> Epoch: 536 Average loss: 3.4570\n",
      "====> Epoch: 538 Average loss: 3.4642\n",
      "====> Epoch: 540 Average loss: 3.4664\n",
      "====> Epoch: 542 Average loss: 3.4554\n",
      "====> Epoch: 544 Average loss: 3.4649\n",
      "====> Epoch: 546 Average loss: 3.4588\n",
      "====> Epoch: 548 Average loss: 3.4595\n",
      "====> Epoch: 550 Average loss: 3.4584\n",
      "====> Epoch: 552 Average loss: 3.4568\n",
      "====> Epoch: 554 Average loss: 3.4592\n",
      "====> Epoch: 556 Average loss: 3.4555\n",
      "====> Epoch: 558 Average loss: 3.4614\n",
      "====> Epoch: 560 Average loss: 3.4546\n",
      "====> Epoch: 562 Average loss: 3.4548\n",
      "====> Epoch: 564 Average loss: 3.4550\n",
      "====> Epoch: 566 Average loss: 3.4618\n",
      "====> Epoch: 568 Average loss: 3.4590\n",
      "====> Epoch: 570 Average loss: 3.4644\n",
      "====> Epoch: 572 Average loss: 3.4612\n",
      "====> Epoch: 574 Average loss: 3.4658\n",
      "====> Epoch: 576 Average loss: 3.4555\n",
      "====> Epoch: 578 Average loss: 3.4592\n",
      "====> Epoch: 580 Average loss: 3.4597\n",
      "====> Epoch: 582 Average loss: 3.4622\n",
      "====> Epoch: 584 Average loss: 3.4593\n",
      "====> Epoch: 586 Average loss: 3.4563\n",
      "====> Epoch: 588 Average loss: 3.4629\n",
      "====> Epoch: 590 Average loss: 3.4619\n",
      "====> Epoch: 592 Average loss: 3.4654\n",
      "====> Epoch: 594 Average loss: 3.4550\n",
      "====> Epoch: 596 Average loss: 3.4633\n",
      "====> Epoch: 598 Average loss: 3.4542\n",
      "====> Epoch: 600 Average loss: 3.4635\n",
      "====> Epoch: 602 Average loss: 3.4552\n",
      "====> Epoch: 604 Average loss: 3.4674\n",
      "====> Epoch: 606 Average loss: 3.4643\n",
      "====> Epoch: 608 Average loss: 3.4640\n",
      "====> Epoch: 610 Average loss: 3.4551\n",
      "====> Epoch: 612 Average loss: 3.4567\n",
      "====> Epoch: 614 Average loss: 3.4612\n",
      "====> Epoch: 616 Average loss: 3.4648\n",
      "====> Epoch: 618 Average loss: 3.4617\n",
      "====> Epoch: 620 Average loss: 3.4640\n",
      "====> Epoch: 622 Average loss: 3.4576\n",
      "====> Epoch: 624 Average loss: 3.4631\n",
      "====> Epoch: 626 Average loss: 3.4591\n",
      "====> Epoch: 628 Average loss: 3.4625\n",
      "====> Epoch: 630 Average loss: 3.4661\n",
      "====> Epoch: 632 Average loss: 3.4539\n",
      "====> Epoch: 634 Average loss: 3.4590\n",
      "====> Epoch: 636 Average loss: 3.4588\n",
      "====> Epoch: 638 Average loss: 3.4626\n",
      "====> Epoch: 640 Average loss: 3.4572\n",
      "====> Epoch: 642 Average loss: 3.4612\n",
      "====> Epoch: 644 Average loss: 3.4576\n",
      "====> Epoch: 646 Average loss: 3.4665\n",
      "====> Epoch: 648 Average loss: 3.4594\n",
      "====> Epoch: 650 Average loss: 3.4586\n",
      "====> Epoch: 652 Average loss: 3.4599\n",
      "====> Epoch: 654 Average loss: 3.4546\n",
      "====> Epoch: 656 Average loss: 3.4535\n",
      "====> Epoch: 658 Average loss: 3.4523\n",
      "====> Epoch: 660 Average loss: 3.4574\n",
      "====> Epoch: 662 Average loss: 3.4620\n",
      "====> Epoch: 664 Average loss: 3.4669\n",
      "====> Epoch: 666 Average loss: 3.4608\n",
      "====> Epoch: 668 Average loss: 3.4618\n",
      "====> Epoch: 670 Average loss: 3.4595\n",
      "====> Epoch: 672 Average loss: 3.4601\n",
      "====> Epoch: 674 Average loss: 3.4537\n",
      "====> Epoch: 676 Average loss: 3.4536\n",
      "====> Epoch: 678 Average loss: 3.4638\n",
      "====> Epoch: 680 Average loss: 3.4587\n",
      "====> Epoch: 682 Average loss: 3.4683\n",
      "====> Epoch: 684 Average loss: 3.4543\n",
      "====> Epoch: 686 Average loss: 3.4635\n",
      "====> Epoch: 688 Average loss: 3.4549\n",
      "====> Epoch: 690 Average loss: 3.4559\n",
      "====> Epoch: 692 Average loss: 3.4611\n",
      "====> Epoch: 694 Average loss: 3.4587\n",
      "====> Epoch: 696 Average loss: 3.4591\n",
      "====> Epoch: 698 Average loss: 3.4535\n",
      "====> Epoch: 700 Average loss: 3.4612\n",
      "====> Epoch: 702 Average loss: 3.4598\n",
      "====> Epoch: 704 Average loss: 3.4592\n",
      "====> Epoch: 706 Average loss: 3.4531\n",
      "====> Epoch: 708 Average loss: 3.4609\n",
      "====> Epoch: 710 Average loss: 3.4613\n",
      "====> Epoch: 712 Average loss: 3.4618\n",
      "====> Epoch: 714 Average loss: 3.4592\n",
      "====> Epoch: 716 Average loss: 3.4579\n",
      "====> Epoch: 718 Average loss: 3.4611\n",
      "====> Epoch: 720 Average loss: 3.4595\n",
      "====> Epoch: 722 Average loss: 3.4599\n",
      "====> Epoch: 724 Average loss: 3.4600\n",
      "====> Epoch: 726 Average loss: 3.4500\n",
      "====> Epoch: 728 Average loss: 3.4606\n",
      "====> Epoch: 730 Average loss: 3.4573\n",
      "====> Epoch: 732 Average loss: 3.4586\n",
      "====> Epoch: 734 Average loss: 3.4527\n",
      "====> Epoch: 736 Average loss: 3.4596\n",
      "====> Epoch: 738 Average loss: 3.4620\n",
      "====> Epoch: 740 Average loss: 3.4583\n",
      "====> Epoch: 742 Average loss: 3.4593\n",
      "====> Epoch: 744 Average loss: 3.4659\n",
      "====> Epoch: 746 Average loss: 3.4549\n",
      "====> Epoch: 748 Average loss: 3.4608\n",
      "====> Epoch: 750 Average loss: 3.4597\n",
      "====> Epoch: 752 Average loss: 3.4546\n",
      "====> Epoch: 754 Average loss: 3.4569\n",
      "====> Epoch: 756 Average loss: 3.4557\n",
      "====> Epoch: 758 Average loss: 3.4572\n",
      "====> Epoch: 760 Average loss: 3.4519\n",
      "====> Epoch: 762 Average loss: 3.4612\n",
      "====> Epoch: 764 Average loss: 3.4620\n",
      "====> Epoch: 766 Average loss: 3.4603\n",
      "====> Epoch: 768 Average loss: 3.4601\n",
      "====> Epoch: 770 Average loss: 3.4639\n",
      "====> Epoch: 772 Average loss: 3.4662\n",
      "====> Epoch: 774 Average loss: 3.4553\n",
      "====> Epoch: 776 Average loss: 3.4654\n",
      "====> Epoch: 778 Average loss: 3.4615\n",
      "====> Epoch: 780 Average loss: 3.4641\n",
      "====> Epoch: 782 Average loss: 3.4576\n",
      "====> Epoch: 784 Average loss: 3.4562\n",
      "====> Epoch: 786 Average loss: 3.4571\n",
      "====> Epoch: 788 Average loss: 3.4567\n",
      "====> Epoch: 790 Average loss: 3.4588\n",
      "====> Epoch: 792 Average loss: 3.4598\n",
      "====> Epoch: 794 Average loss: 3.4608\n",
      "====> Epoch: 796 Average loss: 3.4585\n",
      "====> Epoch: 798 Average loss: 3.4601\n",
      "====> Epoch: 800 Average loss: 3.4556\n",
      "====> Epoch: 802 Average loss: 3.4560\n",
      "====> Epoch: 804 Average loss: 3.4549\n",
      "====> Epoch: 806 Average loss: 3.4539\n",
      "====> Epoch: 808 Average loss: 3.4571\n",
      "====> Epoch: 810 Average loss: 3.4544\n",
      "====> Epoch: 812 Average loss: 3.4583\n",
      "====> Epoch: 814 Average loss: 3.4578\n",
      "====> Epoch: 816 Average loss: 3.4667\n",
      "====> Epoch: 818 Average loss: 3.4554\n",
      "====> Epoch: 820 Average loss: 3.4605\n",
      "====> Epoch: 822 Average loss: 3.4570\n",
      "====> Epoch: 824 Average loss: 3.4559\n",
      "====> Epoch: 826 Average loss: 3.4552\n",
      "====> Epoch: 828 Average loss: 3.4580\n",
      "====> Epoch: 830 Average loss: 3.4515\n",
      "====> Epoch: 832 Average loss: 3.4597\n",
      "====> Epoch: 834 Average loss: 3.4493\n",
      "====> Epoch: 836 Average loss: 3.4596\n",
      "====> Epoch: 838 Average loss: 3.4617\n",
      "====> Epoch: 840 Average loss: 3.4583\n",
      "====> Epoch: 842 Average loss: 3.4611\n",
      "====> Epoch: 844 Average loss: 3.4509\n",
      "====> Epoch: 846 Average loss: 3.4570\n",
      "====> Epoch: 848 Average loss: 3.4542\n",
      "====> Epoch: 850 Average loss: 3.4581\n",
      "====> Epoch: 852 Average loss: 3.4561\n",
      "====> Epoch: 854 Average loss: 3.4574\n",
      "====> Epoch: 856 Average loss: 3.4545\n",
      "====> Epoch: 858 Average loss: 3.4633\n",
      "====> Epoch: 860 Average loss: 3.4607\n",
      "====> Epoch: 862 Average loss: 3.4525\n",
      "====> Epoch: 864 Average loss: 3.4557\n",
      "====> Epoch: 866 Average loss: 3.4614\n",
      "====> Epoch: 868 Average loss: 3.4601\n",
      "====> Epoch: 870 Average loss: 3.4532\n",
      "====> Epoch: 872 Average loss: 3.4513\n",
      "====> Epoch: 874 Average loss: 3.4598\n",
      "====> Epoch: 876 Average loss: 3.4520\n",
      "====> Epoch: 878 Average loss: 3.4582\n",
      "====> Epoch: 880 Average loss: 3.4560\n",
      "====> Epoch: 882 Average loss: 3.4590\n",
      "====> Epoch: 884 Average loss: 3.4562\n",
      "====> Epoch: 886 Average loss: 3.4571\n",
      "====> Epoch: 888 Average loss: 3.4623\n",
      "====> Epoch: 890 Average loss: 3.4645\n",
      "====> Epoch: 892 Average loss: 3.4634\n",
      "====> Epoch: 894 Average loss: 3.4598\n",
      "====> Epoch: 896 Average loss: 3.4590\n",
      "====> Epoch: 898 Average loss: 3.4608\n",
      "====> Epoch: 900 Average loss: 3.4629\n",
      "====> Epoch: 902 Average loss: 3.4643\n",
      "====> Epoch: 904 Average loss: 3.4545\n",
      "====> Epoch: 906 Average loss: 3.4616\n",
      "====> Epoch: 908 Average loss: 3.4557\n",
      "====> Epoch: 910 Average loss: 3.4551\n",
      "====> Epoch: 912 Average loss: 3.4550\n",
      "====> Epoch: 914 Average loss: 3.4591\n",
      "====> Epoch: 916 Average loss: 3.4571\n",
      "====> Epoch: 918 Average loss: 3.4612\n",
      "====> Epoch: 920 Average loss: 3.4589\n",
      "====> Epoch: 922 Average loss: 3.4547\n",
      "====> Epoch: 924 Average loss: 3.4656\n",
      "====> Epoch: 926 Average loss: 3.4569\n",
      "====> Epoch: 928 Average loss: 3.4545\n",
      "====> Epoch: 930 Average loss: 3.4548\n",
      "====> Epoch: 932 Average loss: 3.4603\n",
      "====> Epoch: 934 Average loss: 3.4535\n",
      "====> Epoch: 936 Average loss: 3.4591\n",
      "====> Epoch: 938 Average loss: 3.4578\n",
      "====> Epoch: 940 Average loss: 3.4525\n",
      "====> Epoch: 942 Average loss: 3.4600\n",
      "====> Epoch: 944 Average loss: 3.4569\n",
      "====> Epoch: 946 Average loss: 3.4528\n",
      "====> Epoch: 948 Average loss: 3.4595\n",
      "====> Epoch: 950 Average loss: 3.4613\n",
      "====> Epoch: 952 Average loss: 3.4568\n",
      "====> Epoch: 954 Average loss: 3.4574\n",
      "====> Epoch: 956 Average loss: 3.4562\n",
      "====> Epoch: 958 Average loss: 3.4535\n",
      "====> Epoch: 960 Average loss: 3.4630\n",
      "====> Epoch: 962 Average loss: 3.4613\n",
      "====> Epoch: 964 Average loss: 3.4559\n",
      "====> Epoch: 966 Average loss: 3.4647\n",
      "====> Epoch: 968 Average loss: 3.4548\n",
      "====> Epoch: 970 Average loss: 3.4614\n",
      "====> Epoch: 972 Average loss: 3.4652\n",
      "====> Epoch: 974 Average loss: 3.4546\n",
      "====> Epoch: 976 Average loss: 3.4536\n",
      "====> Epoch: 978 Average loss: 3.4621\n",
      "====> Epoch: 980 Average loss: 3.4553\n",
      "====> Epoch: 982 Average loss: 3.4585\n",
      "====> Epoch: 984 Average loss: 3.4568\n",
      "====> Epoch: 986 Average loss: 3.4585\n",
      "====> Epoch: 988 Average loss: 3.4566\n",
      "====> Epoch: 990 Average loss: 3.4568\n",
      "====> Epoch: 992 Average loss: 3.4484\n",
      "====> Epoch: 994 Average loss: 3.4550\n",
      "====> Epoch: 996 Average loss: 3.4629\n",
      "====> Epoch: 998 Average loss: 3.4593\n",
      "====> Epoch: 1000 Average loss: 3.4600\n",
      "====> Epoch: 2 Average loss: 0.9552\n",
      "====> Epoch: 4 Average loss: 0.9458\n",
      "====> Epoch: 6 Average loss: 0.9456\n",
      "====> Epoch: 8 Average loss: 0.9463\n",
      "====> Epoch: 10 Average loss: 0.9455\n",
      "====> Epoch: 12 Average loss: 0.9454\n",
      "====> Epoch: 14 Average loss: 0.9419\n",
      "====> Epoch: 16 Average loss: 0.9467\n",
      "====> Epoch: 18 Average loss: 0.9440\n",
      "====> Epoch: 20 Average loss: 0.9448\n",
      "====> Epoch: 22 Average loss: 0.9402\n",
      "====> Epoch: 24 Average loss: 0.9417\n",
      "====> Epoch: 26 Average loss: 0.9393\n",
      "====> Epoch: 28 Average loss: 0.9402\n",
      "====> Epoch: 30 Average loss: 0.9456\n",
      "====> Epoch: 32 Average loss: 0.9423\n",
      "====> Epoch: 34 Average loss: 0.9388\n",
      "====> Epoch: 36 Average loss: 0.9385\n",
      "====> Epoch: 38 Average loss: 0.9408\n",
      "====> Epoch: 40 Average loss: 0.9421\n",
      "====> Epoch: 42 Average loss: 0.9376\n",
      "====> Epoch: 44 Average loss: 0.9411\n",
      "====> Epoch: 46 Average loss: 0.9437\n",
      "====> Epoch: 48 Average loss: 0.9417\n",
      "====> Epoch: 50 Average loss: 0.9434\n",
      "====> Epoch: 52 Average loss: 0.9374\n",
      "====> Epoch: 54 Average loss: 0.9349\n",
      "====> Epoch: 56 Average loss: 0.9403\n",
      "====> Epoch: 58 Average loss: 0.9445\n",
      "====> Epoch: 60 Average loss: 0.9395\n",
      "====> Epoch: 62 Average loss: 0.9427\n",
      "====> Epoch: 64 Average loss: 0.9392\n",
      "====> Epoch: 66 Average loss: 0.9444\n",
      "====> Epoch: 68 Average loss: 0.9373\n",
      "====> Epoch: 70 Average loss: 0.9386\n",
      "====> Epoch: 72 Average loss: 0.9375\n",
      "====> Epoch: 74 Average loss: 0.9374\n",
      "====> Epoch: 76 Average loss: 0.9396\n",
      "====> Epoch: 78 Average loss: 0.9377\n",
      "====> Epoch: 80 Average loss: 0.9385\n",
      "====> Epoch: 82 Average loss: 0.9354\n",
      "====> Epoch: 84 Average loss: 0.9382\n",
      "====> Epoch: 86 Average loss: 0.9375\n",
      "====> Epoch: 88 Average loss: 0.9421\n",
      "====> Epoch: 90 Average loss: 0.9383\n",
      "====> Epoch: 92 Average loss: 0.9409\n",
      "====> Epoch: 94 Average loss: 0.9427\n",
      "====> Epoch: 96 Average loss: 0.9381\n",
      "====> Epoch: 98 Average loss: 0.9403\n",
      "====> Epoch: 100 Average loss: 0.9411\n",
      "====> Epoch: 102 Average loss: 0.9356\n",
      "====> Epoch: 104 Average loss: 0.9439\n",
      "====> Epoch: 106 Average loss: 0.9384\n",
      "====> Epoch: 108 Average loss: 0.9322\n",
      "====> Epoch: 110 Average loss: 0.9402\n",
      "====> Epoch: 112 Average loss: 0.9349\n",
      "====> Epoch: 114 Average loss: 0.9345\n",
      "====> Epoch: 116 Average loss: 0.9375\n",
      "====> Epoch: 118 Average loss: 0.9391\n",
      "====> Epoch: 120 Average loss: 0.9374\n",
      "====> Epoch: 122 Average loss: 0.9402\n",
      "====> Epoch: 124 Average loss: 0.9349\n",
      "====> Epoch: 126 Average loss: 0.9398\n",
      "====> Epoch: 128 Average loss: 0.9391\n",
      "====> Epoch: 130 Average loss: 0.9382\n",
      "====> Epoch: 132 Average loss: 0.9403\n",
      "====> Epoch: 134 Average loss: 0.9377\n",
      "====> Epoch: 136 Average loss: 0.9427\n",
      "====> Epoch: 138 Average loss: 0.9420\n",
      "====> Epoch: 140 Average loss: 0.9386\n",
      "====> Epoch: 142 Average loss: 0.9371\n",
      "====> Epoch: 144 Average loss: 0.9416\n",
      "====> Epoch: 146 Average loss: 0.9384\n",
      "====> Epoch: 148 Average loss: 0.9354\n",
      "====> Epoch: 150 Average loss: 0.9379\n",
      "====> Epoch: 152 Average loss: 0.9381\n",
      "====> Epoch: 154 Average loss: 0.9360\n",
      "====> Epoch: 156 Average loss: 0.9406\n",
      "====> Epoch: 158 Average loss: 0.9397\n",
      "====> Epoch: 160 Average loss: 0.9385\n",
      "====> Epoch: 162 Average loss: 0.9424\n",
      "====> Epoch: 164 Average loss: 0.9375\n",
      "====> Epoch: 166 Average loss: 0.9412\n",
      "====> Epoch: 168 Average loss: 0.9375\n",
      "====> Epoch: 170 Average loss: 0.9423\n",
      "====> Epoch: 172 Average loss: 0.9367\n",
      "====> Epoch: 174 Average loss: 0.9399\n",
      "====> Epoch: 176 Average loss: 0.9423\n",
      "====> Epoch: 178 Average loss: 0.9394\n",
      "====> Epoch: 180 Average loss: 0.9380\n",
      "====> Epoch: 182 Average loss: 0.9438\n",
      "====> Epoch: 184 Average loss: 0.9388\n",
      "====> Epoch: 186 Average loss: 0.9387\n",
      "====> Epoch: 188 Average loss: 0.9350\n",
      "====> Epoch: 190 Average loss: 0.9374\n",
      "====> Epoch: 192 Average loss: 0.9379\n",
      "====> Epoch: 194 Average loss: 0.9312\n",
      "====> Epoch: 196 Average loss: 0.9379\n",
      "====> Epoch: 198 Average loss: 0.9388\n",
      "====> Epoch: 200 Average loss: 0.9384\n",
      "====> Epoch: 202 Average loss: 0.9407\n",
      "====> Epoch: 204 Average loss: 0.9394\n",
      "====> Epoch: 206 Average loss: 0.9346\n",
      "====> Epoch: 208 Average loss: 0.9381\n",
      "====> Epoch: 210 Average loss: 0.9388\n",
      "====> Epoch: 212 Average loss: 0.9363\n",
      "====> Epoch: 214 Average loss: 0.9411\n",
      "====> Epoch: 216 Average loss: 0.9389\n",
      "====> Epoch: 218 Average loss: 0.9405\n",
      "====> Epoch: 220 Average loss: 0.9358\n",
      "====> Epoch: 222 Average loss: 0.9382\n",
      "====> Epoch: 224 Average loss: 0.9342\n",
      "====> Epoch: 226 Average loss: 0.9360\n",
      "====> Epoch: 228 Average loss: 0.9353\n",
      "====> Epoch: 230 Average loss: 0.9374\n",
      "====> Epoch: 232 Average loss: 0.9373\n",
      "====> Epoch: 234 Average loss: 0.9366\n",
      "====> Epoch: 236 Average loss: 0.9331\n",
      "====> Epoch: 238 Average loss: 0.9400\n",
      "====> Epoch: 240 Average loss: 0.9347\n",
      "====> Epoch: 242 Average loss: 0.9405\n",
      "====> Epoch: 244 Average loss: 0.9368\n",
      "====> Epoch: 246 Average loss: 0.9371\n",
      "====> Epoch: 248 Average loss: 0.9402\n",
      "====> Epoch: 250 Average loss: 0.9371\n",
      "====> Epoch: 252 Average loss: 0.9403\n",
      "====> Epoch: 254 Average loss: 0.9433\n",
      "====> Epoch: 256 Average loss: 0.9364\n",
      "====> Epoch: 258 Average loss: 0.9369\n",
      "====> Epoch: 260 Average loss: 0.9375\n",
      "====> Epoch: 262 Average loss: 0.9357\n",
      "====> Epoch: 264 Average loss: 0.9435\n",
      "====> Epoch: 266 Average loss: 0.9379\n",
      "====> Epoch: 268 Average loss: 0.9363\n",
      "====> Epoch: 270 Average loss: 0.9367\n",
      "====> Epoch: 272 Average loss: 0.9393\n",
      "====> Epoch: 274 Average loss: 0.9377\n",
      "====> Epoch: 276 Average loss: 0.9383\n",
      "====> Epoch: 278 Average loss: 0.9311\n",
      "====> Epoch: 280 Average loss: 0.9400\n",
      "====> Epoch: 282 Average loss: 0.9338\n",
      "====> Epoch: 284 Average loss: 0.9384\n",
      "====> Epoch: 286 Average loss: 0.9367\n",
      "====> Epoch: 288 Average loss: 0.9396\n",
      "====> Epoch: 290 Average loss: 0.9363\n",
      "====> Epoch: 292 Average loss: 0.9395\n",
      "====> Epoch: 294 Average loss: 0.9384\n",
      "====> Epoch: 296 Average loss: 0.9385\n",
      "====> Epoch: 298 Average loss: 0.9360\n",
      "====> Epoch: 300 Average loss: 0.9327\n",
      "====> Epoch: 302 Average loss: 0.9391\n",
      "====> Epoch: 304 Average loss: 0.9366\n",
      "====> Epoch: 306 Average loss: 0.9348\n",
      "====> Epoch: 308 Average loss: 0.9394\n",
      "====> Epoch: 310 Average loss: 0.9352\n",
      "====> Epoch: 312 Average loss: 0.9381\n",
      "====> Epoch: 314 Average loss: 0.9405\n",
      "====> Epoch: 316 Average loss: 0.9361\n",
      "====> Epoch: 318 Average loss: 0.9344\n",
      "====> Epoch: 320 Average loss: 0.9364\n",
      "====> Epoch: 322 Average loss: 0.9367\n",
      "====> Epoch: 324 Average loss: 0.9357\n",
      "====> Epoch: 326 Average loss: 0.9377\n",
      "====> Epoch: 328 Average loss: 0.9338\n",
      "====> Epoch: 330 Average loss: 0.9337\n",
      "====> Epoch: 332 Average loss: 0.9381\n",
      "====> Epoch: 334 Average loss: 0.9352\n",
      "====> Epoch: 336 Average loss: 0.9367\n",
      "====> Epoch: 338 Average loss: 0.9379\n",
      "====> Epoch: 340 Average loss: 0.9360\n",
      "====> Epoch: 342 Average loss: 0.9372\n",
      "====> Epoch: 344 Average loss: 0.9363\n",
      "====> Epoch: 346 Average loss: 0.9362\n",
      "====> Epoch: 348 Average loss: 0.9380\n",
      "====> Epoch: 350 Average loss: 0.9382\n",
      "====> Epoch: 352 Average loss: 0.9401\n",
      "====> Epoch: 354 Average loss: 0.9347\n",
      "====> Epoch: 356 Average loss: 0.9384\n",
      "====> Epoch: 358 Average loss: 0.9340\n",
      "====> Epoch: 360 Average loss: 0.9385\n",
      "====> Epoch: 362 Average loss: 0.9425\n",
      "====> Epoch: 364 Average loss: 0.9361\n",
      "====> Epoch: 366 Average loss: 0.9322\n",
      "====> Epoch: 368 Average loss: 0.9348\n",
      "====> Epoch: 370 Average loss: 0.9382\n",
      "====> Epoch: 372 Average loss: 0.9360\n",
      "====> Epoch: 374 Average loss: 0.9366\n",
      "====> Epoch: 376 Average loss: 0.9335\n",
      "====> Epoch: 378 Average loss: 0.9378\n",
      "====> Epoch: 380 Average loss: 0.9394\n",
      "====> Epoch: 382 Average loss: 0.9383\n",
      "====> Epoch: 384 Average loss: 0.9352\n",
      "====> Epoch: 386 Average loss: 0.9357\n",
      "====> Epoch: 388 Average loss: 0.9395\n",
      "====> Epoch: 390 Average loss: 0.9384\n",
      "====> Epoch: 392 Average loss: 0.9353\n",
      "====> Epoch: 394 Average loss: 0.9357\n",
      "====> Epoch: 396 Average loss: 0.9380\n",
      "====> Epoch: 398 Average loss: 0.9387\n",
      "====> Epoch: 400 Average loss: 0.9340\n",
      "====> Epoch: 402 Average loss: 0.9367\n",
      "====> Epoch: 404 Average loss: 0.9349\n",
      "====> Epoch: 406 Average loss: 0.9335\n",
      "====> Epoch: 408 Average loss: 0.9352\n",
      "====> Epoch: 410 Average loss: 0.9369\n",
      "====> Epoch: 412 Average loss: 0.9360\n",
      "====> Epoch: 414 Average loss: 0.9369\n",
      "====> Epoch: 416 Average loss: 0.9355\n",
      "====> Epoch: 418 Average loss: 0.9341\n",
      "====> Epoch: 420 Average loss: 0.9371\n",
      "====> Epoch: 422 Average loss: 0.9390\n",
      "====> Epoch: 424 Average loss: 0.9325\n",
      "====> Epoch: 426 Average loss: 0.9395\n",
      "====> Epoch: 428 Average loss: 0.9377\n",
      "====> Epoch: 430 Average loss: 0.9348\n",
      "====> Epoch: 432 Average loss: 0.9384\n",
      "====> Epoch: 434 Average loss: 0.9359\n",
      "====> Epoch: 436 Average loss: 0.9380\n",
      "====> Epoch: 438 Average loss: 0.9376\n",
      "====> Epoch: 440 Average loss: 0.9355\n",
      "====> Epoch: 442 Average loss: 0.9362\n",
      "====> Epoch: 444 Average loss: 0.9413\n",
      "====> Epoch: 446 Average loss: 0.9394\n",
      "====> Epoch: 448 Average loss: 0.9398\n",
      "====> Epoch: 450 Average loss: 0.9370\n",
      "====> Epoch: 452 Average loss: 0.9370\n",
      "====> Epoch: 454 Average loss: 0.9449\n",
      "====> Epoch: 456 Average loss: 0.9361\n",
      "====> Epoch: 458 Average loss: 0.9397\n",
      "====> Epoch: 460 Average loss: 0.9373\n",
      "====> Epoch: 462 Average loss: 0.9394\n",
      "====> Epoch: 464 Average loss: 0.9391\n",
      "====> Epoch: 466 Average loss: 0.9379\n",
      "====> Epoch: 468 Average loss: 0.9372\n",
      "====> Epoch: 470 Average loss: 0.9383\n",
      "====> Epoch: 472 Average loss: 0.9368\n",
      "====> Epoch: 474 Average loss: 0.9377\n",
      "====> Epoch: 476 Average loss: 0.9401\n",
      "====> Epoch: 478 Average loss: 0.9335\n",
      "====> Epoch: 480 Average loss: 0.9382\n",
      "====> Epoch: 482 Average loss: 0.9361\n",
      "====> Epoch: 484 Average loss: 0.9378\n",
      "====> Epoch: 486 Average loss: 0.9373\n",
      "====> Epoch: 488 Average loss: 0.9396\n",
      "====> Epoch: 490 Average loss: 0.9371\n",
      "====> Epoch: 492 Average loss: 0.9377\n",
      "====> Epoch: 494 Average loss: 0.9365\n",
      "====> Epoch: 496 Average loss: 0.9335\n",
      "====> Epoch: 498 Average loss: 0.9393\n",
      "====> Epoch: 500 Average loss: 0.9341\n",
      "====> Epoch: 502 Average loss: 0.9384\n",
      "====> Epoch: 504 Average loss: 0.9364\n",
      "====> Epoch: 506 Average loss: 0.9380\n",
      "====> Epoch: 508 Average loss: 0.9439\n",
      "====> Epoch: 510 Average loss: 0.9402\n",
      "====> Epoch: 512 Average loss: 0.9360\n",
      "====> Epoch: 514 Average loss: 0.9384\n",
      "====> Epoch: 516 Average loss: 0.9403\n",
      "====> Epoch: 518 Average loss: 0.9394\n",
      "====> Epoch: 520 Average loss: 0.9351\n",
      "====> Epoch: 522 Average loss: 0.9407\n",
      "====> Epoch: 524 Average loss: 0.9382\n",
      "====> Epoch: 526 Average loss: 0.9391\n",
      "====> Epoch: 528 Average loss: 0.9361\n",
      "====> Epoch: 530 Average loss: 0.9349\n",
      "====> Epoch: 532 Average loss: 0.9407\n",
      "====> Epoch: 534 Average loss: 0.9347\n",
      "====> Epoch: 536 Average loss: 0.9352\n",
      "====> Epoch: 538 Average loss: 0.9338\n",
      "====> Epoch: 540 Average loss: 0.9369\n",
      "====> Epoch: 542 Average loss: 0.9371\n",
      "====> Epoch: 544 Average loss: 0.9362\n",
      "====> Epoch: 546 Average loss: 0.9351\n",
      "====> Epoch: 548 Average loss: 0.9370\n",
      "====> Epoch: 550 Average loss: 0.9364\n",
      "====> Epoch: 552 Average loss: 0.9327\n",
      "====> Epoch: 554 Average loss: 0.9343\n",
      "====> Epoch: 556 Average loss: 0.9327\n",
      "====> Epoch: 558 Average loss: 0.9378\n",
      "====> Epoch: 560 Average loss: 0.9385\n",
      "====> Epoch: 562 Average loss: 0.9371\n",
      "====> Epoch: 564 Average loss: 0.9373\n",
      "====> Epoch: 566 Average loss: 0.9379\n",
      "====> Epoch: 568 Average loss: 0.9319\n",
      "====> Epoch: 570 Average loss: 0.9375\n",
      "====> Epoch: 572 Average loss: 0.9381\n",
      "====> Epoch: 574 Average loss: 0.9396\n",
      "====> Epoch: 576 Average loss: 0.9357\n",
      "====> Epoch: 578 Average loss: 0.9362\n",
      "====> Epoch: 580 Average loss: 0.9386\n",
      "====> Epoch: 582 Average loss: 0.9341\n",
      "====> Epoch: 584 Average loss: 0.9399\n",
      "====> Epoch: 586 Average loss: 0.9357\n",
      "====> Epoch: 588 Average loss: 0.9352\n",
      "====> Epoch: 590 Average loss: 0.9372\n",
      "====> Epoch: 592 Average loss: 0.9401\n",
      "====> Epoch: 594 Average loss: 0.9356\n",
      "====> Epoch: 596 Average loss: 0.9361\n",
      "====> Epoch: 598 Average loss: 0.9360\n",
      "====> Epoch: 600 Average loss: 0.9367\n",
      "====> Epoch: 602 Average loss: 0.9402\n",
      "====> Epoch: 604 Average loss: 0.9365\n",
      "====> Epoch: 606 Average loss: 0.9391\n",
      "====> Epoch: 608 Average loss: 0.9368\n",
      "====> Epoch: 610 Average loss: 0.9378\n",
      "====> Epoch: 612 Average loss: 0.9355\n",
      "====> Epoch: 614 Average loss: 0.9425\n",
      "====> Epoch: 616 Average loss: 0.9417\n",
      "====> Epoch: 618 Average loss: 0.9402\n",
      "====> Epoch: 620 Average loss: 0.9395\n",
      "====> Epoch: 622 Average loss: 0.9346\n",
      "====> Epoch: 624 Average loss: 0.9342\n",
      "====> Epoch: 626 Average loss: 0.9338\n",
      "====> Epoch: 628 Average loss: 0.9339\n",
      "====> Epoch: 630 Average loss: 0.9381\n",
      "====> Epoch: 632 Average loss: 0.9344\n",
      "====> Epoch: 634 Average loss: 0.9366\n",
      "====> Epoch: 636 Average loss: 0.9382\n",
      "====> Epoch: 638 Average loss: 0.9393\n",
      "====> Epoch: 640 Average loss: 0.9391\n",
      "====> Epoch: 642 Average loss: 0.9382\n",
      "====> Epoch: 644 Average loss: 0.9371\n",
      "====> Epoch: 646 Average loss: 0.9411\n",
      "====> Epoch: 648 Average loss: 0.9382\n",
      "====> Epoch: 650 Average loss: 0.9373\n",
      "====> Epoch: 652 Average loss: 0.9368\n",
      "====> Epoch: 654 Average loss: 0.9423\n",
      "====> Epoch: 656 Average loss: 0.9372\n",
      "====> Epoch: 658 Average loss: 0.9360\n",
      "====> Epoch: 660 Average loss: 0.9408\n",
      "====> Epoch: 662 Average loss: 0.9388\n",
      "====> Epoch: 664 Average loss: 0.9352\n",
      "====> Epoch: 666 Average loss: 0.9421\n",
      "====> Epoch: 668 Average loss: 0.9344\n",
      "====> Epoch: 670 Average loss: 0.9400\n",
      "====> Epoch: 672 Average loss: 0.9321\n",
      "====> Epoch: 674 Average loss: 0.9400\n",
      "====> Epoch: 676 Average loss: 0.9355\n",
      "====> Epoch: 678 Average loss: 0.9388\n",
      "====> Epoch: 680 Average loss: 0.9364\n",
      "====> Epoch: 682 Average loss: 0.9336\n",
      "====> Epoch: 684 Average loss: 0.9382\n",
      "====> Epoch: 686 Average loss: 0.9420\n",
      "====> Epoch: 688 Average loss: 0.9407\n",
      "====> Epoch: 690 Average loss: 0.9298\n",
      "====> Epoch: 692 Average loss: 0.9333\n",
      "====> Epoch: 694 Average loss: 0.9355\n",
      "====> Epoch: 696 Average loss: 0.9387\n",
      "====> Epoch: 698 Average loss: 0.9360\n",
      "====> Epoch: 700 Average loss: 0.9379\n",
      "====> Epoch: 702 Average loss: 0.9355\n",
      "====> Epoch: 704 Average loss: 0.9401\n",
      "====> Epoch: 706 Average loss: 0.9386\n",
      "====> Epoch: 708 Average loss: 0.9350\n",
      "====> Epoch: 710 Average loss: 0.9387\n",
      "====> Epoch: 712 Average loss: 0.9359\n",
      "====> Epoch: 714 Average loss: 0.9339\n",
      "====> Epoch: 716 Average loss: 0.9386\n",
      "====> Epoch: 718 Average loss: 0.9396\n",
      "====> Epoch: 720 Average loss: 0.9359\n",
      "====> Epoch: 722 Average loss: 0.9375\n",
      "====> Epoch: 724 Average loss: 0.9352\n",
      "====> Epoch: 726 Average loss: 0.9353\n",
      "====> Epoch: 728 Average loss: 0.9398\n",
      "====> Epoch: 730 Average loss: 0.9410\n",
      "====> Epoch: 732 Average loss: 0.9362\n",
      "====> Epoch: 734 Average loss: 0.9375\n",
      "====> Epoch: 736 Average loss: 0.9365\n",
      "====> Epoch: 738 Average loss: 0.9353\n",
      "====> Epoch: 740 Average loss: 0.9325\n",
      "====> Epoch: 742 Average loss: 0.9361\n",
      "====> Epoch: 744 Average loss: 0.9367\n",
      "====> Epoch: 746 Average loss: 0.9400\n",
      "====> Epoch: 748 Average loss: 0.9335\n",
      "====> Epoch: 750 Average loss: 0.9345\n",
      "====> Epoch: 752 Average loss: 0.9350\n",
      "====> Epoch: 754 Average loss: 0.9351\n",
      "====> Epoch: 756 Average loss: 0.9375\n",
      "====> Epoch: 758 Average loss: 0.9381\n",
      "====> Epoch: 760 Average loss: 0.9417\n",
      "====> Epoch: 762 Average loss: 0.9372\n",
      "====> Epoch: 764 Average loss: 0.9420\n",
      "====> Epoch: 766 Average loss: 0.9371\n",
      "====> Epoch: 768 Average loss: 0.9377\n",
      "====> Epoch: 770 Average loss: 0.9386\n",
      "====> Epoch: 772 Average loss: 0.9348\n",
      "====> Epoch: 774 Average loss: 0.9376\n",
      "====> Epoch: 776 Average loss: 0.9357\n",
      "====> Epoch: 778 Average loss: 0.9393\n",
      "====> Epoch: 780 Average loss: 0.9358\n",
      "====> Epoch: 782 Average loss: 0.9356\n",
      "====> Epoch: 784 Average loss: 0.9304\n",
      "====> Epoch: 786 Average loss: 0.9414\n",
      "====> Epoch: 788 Average loss: 0.9372\n",
      "====> Epoch: 790 Average loss: 0.9356\n",
      "====> Epoch: 792 Average loss: 0.9449\n",
      "====> Epoch: 794 Average loss: 0.9401\n",
      "====> Epoch: 796 Average loss: 0.9398\n",
      "====> Epoch: 798 Average loss: 0.9360\n",
      "====> Epoch: 800 Average loss: 0.9407\n",
      "====> Epoch: 802 Average loss: 0.9360\n",
      "====> Epoch: 804 Average loss: 0.9385\n",
      "====> Epoch: 806 Average loss: 0.9368\n",
      "====> Epoch: 808 Average loss: 0.9351\n",
      "====> Epoch: 810 Average loss: 0.9386\n",
      "====> Epoch: 812 Average loss: 0.9350\n",
      "====> Epoch: 814 Average loss: 0.9344\n",
      "====> Epoch: 816 Average loss: 0.9353\n",
      "====> Epoch: 818 Average loss: 0.9360\n",
      "====> Epoch: 820 Average loss: 0.9383\n",
      "====> Epoch: 822 Average loss: 0.9402\n",
      "====> Epoch: 824 Average loss: 0.9369\n",
      "====> Epoch: 826 Average loss: 0.9356\n",
      "====> Epoch: 828 Average loss: 0.9350\n",
      "====> Epoch: 830 Average loss: 0.9370\n",
      "====> Epoch: 832 Average loss: 0.9338\n",
      "====> Epoch: 834 Average loss: 0.9434\n",
      "====> Epoch: 836 Average loss: 0.9358\n",
      "====> Epoch: 838 Average loss: 0.9385\n",
      "====> Epoch: 840 Average loss: 0.9376\n",
      "====> Epoch: 842 Average loss: 0.9401\n",
      "====> Epoch: 844 Average loss: 0.9363\n",
      "====> Epoch: 846 Average loss: 0.9352\n",
      "====> Epoch: 848 Average loss: 0.9383\n",
      "====> Epoch: 850 Average loss: 0.9406\n",
      "====> Epoch: 852 Average loss: 0.9397\n",
      "====> Epoch: 854 Average loss: 0.9357\n",
      "====> Epoch: 856 Average loss: 0.9384\n",
      "====> Epoch: 858 Average loss: 0.9356\n",
      "====> Epoch: 860 Average loss: 0.9373\n",
      "====> Epoch: 862 Average loss: 0.9366\n",
      "====> Epoch: 864 Average loss: 0.9379\n",
      "====> Epoch: 866 Average loss: 0.9400\n",
      "====> Epoch: 868 Average loss: 0.9389\n",
      "====> Epoch: 870 Average loss: 0.9313\n",
      "====> Epoch: 872 Average loss: 0.9364\n",
      "====> Epoch: 874 Average loss: 0.9376\n",
      "====> Epoch: 876 Average loss: 0.9431\n",
      "====> Epoch: 878 Average loss: 0.9360\n",
      "====> Epoch: 880 Average loss: 0.9368\n",
      "====> Epoch: 882 Average loss: 0.9387\n",
      "====> Epoch: 884 Average loss: 0.9395\n",
      "====> Epoch: 886 Average loss: 0.9355\n",
      "====> Epoch: 888 Average loss: 0.9379\n",
      "====> Epoch: 890 Average loss: 0.9390\n",
      "====> Epoch: 892 Average loss: 0.9387\n",
      "====> Epoch: 894 Average loss: 0.9416\n",
      "====> Epoch: 896 Average loss: 0.9359\n",
      "====> Epoch: 898 Average loss: 0.9375\n",
      "====> Epoch: 900 Average loss: 0.9360\n",
      "====> Epoch: 902 Average loss: 0.9336\n",
      "====> Epoch: 904 Average loss: 0.9387\n",
      "====> Epoch: 906 Average loss: 0.9378\n",
      "====> Epoch: 908 Average loss: 0.9370\n",
      "====> Epoch: 910 Average loss: 0.9363\n",
      "====> Epoch: 912 Average loss: 0.9337\n",
      "====> Epoch: 914 Average loss: 0.9400\n",
      "====> Epoch: 916 Average loss: 0.9365\n",
      "====> Epoch: 918 Average loss: 0.9393\n",
      "====> Epoch: 920 Average loss: 0.9347\n",
      "====> Epoch: 922 Average loss: 0.9384\n",
      "====> Epoch: 924 Average loss: 0.9350\n",
      "====> Epoch: 926 Average loss: 0.9335\n",
      "====> Epoch: 928 Average loss: 0.9375\n",
      "====> Epoch: 930 Average loss: 0.9390\n",
      "====> Epoch: 932 Average loss: 0.9395\n",
      "====> Epoch: 934 Average loss: 0.9391\n",
      "====> Epoch: 936 Average loss: 0.9346\n",
      "====> Epoch: 938 Average loss: 0.9340\n",
      "====> Epoch: 940 Average loss: 0.9378\n",
      "====> Epoch: 942 Average loss: 0.9409\n",
      "====> Epoch: 944 Average loss: 0.9385\n",
      "====> Epoch: 946 Average loss: 0.9381\n",
      "====> Epoch: 948 Average loss: 0.9363\n",
      "====> Epoch: 950 Average loss: 0.9389\n",
      "====> Epoch: 952 Average loss: 0.9406\n",
      "====> Epoch: 954 Average loss: 0.9390\n",
      "====> Epoch: 956 Average loss: 0.9415\n",
      "====> Epoch: 958 Average loss: 0.9358\n",
      "====> Epoch: 960 Average loss: 0.9356\n",
      "====> Epoch: 962 Average loss: 0.9381\n",
      "====> Epoch: 964 Average loss: 0.9387\n",
      "====> Epoch: 966 Average loss: 0.9432\n",
      "====> Epoch: 968 Average loss: 0.9340\n",
      "====> Epoch: 970 Average loss: 0.9347\n",
      "====> Epoch: 972 Average loss: 0.9360\n",
      "====> Epoch: 974 Average loss: 0.9335\n",
      "====> Epoch: 976 Average loss: 0.9365\n",
      "====> Epoch: 978 Average loss: 0.9374\n",
      "====> Epoch: 980 Average loss: 0.9386\n",
      "====> Epoch: 982 Average loss: 0.9388\n",
      "====> Epoch: 984 Average loss: 0.9369\n",
      "====> Epoch: 986 Average loss: 0.9358\n",
      "====> Epoch: 988 Average loss: 0.9359\n",
      "====> Epoch: 990 Average loss: 0.9402\n",
      "====> Epoch: 992 Average loss: 0.9404\n",
      "====> Epoch: 994 Average loss: 0.9363\n",
      "====> Epoch: 996 Average loss: 0.9360\n",
      "====> Epoch: 998 Average loss: 0.9363\n",
      "====> Epoch: 1000 Average loss: 0.9390\n"
     ]
    }
   ],
   "source": [
    "model_car2, standardizer_car2 = train_encoder(\"client_n_data/cardio_clients/cardio_2.csv\", load_data_v2, \",\")\n",
    "model_car3, standardizer_car3 = train_encoder(\"client_n_data/cardio_clients/cardio_3.csv\", load_data_v2, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_client_n_for_diabetes_ds(model_car1, 'client_n_data/cardio_clients/cardio_1.csv', 'client_n_data/latent/cardio_1_n.csv')\n",
    "generate_latent_client_n_for_diabetes_ds(model_car2, 'client_n_data/cardio_clients/cardio_2.csv', 'client_n_data/latent/cardio_2_n.csv')\n",
    "generate_latent_client_n_for_diabetes_ds(model_car3, 'client_n_data/cardio_clients/cardio_3.csv', 'client_n_data/latent/cardio_3_n.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Synthetic Data Regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82446558, -0.10738604, -0.70312679, ..., -0.14555997,\n",
       "        -0.13987976,  0.14407737],\n",
       "       [ 0.98803076,  0.26679164,  1.09171258, ...,  0.50479601,\n",
       "         0.50116088, -0.46562918],\n",
       "       [-0.60384326, -2.38580069, -1.05241235, ..., -0.16404833,\n",
       "        -0.15464898,  0.11206154],\n",
       "       ...,\n",
       "       [ 0.07394192,  0.13784105,  0.17234722, ..., -0.14442296,\n",
       "        -0.14367248,  0.15206941],\n",
       "       [ 0.03557592,  0.95764422,  0.21473787, ...,  1.15029097,\n",
       "         1.10996377, -1.15720295],\n",
       "       [ 0.46487051, -0.16059849,  0.41815324, ..., -1.10103208,\n",
       "        -1.09973648,  1.10311836]])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_generated_latent = np.load(\"syn_data/syn_latent/bank_generated/X_num_unnorm.npy\")\n",
    "bank_generated_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.19517304, -0.16316561,  0.05517056],\n",
       "       [-5.18313103, -0.1585745 ,  0.06426594],\n",
       "       [ 0.4720868 ,  0.32224879, -0.56347627],\n",
       "       ...,\n",
       "       [ 0.99620575,  5.16704597,  5.27336353],\n",
       "       [ 0.48797082, -0.12506895, -0.69481049],\n",
       "       [ 0.14273366, -1.04822629, -0.94084446]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_generated_latent[:, 6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_1_bank_generated_latent = bank_generated_latent[:, 0:3]\n",
    "np.save('syn_data/syn_latent/bank_generated/X_c1_unnorm.npy', client_1_bank_generated_latent)\n",
    "client_2_bank_generated_latent = bank_generated_latent[:, 3:6]\n",
    "np.save('syn_data/syn_latent/bank_generated/X_c2_unnorm.npy', client_2_bank_generated_latent)\n",
    "client_3_bank_generated_latent =  bank_generated_latent[:, 6:9]\n",
    "np.save('syn_data/syn_latent/bank_generated/X_c3_unnorm.npy', client_3_bank_generated_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Client 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Age', 'Experience', 'Income']\n"
     ]
    }
   ],
   "source": [
    "x_client_1_bank_syn = reconstruction_of_latent(model_b1, \"syn_data/syn_latent/bank_generated/X_c1_unnorm.npy\", \"syn_data/syn_latent/bank_generated/y_train.npy\", dataset_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_client_1_bank_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bank_c1_real = load_data_v2(\"client_n_data/bank_clients/bank_1.csv\", \",\")\n",
    "x_bank_c1_real = x_bank_c1_real[1].inverse_transform(x_bank_c1_real[0].cpu().detach().numpy())\n",
    "x_bank_c1_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.9202213471471097\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_1_bank_syn, x_bank_c1_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6599078273534568"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(x_client_1_bank_syn, x_bank_c1_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Client 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZIP Code', 'Family', 'CCAvg', 'Education'] []\n"
     ]
    }
   ],
   "source": [
    "x_client_2_bank_syn = reconstruction_of_latent(model_b2, \"syn_data/syn_latent/bank_generated/X_c2_unnorm.npy\", \"syn_data/syn_latent/bank_generated/y_train.npy\", dataset_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bank_c2_real = load_data_v2(\"client_n_data/bank_clients/bank_2.csv\", \",\")\n",
    "x_bank_c2_real = x_bank_c2_real[1].inverse_transform(x_bank_c2_real[0].cpu().detach().numpy())\n",
    "x_bank_c2_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8127648780405755\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_2_bank_syn, x_bank_c2_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5087392517175497"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(x_client_2_bank_syn, x_bank_c2_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Client 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mortgage'] ['Personal Loan', 'Securities Account', 'CD Account', 'Online']\n"
     ]
    }
   ],
   "source": [
    "x_client_3_bank_syn = reconstruction_of_latent(model_b3, \"syn_data/syn_latent/bank_generated/X_c3_unnorm.npy\", \"syn_data/syn_latent/bank_generated/y_train.npy\", dataset_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.080957  ,  0.0762483 ,  0.10269814,  0.05266344,  0.59364074],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_client_3_bank_syn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bank_c3_real = load_data_v2(\"client_n_data/bank_clients/bank_3.csv\", \",\")\n",
    "x_bank_c3_real = x_bank_c3_real[1].inverse_transform(x_bank_c3_real[0].cpu().detach().numpy())\n",
    "x_bank_c3_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.29929480879930015\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_3_bank_syn, x_bank_c3_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6017650031482227"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(x_client_3_bank_syn, x_bank_c3_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 9)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_n_generated_latent = np.load(\"syn_data/syn_latent/cardio_generated/X_num_unnorm.npy\")\n",
    "cardio_n_generated_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_1_cardio_generated_latent = bank_generated_latent[:, 0:3]\n",
    "np.save('syn_data/syn_latent/cardio_generated/X_c1_unnorm.npy', client_1_cardio_generated_latent)\n",
    "client_2_cardio_generated_latent = bank_generated_latent[:, 3:6]\n",
    "np.save('syn_data/syn_latent/cardio_generated/X_c2_unnorm.npy', client_2_cardio_generated_latent)\n",
    "client_3_cardio_generated_latent =  bank_generated_latent[:, 6:9]\n",
    "np.save('syn_data/syn_latent/cardio_generated/X_c3_unnorm.npy', client_3_cardio_generated_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio Client 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_client_1_cardio_syn = reconstruction_of_latent(model_car1, \"syn_data/syn_latent/cardio_generated/X_c1_unnorm.npy\", \"syn_data/syn_latent/cardio_generated/y_train.npy\", standardizer_car1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 4)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cardio_c1_real = load_diabetes_data(\"client_n_data/cardio_clients/cardio_1.csv\", \",\")\n",
    "x_cardio_c1_real = x_cardio_c1_real[1].inverse_transform(x_cardio_c1_real[0].cpu().detach().numpy())\n",
    "x_cardio_c1_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9312500e-03, 1.8393000e+04, 2.0000000e+00, 1.6800000e+02],\n",
       "       [1.0019312e+00, 2.0228000e+04, 1.0000000e+00, 1.5600000e+02],\n",
       "       [2.0019312e+00, 1.8857000e+04, 1.0000000e+00, 1.6500000e+02],\n",
       "       ...,\n",
       "       [9.9996000e+04, 1.9066000e+04, 2.0000000e+00, 1.8300000e+02],\n",
       "       [9.9998000e+04, 2.2431000e+04, 1.0000000e+00, 1.6300000e+02],\n",
       "       [9.9999000e+04, 2.0540000e+04, 1.0000000e+00, 1.7000000e+02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cardio_c1_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_client_1_cardio_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7337510021837952\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_1_cardio_syn, x_cardio_c1_real[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio Client 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 4)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_client_2_cardio_syn = reconstruction_of_latent(model_car2, \"syn_data/syn_latent/cardio_generated/X_c2_unnorm.npy\", \"syn_data/syn_latent/cardio_generated/y_train.npy\", standardizer_car2)\n",
    "x_cardio_c2_real = load_diabetes_data(\"client_n_data/cardio_clients/cardio_2.csv\", \",\")\n",
    "x_cardio_c2_real = x_cardio_c2_real[1].inverse_transform(x_cardio_c2_real[0].cpu().detach().numpy())\n",
    "x_cardio_c2_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 82.66248  , 133.91621  ,  91.236404 ,   1.1893916],\n",
       "       [ 68.02494  , 122.48908  ,  83.65555  ,   1.1430093],\n",
       "       [ 74.71665  , 127.00881  ,  86.32303  ,   1.1424247],\n",
       "       ...,\n",
       "       [ 69.823616 , 123.12251  ,  84.845    ,   1.1328647],\n",
       "       [ 66.235756 , 120.976974 ,  82.99318  ,   1.1562182],\n",
       "       [ 71.67981  , 123.83279  ,  85.49577  ,   1.1278017]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_client_2_cardio_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7229961610931834\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_2_cardio_syn, x_cardio_c2_real[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio Client 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10,  1.0000000e+00],\n",
       "       [ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10,  1.0000000e+00],\n",
       "       [ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10, -9.0462819e-09],\n",
       "       ...,\n",
       "       [ 1.0000000e+00, -2.7639526e-09,  1.0000000e+00, -9.0462819e-09],\n",
       "       [ 2.0000000e+00, -2.7639526e-09, -1.9243784e-10, -9.0462819e-09],\n",
       "       [ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10,  1.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_client_3_cardio_syn = reconstruction_of_latent(model_car3, \"syn_data/syn_latent/cardio_generated/X_c3_unnorm.npy\", \"syn_data/syn_latent/cardio_generated/y_train.npy\", standardizer_car3)\n",
    "x_cardio_c3_real = load_diabetes_data(\"client_n_data/cardio_clients/cardio_3.csv\", \",\")\n",
    "x_cardio_c3_real = x_cardio_c3_real[1].inverse_transform(x_cardio_c3_real[0].cpu().detach().numpy())\n",
    "x_cardio_c3_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10,  1.0000000e+00],\n",
       "       [ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10,  1.0000000e+00],\n",
       "       [ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10, -9.0462819e-09],\n",
       "       ...,\n",
       "       [ 1.0000000e+00, -2.7639526e-09,  1.0000000e+00, -9.0462819e-09],\n",
       "       [ 2.0000000e+00, -2.7639526e-09, -1.9243784e-10, -9.0462819e-09],\n",
       "       [ 1.0000000e+00, -2.7639526e-09, -1.9243784e-10,  1.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cardio_c3_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.6009089888327613\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_client_3_cardio_syn, x_cardio_c3_real[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5728722256751043"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(x_client_3_cardio_syn, x_cardio_c3_real[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Thiels U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Similarities (Theil's U): {'feature1': 0.9992314608332202, 'feature2': 0.9999459194865851, 'feature3': 0.9999711181199618, 'feature4': 0.9999128704433407}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_categorical_similarity_v2(col_real, col_synthetic):\n",
    "    # Ensure the input columns are of the same length\n",
    "    if len(col_real) != len(col_synthetic):\n",
    "        raise ValueError(\"Input columns must have the same length.\")\n",
    "    \n",
    "    # Compute value counts (probabilities) for each unique value in the columns\n",
    "    p_real = pd.Series(col_real).value_counts(normalize=True)\n",
    "    p_synthetic = pd.Series(col_synthetic).value_counts(normalize=True)\n",
    "    \n",
    "    # Align the indices of the real and synthetic distributions\n",
    "    p_real, p_synthetic = p_real.align(p_synthetic, fill_value=0)\n",
    "    \n",
    "    # Compute Theil's U\n",
    "    epsilon = 1e-10  # Small value to avoid division by zero or log of zero\n",
    "    p_real += epsilon\n",
    "    p_synthetic += epsilon\n",
    "    u = (p_real * np.log(p_real / p_synthetic)).sum()\n",
    "    \n",
    "    # Return similarity measure\n",
    "    return 1 - u\n",
    "\n",
    "# Example usage\n",
    "# Create sample dataframes with columns having 0s and 1s\n",
    "data_real = pd.DataFrame({\n",
    "    'feature1': np.random.choice([0, 1], size=5000),\n",
    "    'feature2': np.random.choice([0, 1], size=5000),\n",
    "    'feature3': np.random.choice([0, 1], size=5000),\n",
    "    'feature4': np.random.choice([0, 1], size=5000)\n",
    "})\n",
    "\n",
    "data_synthetic = pd.DataFrame({\n",
    "    'feature1': np.random.choice([0, 1], size=5000),\n",
    "    'feature2': np.random.choice([0, 1], size=5000),\n",
    "    'feature3': np.random.choice([0, 1], size=5000),\n",
    "    'feature4': np.random.choice([0, 1], size=5000)\n",
    "})\n",
    "\n",
    "# Compute similarity for each column\n",
    "similarities = {}\n",
    "for column in data_real.columns:\n",
    "    similarity = compute_categorical_similarity(data_real[column], data_synthetic[column])\n",
    "    similarities[column] = similarity\n",
    "\n",
    "print('Categorical Similarities (Theil\\'s U):', similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>-9.046282e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>-9.046282e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-9.046282e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>-9.046282e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.763953e-09</td>\n",
       "      <td>-1.924378e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       col1          col2          col3          col4\n",
       "0       1.0 -2.763953e-09 -1.924378e-10  1.000000e+00\n",
       "1       1.0 -2.763953e-09 -1.924378e-10  1.000000e+00\n",
       "2       1.0 -2.763953e-09 -1.924378e-10 -9.046282e-09\n",
       "3       1.0 -2.763953e-09 -1.924378e-10  1.000000e+00\n",
       "4       1.0 -2.763953e-09 -1.924378e-10 -9.046282e-09\n",
       "...     ...           ...           ...           ...\n",
       "69995   1.0  1.000000e+00 -1.924378e-10  1.000000e+00\n",
       "69996   2.0 -2.763953e-09 -1.924378e-10  1.000000e+00\n",
       "69997   1.0 -2.763953e-09  1.000000e+00 -9.046282e-09\n",
       "69998   2.0 -2.763953e-09 -1.924378e-10 -9.046282e-09\n",
       "69999   1.0 -2.763953e-09 -1.924378e-10  1.000000e+00\n",
       "\n",
       "[70000 rows x 4 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(x_cardio_c3_real, columns=['col1', 'col2', 'col3', 'col4'])\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Similarities (Theil's U): {'col1': -21.489038688118, 'col2': -21.726096630021438, 'col3': -21.811164839042853, 'col4': -21.51967653053544}\n"
     ]
    }
   ],
   "source": [
    "similarities = {}\n",
    "x_client_3_cardio_syn_df = pd.DataFrame(x_client_3_cardio_syn, columns=['col1', 'col2', 'col3', 'col4'])\n",
    "x_cardio_c3_real_df = pd.DataFrame( x_cardio_c3_real[:5000], columns=['col1', 'col2', 'col3', 'col4'])\n",
    "for column in x_cardio_c3_real_df.columns:\n",
    "    similarity = compute_categorical_similarity(x_cardio_c3_real_df[column], x_client_3_cardio_syn_df[column])\n",
    "    similarities[column] = similarity\n",
    "\n",
    "print('Categorical Similarities (Theil\\'s U):', similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment On Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7321, -0.4361,  0.4435,  ...,  0.0000,  0.0000,  1.0000],\n",
       "         [-1.7320,  0.3077, -1.0182,  ...,  0.0000,  0.0000,  1.0000],\n",
       "         [-1.7320, -0.2480,  0.0780,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.7339, -0.1633,  2.2705,  ...,  0.0000,  1.0000,  0.0000],\n",
       "         [ 1.7339,  1.2006, -0.1656,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.7340,  0.4341,  0.6871,  ...,  0.0000,  0.0000,  1.0000]],\n",
       "        dtype=torch.float64),\n",
       " StandardScaler(),\n",
       " array([0, 1, 1, ..., 1, 1, 0]),\n",
       " ['id', 'age', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc'],\n",
       " ['gender', 'smoke', 'alco', 'active'])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Fixed Non Scaling of Categorical Data'''\n",
    "def load_data_v2(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    binary_categorical_cols = [col for col in df_base.columns if df_base[col].nunique() == 2 and df_base[col].dtype == 'int64']\n",
    "    continuous_numerical_cols = [col for col in df_base.columns if col not in binary_categorical_cols and df_base[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "    continuous_data = df_base[continuous_numerical_cols].values.astype('float32')\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    continuous_data = standardizer.fit_transform(continuous_data)\n",
    "\n",
    "    binary_data = df_base[binary_categorical_cols].values.astype('int64')\n",
    "\n",
    "    x = np.hstack((continuous_data, binary_data))\n",
    "    # Convert to torch tensor and move to device\n",
    "    x_train = torch.from_numpy(x).to(device)\n",
    "    return x_train, standardizer, df_target, continuous_numerical_cols, binary_categorical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_custom(x_tensor, standardizer, continuous_numerical_cols, binary_categorical_cols):\n",
    "    \n",
    "    x_array = x_tensor.cpu().detach().numpy()\n",
    "    df = pd.DataFrame(x_array, columns=continuous_numerical_cols + binary_categorical_cols)\n",
    "    # Separate continuous and binary data\n",
    "    continuous_data = df[continuous_numerical_cols].values\n",
    "    binary_data = df[binary_categorical_cols].values\n",
    "    # Apply inverse transformation only to continuous data\n",
    "    continuous_data = standardizer.inverse_transform(continuous_data)\n",
    "    \n",
    "    # Reconstruct the dataframe to maintain original order\n",
    "    continuous_df = pd.DataFrame(continuous_data, columns=continuous_numerical_cols)\n",
    "    binary_df = pd.DataFrame(binary_data, columns=binary_categorical_cols)\n",
    "\n",
    "    processed_df = pd.concat([continuous_df, binary_df], axis=1)\n",
    "    processed_df = processed_df[df.columns]\n",
    "\n",
    "    return processed_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.30117282e-03, 1.83930000e+04, 1.68000000e+02, 6.19999999e+01,\n",
       "       1.10000000e+02, 7.99999997e+01, 1.00000002e+00, 9.99999998e-01,\n",
       "       2.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse_transform_custom(x[0], x[1], x[3], x[4])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVAE Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Table Metadarta API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'METADATA_SPEC_VERSION': 'SINGLE_TABLE_V1',\n",
       " 'columns': {'ID': {'sdtype': 'id'},\n",
       "  'Age': {'sdtype': 'numerical'},\n",
       "  'Experience': {'sdtype': 'numerical'},\n",
       "  'Income': {'sdtype': 'numerical'},\n",
       "  'ZIP Code': {'sdtype': 'postcode', 'pii': True},\n",
       "  'Family': {'sdtype': 'categorical'},\n",
       "  'CCAvg': {'sdtype': 'numerical'},\n",
       "  'Education': {'sdtype': 'categorical'},\n",
       "  'Mortgage': {'sdtype': 'numerical'},\n",
       "  'Personal Loan': {'sdtype': 'categorical'},\n",
       "  'Securities Account': {'sdtype': 'categorical'},\n",
       "  'CD Account': {'sdtype': 'categorical'},\n",
       "  'Online': {'sdtype': 'categorical'},\n",
       "  'CreditCard': {'sdtype': 'categorical'}},\n",
       " 'primary_key': 'ID'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "meta = metadata.detect_from_csv('Data/bank.csv')\n",
    "\n",
    "metadata.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msdv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msingle_table\u001b[39;00m \u001b[39mimport\u001b[39;00m TVAESynthesizer\n\u001b[1;32m      2\u001b[0m synthesizer \u001b[39m=\u001b[39m TVAESynthesizer(\n\u001b[0;32m----> 3\u001b[0m     metadata, \u001b[39m# required\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     enforce_min_max_values\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     enforce_rounding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata' is not defined"
     ]
    }
   ],
   "source": [
    "from sdv.single_table import TVAESynthesizer\n",
    "synthesizer = TVAESynthesizer(\n",
    "    metadata, # required\n",
    "    enforce_min_max_values=True,\n",
    "    enforce_rounding=True,\n",
    "    epochs=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tvae = pd.read_csv('Data/diabetes.csv')\n",
    "# data_tvae\n",
    "syn_data = synthesizer.fit(data_tvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data_data = synthesizer.sample(num_rows=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = synthesizer._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ctgan.synthesizers.tvae.TVAE at 0x296672e80>"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TVAE module.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Linear, Module, Parameter, ReLU, Sequential\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ctgan.data_transformer import DataTransformer\n",
    "from ctgan.synthesizers.base import BaseSynthesizer, random_state\n",
    "\n",
    "\n",
    "class Encoder(Module):\n",
    "    \"\"\"Encoder for the TVAE.\n",
    "\n",
    "    Args:\n",
    "        data_dim (int):\n",
    "            Dimensions of the data.\n",
    "        compress_dims (tuple or list of ints):\n",
    "            Size of each hidden layer.\n",
    "        embedding_dim (int):\n",
    "            Size of the output vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dim, compress_dims, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        dim = data_dim\n",
    "        seq = []\n",
    "        for item in list(compress_dims):\n",
    "            seq += [\n",
    "                Linear(dim, item),\n",
    "                ReLU()\n",
    "            ]\n",
    "            dim = item\n",
    "\n",
    "        self.seq = Sequential(*seq)\n",
    "        self.fc1 = Linear(dim, embedding_dim)\n",
    "        self.fc2 = Linear(dim, embedding_dim)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"Encode the passed `input_`.\"\"\"\n",
    "        feature = self.seq(input_)\n",
    "        mu = self.fc1(feature)\n",
    "        logvar = self.fc2(feature)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu, std, logvar\n",
    "\n",
    "\n",
    "class Decoder(Module):\n",
    "    \"\"\"Decoder for the TVAE.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int):\n",
    "            Size of the input vector.\n",
    "        decompress_dims (tuple or list of ints):\n",
    "            Size of each hidden layer.\n",
    "        data_dim (int):\n",
    "            Dimensions of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, decompress_dims, data_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        dim = embedding_dim\n",
    "        seq = []\n",
    "        for item in list(decompress_dims):\n",
    "            seq += [Linear(dim, item), ReLU()]\n",
    "            dim = item\n",
    "\n",
    "        seq.append(Linear(dim, data_dim))\n",
    "        self.seq = Sequential(*seq)\n",
    "        self.sigma = Parameter(torch.ones(data_dim) * 0.1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"Decode the passed `input_`.\"\"\"\n",
    "        return self.seq(input_), self.sigma\n",
    "\n",
    "\n",
    "def _loss_function(recon_x, x, sigmas, mu, logvar, output_info, factor):\n",
    "    st = 0\n",
    "    loss = []\n",
    "    for column_info in output_info:\n",
    "        for span_info in column_info:\n",
    "            if span_info.activation_fn != 'softmax':\n",
    "                ed = st + span_info.dim\n",
    "                std = sigmas[st]\n",
    "                eq = x[:, st] - torch.tanh(recon_x[:, st])\n",
    "                loss.append((eq ** 2 / 2 / (std ** 2)).sum())\n",
    "                loss.append(torch.log(std) * x.size()[0])\n",
    "                st = ed\n",
    "\n",
    "            else:\n",
    "                ed = st + span_info.dim\n",
    "                loss.append(cross_entropy(\n",
    "                    recon_x[:, st:ed], torch.argmax(x[:, st:ed], dim=-1), reduction='sum'))\n",
    "                st = ed\n",
    "\n",
    "    assert st == recon_x.size()[1]\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
    "    return sum(loss) * factor / x.size()[0], KLD / x.size()[0]\n",
    "\n",
    "\n",
    "class TVAE(BaseSynthesizer):\n",
    "    \"\"\"TVAE.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=3,\n",
    "        compress_dims=(128, 128),\n",
    "        decompress_dims=(128, 128),\n",
    "        l2scale=1e-5,\n",
    "        batch_size=500,\n",
    "        epochs=300,\n",
    "        loss_factor=2,\n",
    "        cuda=False,\n",
    "        verbose=False\n",
    "    ):\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.compress_dims = compress_dims\n",
    "        self.decompress_dims = decompress_dims\n",
    "\n",
    "        self.l2scale = l2scale\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_factor = loss_factor\n",
    "        self.epochs = epochs\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Batch', 'Loss'])\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if not cuda or not torch.cuda.is_available():\n",
    "            device = 'cpu'\n",
    "        elif isinstance(cuda, str):\n",
    "            device = cuda\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "\n",
    "        self._device = torch.device(device)\n",
    "\n",
    "    @random_state\n",
    "    def fit(self, train_data, discrete_columns=()):\n",
    "        \"\"\"Fit the TVAE Synthesizer models to the training data.\n",
    "\n",
    "        Args:\n",
    "            train_data (numpy.ndarray or pandas.DataFrame):\n",
    "                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n",
    "            discrete_columns (list-like):\n",
    "                List of discrete columns to be used to generate the Conditional\n",
    "                Vector. If ``train_data`` is a Numpy array, this list should\n",
    "                contain the integer indices of the columns. Otherwise, if it is\n",
    "                a ``pandas.DataFrame``, this list should contain the column names.\n",
    "        \"\"\"\n",
    "        self.transformer = DataTransformer()\n",
    "        self.transformer.fit(train_data, discrete_columns)\n",
    "        train_data = self.transformer.transform(train_data)\n",
    "        dataset = TensorDataset(torch.from_numpy(train_data.astype('float32')).to(self._device))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        data_dim = self.transformer.output_dimensions\n",
    "        encoder = Encoder(data_dim, self.compress_dims, self.embedding_dim).to(self._device)\n",
    "        self.decoder = Decoder(self.embedding_dim, self.decompress_dims, data_dim).to(self._device)\n",
    "        optimizerAE = Adam(\n",
    "            list(encoder.parameters()) + list(self.decoder.parameters()),\n",
    "            weight_decay=self.l2scale)\n",
    "\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Batch', 'Loss'])\n",
    "        iterator = tqdm(range(self.epochs), disable=(not self.verbose))\n",
    "        if self.verbose:\n",
    "            iterator_description = 'Loss: {loss:.3f}'\n",
    "            iterator.set_description(iterator_description.format(loss=0))\n",
    "\n",
    "        for i in iterator:\n",
    "            latent_embeddings = []\n",
    "            loss_values = []\n",
    "            batch = []\n",
    "            for id_, data in enumerate(loader):\n",
    "                optimizerAE.zero_grad()\n",
    "                real = data[0].to(self._device)\n",
    "                mu, std, logvar = encoder(real)\n",
    "                eps = torch.randn_like(std)\n",
    "                emb = eps * std + mu\n",
    "                latent_embeddings.append(emb.detach())\n",
    "                rec, sigmas = self.decoder(emb)\n",
    "                loss_1, loss_2 = _loss_function(\n",
    "                    rec, real, sigmas, mu, logvar,\n",
    "                    self.transformer.output_info_list, self.loss_factor\n",
    "                )\n",
    "                loss = loss_1 + loss_2\n",
    "                loss.backward()\n",
    "                optimizerAE.step()\n",
    "                self.decoder.sigma.data.clamp_(0.01, 1.0)\n",
    "\n",
    "                batch.append(id_)\n",
    "                loss_values.append(loss.detach().cpu().item())\n",
    "\n",
    "            epoch_loss_df = pd.DataFrame({\n",
    "                'Epoch': [i] * len(batch),\n",
    "                'Batch': batch,\n",
    "                'Loss': loss_values\n",
    "            })\n",
    "            if not self.loss_values.empty:\n",
    "                self.loss_values = pd.concat(\n",
    "                    [self.loss_values, epoch_loss_df]\n",
    "                ).reset_index(drop=True)\n",
    "            else:\n",
    "                self.loss_values = epoch_loss_df\n",
    "\n",
    "            if self.verbose:\n",
    "                iterator.set_description(\n",
    "                    iterator_description.format(\n",
    "                        loss=loss.detach().cpu().item()))\n",
    "        return latent_embeddings\n",
    "\n",
    "    @random_state\n",
    "    def sample(self, samples, noise):\n",
    "        \"\"\"Sample data similar to the training data.\n",
    "\n",
    "        Args:\n",
    "            samples (int):\n",
    "                Number of rows to sample.\n",
    "            noise (tensor):\n",
    "                Noise\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray or pandas.DataFrame\n",
    "        \"\"\"\n",
    "        print(\"Here\")\n",
    "        self.decoder.eval()\n",
    "        steps = samples // self.batch_size + 1\n",
    "        data = []\n",
    "        for hoola in range(steps):\n",
    "            # mean = torch.zeros(self.batch_size, self.embedding_dim)\n",
    "            # std = mean + 1\n",
    "            # noise = torch.normal(mean=mean, std=std).to(self._device)\n",
    "            fake, sigmas = self.decoder(noise)\n",
    "            fake = torch.tanh(fake)\n",
    "            data.append(fake.detach().cpu().numpy())\n",
    "\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        data = data[:samples]\n",
    "        return self.transformer.inverse_transform(data, sigmas.detach().cpu().numpy())\n",
    "\n",
    "    def set_device(self, device):\n",
    "        \"\"\"Set the `device` to be used ('GPU' or 'CPU).\"\"\"\n",
    "        self._device = device\n",
    "        self.decoder.to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DNU\"\"\"\n",
    "discrete_columns = [\n",
    "    'Family',\n",
    "    'Education',\n",
    "    'Personal Loan',\n",
    "    'Securities Account',\n",
    "    'CD Account',\n",
    "    'Online',\n",
    "    'CreditCard'\n",
    "]\n",
    "# train_data_bank = pd.read_csv(\"Data/bank.csv\")\n",
    "train_data_bank = pd.read_csv(\"client_n_data/bank_clients/bank_1.csv\")\n",
    "tave_custom = TVAE()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DNU\"\"\"\n",
    "model_tvae_custom = tave_custom.fit(train_data_bank, discrete_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3777e-01, -2.3564e+00, -5.3486e-01],\n",
       "        [-5.6708e-02, -1.3972e+00, -1.0256e+00],\n",
       "        [ 4.2632e-01, -3.2646e-01, -2.2598e-03],\n",
       "        ...,\n",
       "        [ 1.6345e+00, -1.6921e+00,  1.0038e+00],\n",
       "        [ 9.1276e-01, -9.5760e-01, -2.0631e+00],\n",
       "        [-1.4227e+00, -1.6125e+00, -1.2580e+00]])"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"DNU\"\"\"\n",
    "unbatched_data = torch.cat(model_tvae_custom, dim=0)\n",
    "\n",
    "unbatched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>403</td>\n",
       "      <td>57</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178</td>\n",
       "      <td>57</td>\n",
       "      <td>26</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3696</td>\n",
       "      <td>52</td>\n",
       "      <td>25</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>799</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4410</td>\n",
       "      <td>46</td>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>647</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-79</td>\n",
       "      <td>55</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>527</td>\n",
       "      <td>47</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>229</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Age  Experience  Income  CreditCard\n",
       "0      403   57          33      34           0\n",
       "1      178   57          26      52           0\n",
       "2     3696   52          25      58           0\n",
       "3      799   35          10      40           0\n",
       "4      540   36          12      22           0\n",
       "...    ...  ...         ...     ...         ...\n",
       "4995  4410   46          23      47           0\n",
       "4996   647   50          26      43           0\n",
       "4997   -79   55          30      50           0\n",
       "4998   527   47          22      19           0\n",
       "4999   229   62          32      57           0\n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"DNU\"\"\"\n",
    "data_gen_tvae = tave_custom.sample(5000, unbatched_data)\n",
    "data_gen_tvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/ynmqlkx14wd993676tgwybz80000gn/T/ipykernel_2863/2709757682.py:32: RuntimeWarning: ks_2samp: Exact calculation unsuccessful. Switching to method=asymp.\n",
      "  similarity, _ = ks_2samp(col_real, col_synthetic)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8168571428571428"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"DNU\"\"\"\n",
    "kolmogorov_smirnov_similarity(data_gen_tvae.to_numpy(), train_data_bank.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_column_indices(metadata_dict):\n",
    "    categorical_indices = []\n",
    "    columns = metadata_dict.get('columns', {})\n",
    "    column_names = list(columns.keys())[:-1]  # Exclude the last key\n",
    "    for index, column_name in enumerate(column_names):\n",
    "        column_data = columns[column_name]\n",
    "        if column_data.get('sdtype') == 'categorical':\n",
    "            categorical_indices.append(index)\n",
    "    return categorical_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_latent(model, source='client_n_data/diabetes_clients/client_1.csv', path=\"client_n_data/latent/client_1.csv\"):\n",
    "    DATA_PATH = source\n",
    "    df = pd.read_csv(DATA_PATH, sep=\",\")\n",
    "    actual_data = df.iloc[:, :-1]\n",
    "    outcomes = df.iloc[:, -1]\n",
    "\n",
    "    latents = []\n",
    "    metadata = SingleTableMetadata()\n",
    "    meta = metadata.detect_from_csv(source)\n",
    "\n",
    "    discrete_columns = categorical_column_indices(metadata.to_dict())\n",
    "    print(discrete_columns)\n",
    "    latents = model.fit(actual_data, discrete_columns)\n",
    "    unbatched_latent = torch.cat(latents, dim=0)\n",
    "\n",
    "    latents_df = pd.DataFrame(unbatched_latent)\n",
    "    outcomes_df = pd.DataFrame(outcomes)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meduim Dataset TVAE GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[1, 3]\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "loan_data = TVAE()\n",
    "generate_and_save_latent(loan_data, \"client_n_data/bank_clients/bank_1.csv\", path=\"client_n_data/latent/bank_tvae_1.csv\")\n",
    "loan_data_client2 = TVAE()\n",
    "generate_and_save_latent(loan_data_client2, \"client_n_data/bank_clients/bank_2.csv\", path=\"client_n_data/latent/bank_tvae_2.csv\")\n",
    "loan_data_client3 = TVAE()\n",
    "generate_and_save_latent(loan_data_client3, \"client_n_data/bank_clients/bank_3.csv\", path=\"client_n_data/latent/bank_tvae_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9857763935745911"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Pre Test\"\"\"\n",
    "bank_client_3_generated_latent = pd.read_csv(\"client_n_data/latent/bank_tvae_3.csv\")\n",
    "bank_client_3_generated_latent = bank_client_3_generated_latent.iloc[:, :-1].to_numpy()\n",
    "bank_client_3_actual_gen = loan_data_client3.sample(5000, torch.tensor(bank_client_3_generated_latent).float())\n",
    "reeeeel = pd.read_csv(\"client_n_data/bank_clients/bank_3.csv\", ).iloc[:, :-1]\n",
    "jensen_shannon_similarity(bank_client_3_actual_gen.to_numpy(), reeeeel.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_gen_ddpm = np.load(\"syn_data/syn_latent/bank_gen_tvae/X_num_unnorm.npy\")\n",
    "bank_client_1_generated_latent = bank_gen_ddpm[:, 0:3]\n",
    "bank_client_2_generated_latent = bank_gen_ddpm[:, 3:6]\n",
    "bank_client_3_generated_latent = bank_gen_ddpm[:, 6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Here\n",
      "Here\n"
     ]
    }
   ],
   "source": [
    "bank_c1_actual_gen = loan_data.sample(5000, torch.tensor(bank_client_1_generated_latent).float())\n",
    "bank_c2_actual_gen = loan_data_client2.sample(5000, torch.tensor(bank_client_2_generated_latent).float())\n",
    "bank_c3_actual_gen = loan_data_client3.sample(5000, torch.tensor(bank_client_3_generated_latent).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_bank_1_data_tvae = pd.read_csv(\"client_n_data/bank_clients/bank_1.csv\")\n",
    "actual_bank_1_data_tvae = actual_bank_1_data_tvae.iloc[: , :-1]\n",
    "\n",
    "actual_bank_2_data_tvae = pd.read_csv(\"client_n_data/bank_clients/bank_2.csv\")\n",
    "actual_bank_2_data_tvae = actual_bank_2_data_tvae.iloc[: , :-1]\n",
    "\n",
    "actual_bank_3_data_tvae = pd.read_csv(\"client_n_data/bank_clients/bank_3.csv\")\n",
    "actual_bank_3_data_tvae = actual_bank_3_data_tvae.iloc[: , :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8542637491600948\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(actual_bank_1_data_tvae.to_numpy(), bank_c1_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.9183453392812382\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(actual_bank_2_data_tvae.to_numpy(), bank_c2_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mortgage\n",
      "Personal Loan\n",
      "Securities Account\n",
      "CD Account\n",
      "Online\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1.2243042930235055,\n",
       " 0.9314255630121435,\n",
       " 0.9979722162490516,\n",
       " -0.1626946771629656,\n",
       " 0.9997764523998197]"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Thiel U\"\"\"\n",
    "similarities = []\n",
    "for col in actual_bank_3_data_tvae.columns:\n",
    "    print(col)\n",
    "    similarity = compute_categorical_similarity_v2(actual_bank_3_data_tvae[col], bank_c3_actual_gen[col])\n",
    "    similarities.append(similarity)\n",
    "\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 939,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(actual_bank_3_data_tvae, bank_c3_actual_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93956"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolmogorov_smirnov_similarity(actual_bank_3_data_tvae.to_numpy(), bank_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6143557148412802"
      ]
     },
     "execution_count": 997,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_mean_absolute_similarity(actual_bank_3_data_tvae.to_numpy(), bank_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnobchowdhury/anaconda3/envs/distributed-3-9/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 998,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy Diabetes TVAE GAN GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "diabetes_client_1 = TVAE()\n",
    "generate_and_save_latent(diabetes_client_1, \"client_n_data/diabetes_clients/client_1.csv\", path=\"client_n_data/latent/diabetes_tvae_1.csv\")\n",
    "diabetes_client_2 = TVAE()\n",
    "generate_and_save_latent(diabetes_client_2, \"client_n_data/diabetes_clients/client_2.csv\", path=\"client_n_data/latent/diabetes_tvae_2.csv\")\n",
    "diabetes_client_3 = TVAE()\n",
    "generate_and_save_latent(diabetes_client_3, \"client_n_data/diabetes_clients/client_3.csv\", path=\"client_n_data/latent/diabetes_tvae_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_gen_ddpm = np.load(\"syn_data/syn_latent/diabetes_gen_tvae/X_num_unnorm.npy\")\n",
    "diabetes_client_1_generated_latent = diabetes_gen_ddpm[:, 0:3]\n",
    "diabetes_client_2_generated_latent = diabetes_gen_ddpm[:, 3:6]\n",
    "diabetes_client_3_generated_latent = diabetes_gen_ddpm[:, 6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Here\n",
      "Here\n"
     ]
    }
   ],
   "source": [
    "diabetes_c1_actual_gen = diabetes_client_1.sample(5000, torch.tensor(diabetes_client_1_generated_latent).float())\n",
    "diabetes_c2_actual_gen = diabetes_client_2.sample(5000, torch.tensor(diabetes_client_2_generated_latent).float())\n",
    "diabetes_c3_actual_gen = diabetes_client_3.sample(5000, torch.tensor(diabetes_client_3_generated_latent).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_diabetes_1_data_tvae = pd.read_csv(\"client_n_data/diabetes_clients/client_1.csv\")\n",
    "actual_diabetes_1_data_tvae = actual_diabetes_1_data_tvae.iloc[: , :-1]\n",
    "\n",
    "actual_diabetes_2_data_tvae = pd.read_csv(\"client_n_data/diabetes_clients/client_2.csv\")\n",
    "actual_diabetes_2_data_tvae = actual_diabetes_2_data_tvae.iloc[: , :-1]\n",
    "\n",
    "actual_diabetes_3_data_tvae = pd.read_csv(\"client_n_data/diabetes_clients/client_3.csv\")\n",
    "actual_diabetes_3_data_tvae = actual_diabetes_3_data_tvae.iloc[: , :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8959707295548046\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(actual_diabetes_1_data_tvae.to_numpy(), diabetes_c1_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_similarity(actual_diabetes_2_data_tvae.to_numpy(), diabetes_c2_actual_gen.iloc[:768].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5761575459734001"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(actual_diabetes_2_data_tvae.to_numpy(), diabetes_c2_actual_gen.iloc[:768].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904296875"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolmogorov_smirnov_similarity(actual_diabetes_2_data_tvae.to_numpy(), diabetes_c2_actual_gen.iloc[:768].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6735895613231848"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_mean_absolute_similarity(actual_diabetes_2_data_tvae.to_numpy(), diabetes_c2_actual_gen.iloc[:768].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pres\n",
      "skin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.17488178100690988, 0.2500766114718487]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_diabetes_2_data_tvae_col = actual_diabetes_2_data_tvae\n",
    "similarities = []\n",
    "for col in actual_diabetes_2_data_tvae_col.columns:\n",
    "    print(col)\n",
    "    similarity = compute_categorical_similarity_v2(actual_diabetes_2_data_tvae_col[col], diabetes_c2_actual_gen.iloc[:768][col])\n",
    "    similarities.append(similarity)\n",
    "\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7325254579089375\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(actual_diabetes_3_data_tvae.to_numpy(), diabetes_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio TVAE GEN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'client_n_data/cardio_clients/client_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[999], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m generate_and_save_latent(cardio_client_1, \u001b[39m\"\u001b[39m\u001b[39mclient_n_data/cardio_clients/cardio_1.csv\u001b[39m\u001b[39m\"\u001b[39m, path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclient_n_data/latent/cardio_tvae_1.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m cardio_client_2 \u001b[39m=\u001b[39m TVAE()\n\u001b[0;32m----> 4\u001b[0m generate_and_save_latent(cardio_client_2, \u001b[39m\"\u001b[39;49m\u001b[39mclient_n_data/cardio_clients/client_2.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclient_n_data/latent/cardio_tvae_2.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m cardio_client_3 \u001b[39m=\u001b[39m TVAE()\n\u001b[1;32m      6\u001b[0m generate_and_save_latent(cardio_client_3, \u001b[39m\"\u001b[39m\u001b[39mclient_n_data/cardio_clients/client_3.csv\u001b[39m\u001b[39m\"\u001b[39m, path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclient_n_data/latent/cardio_tvae_3.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[837], line 3\u001b[0m, in \u001b[0;36mgenerate_and_save_latent\u001b[0;34m(model, source, path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_and_save_latent\u001b[39m(model, source\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclient_n_data/diabetes_clients/client_1.csv\u001b[39m\u001b[39m'\u001b[39m, path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclient_n_data/latent/client_1.csv\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     DATA_PATH \u001b[39m=\u001b[39m source\n\u001b[0;32m----> 3\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(DATA_PATH, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m     actual_data \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m     outcomes \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/distributed-3-9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/distributed-3-9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/distributed-3-9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/distributed-3-9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/distributed-3-9/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_n_data/cardio_clients/client_2.csv'"
     ]
    }
   ],
   "source": [
    "cardio_client_1 = TVAE()\n",
    "generate_and_save_latent(cardio_client_1, \"client_n_data/cardio_clients/cardio_1.csv\", path=\"client_n_data/latent/cardio_tvae_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "cardio_client_2 = TVAE()\n",
    "generate_and_save_latent(cardio_client_2, \"client_n_data/cardio_clients/cardio_2.csv\", path=\"client_n_data/latent/cardio_tvae_2.csv\")\n",
    "cardio_client_3 = TVAE()\n",
    "generate_and_save_latent(cardio_client_3, \"client_n_data/cardio_clients/cardio_3.csv\", path=\"client_n_data/latent/cardio_tvae_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_gen_ddpm = np.load(\"syn_data/syn_latent/cardio_gen_tvae/X_num_unnorm.npy\")\n",
    "cardio_client_1_generated_latent = cardio_gen_ddpm[:, 0:3]\n",
    "cardio_client_2_generated_latent = cardio_gen_ddpm[:, 3:6]\n",
    "cardio_client_3_generated_latent = cardio_gen_ddpm[:, 6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cardio_client_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cardio_c1_actual_gen \u001b[39m=\u001b[39m cardio_client_1\u001b[39m.\u001b[39msample(\u001b[39m5000\u001b[39m, torch\u001b[39m.\u001b[39mtensor(cardio_client_1_generated_latent)\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m      2\u001b[0m cardio_c2_actual_gen \u001b[39m=\u001b[39m cardio_client_2\u001b[39m.\u001b[39msample(\u001b[39m5000\u001b[39m, torch\u001b[39m.\u001b[39mtensor(cardio_client_2_generated_latent)\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m      3\u001b[0m cardio_c3_actual_gen \u001b[39m=\u001b[39m cardio_client_3\u001b[39m.\u001b[39msample(\u001b[39m5000\u001b[39m, torch\u001b[39m.\u001b[39mtensor(cardio_client_3_generated_latent)\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cardio_client_1' is not defined"
     ]
    }
   ],
   "source": [
    "cardio_c1_actual_gen = cardio_client_1.sample(5000, torch.tensor(cardio_client_1_generated_latent).float())\n",
    "cardio_c2_actual_gen = cardio_client_2.sample(5000, torch.tensor(cardio_client_2_generated_latent).float())\n",
    "cardio_c3_actual_gen = cardio_client_3.sample(5000, torch.tensor(cardio_client_3_generated_latent).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_cardio_1_data_tvae = pd.read_csv(\"client_n_data/cardio_clients/cardio_1.csv\")\n",
    "actual_cardio_1_data_tvae = actual_cardio_1_data_tvae.iloc[: , :-1]\n",
    "\n",
    "actual_cardio_2_data_tvae = pd.read_csv(\"client_n_data/cardio_clients/cardio_2.csv\")\n",
    "actual_cardio_2_data_tvae = actual_cardio_2_data_tvae.iloc[: , :-1]\n",
    "\n",
    "actual_cardio_3_data_tvae = pd.read_csv(\"client_n_data/cardio_clients/cardio_3.csv\")\n",
    "actual_cardio_3_data_tvae = actual_cardio_3_data_tvae.iloc[: , :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.6998977103783992\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(actual_cardio_1_data_tvae.to_numpy(), cardio_c1_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.9487162136387917\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(actual_cardio_2_data_tvae.to_numpy(), cardio_c2_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cardio_c3_actual_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cardio_c3_actual_gen\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cardio_c3_actual_gen' is not defined"
     ]
    }
   ],
   "source": [
    "cardio_c3_actual_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnobchowdhury/anaconda3/envs/distributed-3-9/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: nan\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(actual_cardio_3_data_tvae.to_numpy(), cardio_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962375673357506"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_similarity(actual_cardio_3_data_tvae.to_numpy(), cardio_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790119081082638"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensen_shannon_similarity(actual_cardio_3_data_tvae.to_numpy(), cardio_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9844178571428571"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolmogorov_smirnov_similarity(actual_cardio_3_data_tvae.to_numpy(), cardio_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8760279810349146"
      ]
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_mean_absolute_similarity(actual_cardio_3_data_tvae.to_numpy(), cardio_c3_actual_gen.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 1028,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_cardio_3_data_tvae.iloc[:5000, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 1027,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_c3_actual_gen.iloc[:5000, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gluc\n",
      "smoke\n",
      "alco\n",
      "active\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9984389165488295,\n",
       " 0.9965288791630152,\n",
       " 0.9961797318412299,\n",
       " 0.9986231736120271]"
      ]
     },
     "execution_count": 1029,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Thiel U\"\"\"\n",
    "\n",
    "actual_cardio_3_data_tvae_5k = actual_cardio_3_data_tvae.iloc[:5000, :]\n",
    "similarities = []\n",
    "for col in actual_cardio_3_data_tvae_5k.columns:\n",
    "    print(col)\n",
    "    similarity = compute_categorical_similarity_v2(actual_cardio_3_data_tvae_5k[col], cardio_c3_actual_gen[col])\n",
    "    similarities.append(similarity)\n",
    "\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDecoder(nn.Module):\n",
    "    def __init__(self, H2, continuous_features, categorical_features):\n",
    "        super(CustomDecoder, self).__init__()\n",
    "        self.H2 = H2\n",
    "        self.continuous_features = continuous_features\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "        # Heads for Continuous Features\n",
    "        self.continuous_heads = nn.ModuleDict({\n",
    "            feature: nn.Linear(H2, 2) for feature in continuous_features\n",
    "        })\n",
    "        \n",
    "        # Heads for Categorical Features\n",
    "        self.categorical_heads = nn.ModuleDict({\n",
    "            feature: nn.Linear(H2, n_classes) for feature, n_classes in categorical_features.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "        \n",
    "        # Process Continuous Features\n",
    "        for feature in self.continuous_features:\n",
    "            head = self.continuous_heads[feature]\n",
    "            mean_var = head(x)\n",
    "            mean, log_var = mean_var.chunk(2, dim=-1)  # Split into mean and variance\n",
    "            outputs[feature] = (mean, torch.exp(log_var))  # Return mean and variance\n",
    "        \n",
    "        # Process Categorical Features\n",
    "        for feature, n_classes in self.categorical_features.items():\n",
    "            head = self.categorical_heads[feature]\n",
    "            logits = head(x)\n",
    "            probabilities = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "            outputs[feature] = probabilities\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bank: 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_ctgan = pd.read_csv(\"centralized_data_gen/bank_CTGAN.csv\")#.iloc[:,1:]\n",
    "bank_real = pd.read_csv(\"Data/bank.csv\")\n",
    "bank_ctgan['CCAvg'] = bank_ctgan['CCAvg'].str.replace(',', '.').astype(float)\n",
    "bank_ctgan = bank_ctgan.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773153107</td>\n",
       "      <td>42</td>\n",
       "      <td>31</td>\n",
       "      <td>143</td>\n",
       "      <td>19052</td>\n",
       "      <td>2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3</td>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295010557</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>196</td>\n",
       "      <td>55911</td>\n",
       "      <td>2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>475410988</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>90925</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>421328571</td>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "      <td>73</td>\n",
       "      <td>31930</td>\n",
       "      <td>3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>646285693</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>101</td>\n",
       "      <td>57866</td>\n",
       "      <td>1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>977707134</td>\n",
       "      <td>60</td>\n",
       "      <td>26</td>\n",
       "      <td>114</td>\n",
       "      <td>51968</td>\n",
       "      <td>1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>662078397</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>97655</td>\n",
       "      <td>2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>883705473</td>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>127</td>\n",
       "      <td>43471</td>\n",
       "      <td>4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>453804152</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "      <td>58726</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>843459101</td>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "      <td>30067</td>\n",
       "      <td>4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  \\\n",
       "0     773153107   42          31     143     19052       2    3.4          3   \n",
       "1     295010557   28          30     196     55911       2    8.3          1   \n",
       "2     475410988   27          36      26     90925       4    0.5          1   \n",
       "3     421328571   26          38      73     31930       3    2.4          1   \n",
       "4     646285693   23           6     101     57866       1    3.3          1   \n",
       "...         ...  ...         ...     ...       ...     ...    ...        ...   \n",
       "4995  977707134   60          26     114     51968       1    3.9          1   \n",
       "4996  662078397   58           2     107     97655       2    3.1          1   \n",
       "4997  883705473   36          20     127     43471       4    2.2          3   \n",
       "4998  453804152   34          16      55     58726       4    0.2          3   \n",
       "4999  843459101   59          34      23     30067       4    2.1          2   \n",
       "\n",
       "      Mortgage  Personal Loan  Securities Account  CD Account  Online  \\\n",
       "0          510              1                   1           1       1   \n",
       "1            0              0                   0           0       1   \n",
       "2            0              0                   1           0       1   \n",
       "3            0              0                   0           0       0   \n",
       "4          221              1                   1           1       1   \n",
       "...        ...            ...                 ...         ...     ...   \n",
       "4995         1              0                   0           0       1   \n",
       "4996         0              0                   0           0       0   \n",
       "4997         2              0                   0           0       1   \n",
       "4998        83              0                   0           0       0   \n",
       "4999         0              0                   0           0       0   \n",
       "\n",
       "      CreditCard  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              0  \n",
       "...          ...  \n",
       "4995           0  \n",
       "4996           1  \n",
       "4997           1  \n",
       "4998           1  \n",
       "4999           0  \n",
       "\n",
       "[5000 rows x 14 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_ctgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7058689733834619\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(bank_real.to_numpy(), bank_ctgan.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diabetes 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.2</td>\n",
       "      <td>1.421</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>32.3</td>\n",
       "      <td>0.729</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>41.9</td>\n",
       "      <td>1.768</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>0.331</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>34.9</td>\n",
       "      <td>0.089</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0.418</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>1.419</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.290</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>2.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>47.6</td>\n",
       "      <td>0.943</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>50.9</td>\n",
       "      <td>0.278</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            8.0     62.0           57.0            6.0      1.0  46.2   \n",
       "1            0.0    157.0           46.0           23.0     14.0  32.3   \n",
       "2            2.0     56.0           65.0           40.0      9.0  41.9   \n",
       "3            6.0    143.0           55.0           30.0      0.0  30.6   \n",
       "4            5.0     67.0           31.0           57.0     25.0  34.9   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763          2.0     50.0           46.0            9.0      5.0  41.4   \n",
       "764          2.0    159.0           61.0            0.0    175.0  30.4   \n",
       "765          0.0    180.0           36.0           37.0     19.0  20.7   \n",
       "766          2.0    139.0           44.0            1.0    134.0  47.6   \n",
       "767          2.0     91.0           47.0            0.0    362.0  50.9   \n",
       "\n",
       "     DiabetesPedigreeFunction   Age  \n",
       "0                       1.421  31.0  \n",
       "1                       0.729  69.0  \n",
       "2                       1.768  37.0  \n",
       "3                       0.331  36.0  \n",
       "4                       0.089  31.0  \n",
       "..                        ...   ...  \n",
       "763                     0.418  35.0  \n",
       "764                     1.419  34.0  \n",
       "765                     0.290  27.0  \n",
       "766                     0.943  55.0  \n",
       "767                     0.278  52.0  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_ctgan = pd.read_csv(\"centralized_data_gen/diabetes_CTGAN.csv\")\n",
    "diabetes_ctgan = diabetes_ctgan.iloc[:, 1:-1]\n",
    "diabetes_ctgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  insu  mass   pedi  age\n",
       "0       6   148    72    35     0  33.6  0.627   50\n",
       "1       1    85    66    29     0  26.6  0.351   31\n",
       "2       8   183    64     0     0  23.3  0.672   32\n",
       "3       1    89    66    23    94  28.1  0.167   21\n",
       "4       0   137    40    35   168  43.1  2.288   33\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...\n",
       "763    10   101    76    48   180  32.9  0.171   63\n",
       "764     2   122    70    27     0  36.8  0.340   27\n",
       "765     5   121    72    23   112  26.2  0.245   30\n",
       "766     1   126    60     0     0  30.1  0.349   47\n",
       "767     1    93    70    31     0  30.4  0.315   23\n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_real = pd.read_csv(\"Data/diabetes.csv\")\n",
    "diabetes_real = diabetes_real.iloc[:, :-1]\n",
    "diabetes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7662461372023996\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(diabetes_ctgan.to_numpy(), diabetes_real.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cardio 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>817023912</td>\n",
       "      <td>21514</td>\n",
       "      <td>2</td>\n",
       "      <td>162</td>\n",
       "      <td>75.66</td>\n",
       "      <td>120</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570334775</td>\n",
       "      <td>22218</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>79.82</td>\n",
       "      <td>102</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>382444845</td>\n",
       "      <td>19036</td>\n",
       "      <td>2</td>\n",
       "      <td>181</td>\n",
       "      <td>66.99</td>\n",
       "      <td>119</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153988967</td>\n",
       "      <td>17731</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>80.62</td>\n",
       "      <td>119</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>842781231</td>\n",
       "      <td>19659</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>90.92</td>\n",
       "      <td>161</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>444794722</td>\n",
       "      <td>18378</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>102.39</td>\n",
       "      <td>130</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>782843220</td>\n",
       "      <td>17490</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>87.00</td>\n",
       "      <td>105</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>794133946</td>\n",
       "      <td>17616</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>69.51</td>\n",
       "      <td>152</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>68294833</td>\n",
       "      <td>19647</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>76.67</td>\n",
       "      <td>138</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>64114036</td>\n",
       "      <td>20620</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>56.84</td>\n",
       "      <td>132</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  \\\n",
       "0      817023912  21514       2     162   75.66    120     77            1   \n",
       "1      570334775  22218       1     165   79.82    102     66            1   \n",
       "2      382444845  19036       2     181   66.99    119     72            1   \n",
       "3      153988967  17731       2     169   80.62    119     55            1   \n",
       "4      842781231  19659       2     182   90.92    161     81            3   \n",
       "...          ...    ...     ...     ...     ...    ...    ...          ...   \n",
       "69995  444794722  18378       1     162  102.39    130     69            1   \n",
       "69996  782843220  17490       2     164   87.00    105     70            1   \n",
       "69997  794133946  17616       1     156   69.51    152     75            1   \n",
       "69998   68294833  19647       1     149   76.67    138     92            1   \n",
       "69999   64114036  20620       1     155   56.84    132     79            2   \n",
       "\n",
       "       gluc  smoke  alco  active  cardio  \n",
       "0         1      0     0       1       0  \n",
       "1         1      0     0       0       0  \n",
       "2         1      1     0       1       1  \n",
       "3         1      0     0       1       0  \n",
       "4         1      1     1       1       1  \n",
       "...     ...    ...   ...     ...     ...  \n",
       "69995     1      0     0       1       0  \n",
       "69996     1      0     0       1       0  \n",
       "69997     1      0     0       1       1  \n",
       "69998     1      0     0       1       0  \n",
       "69999     1      0     0       1       0  \n",
       "\n",
       "[70000 rows x 13 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_tabddpm = pd.read_csv(\"centralized_data_gen/cardio_CTGAN.csv\")\n",
    "cardio_tabddpm = cardio_tabddpm.iloc[:, 1:]\n",
    "cardio_tabddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>99993</td>\n",
       "      <td>19240</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>99995</td>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>99996</td>\n",
       "      <td>19066</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>99998</td>\n",
       "      <td>22431</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>99999</td>\n",
       "      <td>20540</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  \\\n",
       "0          0  18393       2     168    62.0    110     80            1     1   \n",
       "1          1  20228       1     156    85.0    140     90            3     1   \n",
       "2          2  18857       1     165    64.0    130     70            3     1   \n",
       "3          3  17623       2     169    82.0    150    100            1     1   \n",
       "4          4  17474       1     156    56.0    100     60            1     1   \n",
       "...      ...    ...     ...     ...     ...    ...    ...          ...   ...   \n",
       "69995  99993  19240       2     168    76.0    120     80            1     1   \n",
       "69996  99995  22601       1     158   126.0    140     90            2     2   \n",
       "69997  99996  19066       2     183   105.0    180     90            3     1   \n",
       "69998  99998  22431       1     163    72.0    135     80            1     2   \n",
       "69999  99999  20540       1     170    72.0    120     80            2     1   \n",
       "\n",
       "       smoke  alco  active  cardio  \n",
       "0          0     0       1       0  \n",
       "1          0     0       1       1  \n",
       "2          0     0       0       1  \n",
       "3          0     0       1       1  \n",
       "4          0     0       0       0  \n",
       "...      ...   ...     ...     ...  \n",
       "69995      1     0       1       0  \n",
       "69996      0     0       1       1  \n",
       "69997      0     1       0       1  \n",
       "69998      0     0       0       1  \n",
       "69999      0     0       1       0  \n",
       "\n",
       "[70000 rows x 13 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_real = pd.read_csv(\"Data/cardio_train.csv\", sep=\";\")\n",
    "cardio_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.88658632849786\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(cardio_tabddpm.to_numpy(), cardio_real.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabddpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diabetes: 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>140.069267</td>\n",
       "      <td>73.183003</td>\n",
       "      <td>30.171821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.390122</td>\n",
       "      <td>0.250685</td>\n",
       "      <td>21.777885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>144.043381</td>\n",
       "      <td>81.845612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.743503</td>\n",
       "      <td>0.112416</td>\n",
       "      <td>48.877731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>133.511609</td>\n",
       "      <td>73.295214</td>\n",
       "      <td>31.046656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.689396</td>\n",
       "      <td>0.233206</td>\n",
       "      <td>41.942528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.758328</td>\n",
       "      <td>59.453588</td>\n",
       "      <td>34.690728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.313570</td>\n",
       "      <td>0.225102</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.399746</td>\n",
       "      <td>55.132142</td>\n",
       "      <td>23.527988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.803863</td>\n",
       "      <td>0.133688</td>\n",
       "      <td>22.740768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>5.0</td>\n",
       "      <td>96.574792</td>\n",
       "      <td>87.161192</td>\n",
       "      <td>15.851913</td>\n",
       "      <td>181.723822</td>\n",
       "      <td>34.685963</td>\n",
       "      <td>0.407741</td>\n",
       "      <td>30.221269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.0</td>\n",
       "      <td>165.786133</td>\n",
       "      <td>80.099040</td>\n",
       "      <td>50.406801</td>\n",
       "      <td>213.809464</td>\n",
       "      <td>39.514917</td>\n",
       "      <td>1.470972</td>\n",
       "      <td>24.651495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.329000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>3.0</td>\n",
       "      <td>134.922533</td>\n",
       "      <td>66.897980</td>\n",
       "      <td>23.382913</td>\n",
       "      <td>252.922705</td>\n",
       "      <td>29.661195</td>\n",
       "      <td>0.285518</td>\n",
       "      <td>25.997983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.0</td>\n",
       "      <td>125.022234</td>\n",
       "      <td>59.201073</td>\n",
       "      <td>23.719222</td>\n",
       "      <td>323.735879</td>\n",
       "      <td>36.158683</td>\n",
       "      <td>0.185580</td>\n",
       "      <td>22.941836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "0            0.0  140.069267      73.183003      30.171821    0.000000   \n",
       "1            7.0  144.043381      81.845612       0.000000    0.000000   \n",
       "2            7.0  133.511609      73.295214      31.046656    0.000000   \n",
       "3            1.0   79.758328      59.453588      34.690728    0.000000   \n",
       "4            0.0   36.399746      55.132142      23.527988    0.000000   \n",
       "..           ...         ...            ...            ...         ...   \n",
       "763          5.0   96.574792      87.161192      15.851913  181.723822   \n",
       "764          0.0  165.786133      80.099040      50.406801  213.809464   \n",
       "765          0.0    0.000000     110.000000      60.000000  846.000000   \n",
       "766          3.0  134.922533      66.897980      23.382913  252.922705   \n",
       "767          0.0  125.022234      59.201073      23.719222  323.735879   \n",
       "\n",
       "           BMI  DiabetesPedigreeFunction        Age  \n",
       "0    31.390122                  0.250685  21.777885  \n",
       "1    32.743503                  0.112416  48.877731  \n",
       "2    32.689396                  0.233206  41.942528  \n",
       "3    32.313570                  0.225102  21.000000  \n",
       "4    32.803863                  0.133688  22.740768  \n",
       "..         ...                       ...        ...  \n",
       "763  34.685963                  0.407741  30.221269  \n",
       "764  39.514917                  1.470972  24.651495  \n",
       "765  67.100000                  2.329000  81.000000  \n",
       "766  29.661195                  0.285518  25.997983  \n",
       "767  36.158683                  0.185580  22.941836  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_tabddpm = pd.read_csv(\"centralized_data_gen/dabetes_tabddpm_synth.csv\", sep=\",\")\n",
    "diabetes_tabddpm = diabetes_tabddpm.iloc[:, 1:]\n",
    "diabetes_tabddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  insu  mass   pedi  age\n",
       "0       6   148    72    35     0  33.6  0.627   50\n",
       "1       1    85    66    29     0  26.6  0.351   31\n",
       "2       8   183    64     0     0  23.3  0.672   32\n",
       "3       1    89    66    23    94  28.1  0.167   21\n",
       "4       0   137    40    35   168  43.1  2.288   33\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...\n",
       "763    10   101    76    48   180  32.9  0.171   63\n",
       "764     2   122    70    27     0  36.8  0.340   27\n",
       "765     5   121    72    23   112  26.2  0.245   30\n",
       "766     1   126    60     0     0  30.1  0.349   47\n",
       "767     1    93    70    31     0  30.4  0.315   23\n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_real = pd.read_csv(\"Data/diabetes.csv\")\n",
    "diabetes_real = diabetes_real.iloc[:, :-1]\n",
    "diabetes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7886061144859609\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(diabetes_tabddpm.to_numpy(), diabetes_real.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bank 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_tabddpm = pd.read_csv(\"centralized_data_gen/bank_tabddpm_synth.csv\", sep=\";\")\n",
    "bank_tabddpm[\"Age\"] = bank_tabddpm[\"Age\"].str.replace(\",\", \".\").astype(float).round().astype(int)\n",
    "bank_tabddpm[\"Experience\"] = bank_tabddpm[\"Experience\"].str.replace(\",\", \".\").astype(float).round().astype(int)\n",
    "bank_tabddpm[\"Income\"] = bank_tabddpm[\"Income\"].str.replace(\",\", \".\").astype(float).round().astype(int)\n",
    "bank_tabddpm[\"ZIP Code\"] = bank_tabddpm[\"ZIP Code\"].str.replace(\",\", \".\").astype(float).round().astype(int)\n",
    "bank_tabddpm[\"CCAvg\"] = bank_tabddpm[\"CCAvg\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>92075</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>-2</td>\n",
       "      <td>10</td>\n",
       "      <td>90639</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>94553</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>95918</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>94007</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>67</td>\n",
       "      <td>41</td>\n",
       "      <td>83</td>\n",
       "      <td>92102</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>23</td>\n",
       "      <td>-3</td>\n",
       "      <td>8</td>\n",
       "      <td>96651</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>92</td>\n",
       "      <td>94096</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>80</td>\n",
       "      <td>91747</td>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>95135</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "0      30           6      43     92075       1    2.0          2         0   \n",
       "1      23          -2      10     90639       3    1.0          2         0   \n",
       "2      45          18      28     94553       1   15.0          3       101   \n",
       "3      27           7       8     95918       4    1.0          1         0   \n",
       "4      48          21      60     94007       1   49.0          2         0   \n",
       "...   ...         ...     ...       ...     ...    ...        ...       ...   \n",
       "4995   67          41      83     92102       3   24.0          1         0   \n",
       "4996   23          -3       8     96651       2    7.0          1         0   \n",
       "4997   40          15      92     94096       2   19.0          1         0   \n",
       "4998   58          31      80     91747       1   57.0          1         0   \n",
       "4999   27           2      69     95135       1    1.0          3         0   \n",
       "\n",
       "      Personal Loan  Securities Account  CD Account  Online  \n",
       "0                 0                   1           1       1  \n",
       "1                 0                   0           0       0  \n",
       "2                 0                   0           0       1  \n",
       "3                 0                   0           0       0  \n",
       "4                 0                   0           0       1  \n",
       "...             ...                 ...         ...     ...  \n",
       "4995              0                   0           0       0  \n",
       "4996              0                   0           0       0  \n",
       "4997              0                   0           0       0  \n",
       "4998              0                   0           0       1  \n",
       "4999              0                   0           0       0  \n",
       "\n",
       "[5000 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_tabddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>92697</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>92037</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>93023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>90034</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>92612</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "0      25           1      49     91107       4    1.6          1         0   \n",
       "1      45          19      34     90089       3    1.5          1         0   \n",
       "2      39          15      11     94720       1    1.0          1         0   \n",
       "3      35           9     100     94112       1    2.7          2         0   \n",
       "4      35           8      45     91330       4    1.0          2         0   \n",
       "...   ...         ...     ...       ...     ...    ...        ...       ...   \n",
       "4995   29           3      40     92697       1    1.9          3         0   \n",
       "4996   30           4      15     92037       4    0.4          1        85   \n",
       "4997   63          39      24     93023       2    0.3          3         0   \n",
       "4998   65          40      49     90034       3    0.5          2         0   \n",
       "4999   28           4      83     92612       3    0.8          1         0   \n",
       "\n",
       "      Personal Loan  Securities Account  CD Account  Online  \n",
       "0                 0                   1           0       0  \n",
       "1                 0                   1           0       0  \n",
       "2                 0                   0           0       0  \n",
       "3                 0                   0           0       0  \n",
       "4                 0                   0           0       0  \n",
       "...             ...                 ...         ...     ...  \n",
       "4995              0                   0           0       1  \n",
       "4996              0                   0           0       1  \n",
       "4997              0                   0           0       0  \n",
       "4998              0                   0           0       1  \n",
       "4999              0                   0           0       1  \n",
       "\n",
       "[5000 rows x 12 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_real = pd.read_csv(\"Data/bank.csv\", sep=\",\")\n",
    "bank_real = bank_real.iloc[:, 1:-1]\n",
    "bank_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.9269400954686651\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(bank_real.to_numpy(), bank_tabddpm.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cardio 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96581</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77306</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3965</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18320</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>75354</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>61.198263</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>61921</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>90037</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>11232</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>39639</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  gender  height     weight  ap_hi  ap_lo  cholesterol  gluc  \\\n",
       "0      96581       2     174  81.000000    130     80            1     1   \n",
       "1      77306       2     176  72.000000    120     70            1     1   \n",
       "2       3965       1     158  72.000000    120     80            1     1   \n",
       "3      18320       2     160  60.000000    120     80            1     1   \n",
       "4       1987       1     169  88.000000    110     70            1     1   \n",
       "...      ...     ...     ...        ...    ...    ...          ...   ...   \n",
       "69995  75354       2     165  61.198263    120     80            3     3   \n",
       "69996  61921       2     148  48.000000    130     90            1     1   \n",
       "69997  90037       2     165  70.000000    120     80            1     1   \n",
       "69998  11232       2     172  78.000000    120     80            1     1   \n",
       "69999  39639       1     157  80.000000    110     60            1     1   \n",
       "\n",
       "       smoke  alco  active  \n",
       "0          0     0       0  \n",
       "1          0     0       1  \n",
       "2          0     0       1  \n",
       "3          0     0       0  \n",
       "4          0     0       1  \n",
       "...      ...   ...     ...  \n",
       "69995      0     0       1  \n",
       "69996      1     0       1  \n",
       "69997      0     0       1  \n",
       "69998      1     1       0  \n",
       "69999      0     0       1  \n",
       "\n",
       "[70000 rows x 11 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_tabddpm = pd.read_csv(\"centralized_data_gen/cardio_tabddpm_synth.csv\", sep=\";\")\n",
    "cardio_tabddpm[\"id\"] = cardio_tabddpm[\"id\"].str.replace(\",\", \".\").astype(float).round().astype(int)\n",
    "cardio_tabddpm[\"age\"] = cardio_tabddpm[\"id\"].astype(int)\n",
    "cardio_tabddpm[\"height\"] = cardio_tabddpm[\"height\"].str.replace(\",\", \".\").astype(float).round().astype(int)\n",
    "cardio_tabddpm[\"weight\"] = cardio_tabddpm[\"weight\"].str.replace(\",\", \".\").astype(float)\n",
    "cardio_tabddpm[\"ap_hi\"] = cardio_tabddpm[\"ap_hi\"].str.replace(\",\", \".\").astype(float).round().astype(int)\n",
    "\n",
    "\n",
    "\n",
    "cardio_tabddpm = cardio_tabddpm.iloc[:, 1:]\n",
    "cardio_tabddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>19240</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>19066</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>22431</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>20540</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0      18393       2     168    62.0    110     80            1     1      0   \n",
       "1      20228       1     156    85.0    140     90            3     1      0   \n",
       "2      18857       1     165    64.0    130     70            3     1      0   \n",
       "3      17623       2     169    82.0    150    100            1     1      0   \n",
       "4      17474       1     156    56.0    100     60            1     1      0   \n",
       "...      ...     ...     ...     ...    ...    ...          ...   ...    ...   \n",
       "69995  19240       2     168    76.0    120     80            1     1      1   \n",
       "69996  22601       1     158   126.0    140     90            2     2      0   \n",
       "69997  19066       2     183   105.0    180     90            3     1      0   \n",
       "69998  22431       1     163    72.0    135     80            1     2      0   \n",
       "69999  20540       1     170    72.0    120     80            2     1      0   \n",
       "\n",
       "       alco  active  \n",
       "0         0       1  \n",
       "1         0       1  \n",
       "2         0       0  \n",
       "3         0       1  \n",
       "4         0       0  \n",
       "...     ...     ...  \n",
       "69995     0       1  \n",
       "69996     0       1  \n",
       "69997     1       0  \n",
       "69998     0       0  \n",
       "69999     0       1  \n",
       "\n",
       "[70000 rows x 11 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_real = pd.read_csv(\"Data/cardio_train.csv\", sep=\";\")\n",
    "cardio_real = cardio_real.iloc[:, 1:-1]\n",
    "cardio_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.9543470153748727\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(cardio_tabddpm.to_numpy(), cardio_real.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABSyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardio - 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20372.195</td>\n",
       "      <td>2</td>\n",
       "      <td>172.00000</td>\n",
       "      <td>97.00000</td>\n",
       "      <td>120.00000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21135.533</td>\n",
       "      <td>2</td>\n",
       "      <td>177.00000</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>120.00000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21979.768</td>\n",
       "      <td>2</td>\n",
       "      <td>174.00000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>134.89389</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21754.598</td>\n",
       "      <td>2</td>\n",
       "      <td>165.00000</td>\n",
       "      <td>79.00000</td>\n",
       "      <td>140.00000</td>\n",
       "      <td>90.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15217.372</td>\n",
       "      <td>1</td>\n",
       "      <td>169.00000</td>\n",
       "      <td>57.00000</td>\n",
       "      <td>130.00000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>21256.832</td>\n",
       "      <td>2</td>\n",
       "      <td>176.00000</td>\n",
       "      <td>90.00000</td>\n",
       "      <td>120.00000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>23423.223</td>\n",
       "      <td>1</td>\n",
       "      <td>139.22299</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>130.00000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>21869.270</td>\n",
       "      <td>1</td>\n",
       "      <td>167.00000</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>120.00000</td>\n",
       "      <td>90.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>21123.682</td>\n",
       "      <td>1</td>\n",
       "      <td>165.00000</td>\n",
       "      <td>109.26121</td>\n",
       "      <td>160.00000</td>\n",
       "      <td>1109.1909</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>23475.818</td>\n",
       "      <td>1</td>\n",
       "      <td>143.71674</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>130.00000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             age  gender     height     weight      ap_hi      ap_lo  \\\n",
       "0      20372.195       2  172.00000   97.00000  120.00000    80.0000   \n",
       "1      21135.533       2  177.00000   85.00000  120.00000    80.0000   \n",
       "2      21979.768       2  174.00000   66.00000  134.89389    80.0000   \n",
       "3      21754.598       2  165.00000   79.00000  140.00000    90.0000   \n",
       "4      15217.372       1  169.00000   57.00000  130.00000    80.0000   \n",
       "...          ...     ...        ...        ...        ...        ...   \n",
       "69995  21256.832       2  176.00000   90.00000  120.00000    80.0000   \n",
       "69996  23423.223       1  139.22299   75.00000  130.00000    80.0000   \n",
       "69997  21869.270       1  167.00000   75.00000  120.00000    90.0000   \n",
       "69998  21123.682       1  165.00000  109.26121  160.00000  1109.1909   \n",
       "69999  23475.818       1  143.71674   68.00000  130.00000    80.0000   \n",
       "\n",
       "       cholesterol  gluc  smoke  alco  active  \n",
       "0                1     1      0     0       1  \n",
       "1                1     1      1     0       1  \n",
       "2                1     1      0     1       1  \n",
       "3                1     1      0     0       1  \n",
       "4                2     2      0     0       0  \n",
       "...            ...   ...    ...   ...     ...  \n",
       "69995            1     1      0     0       1  \n",
       "69996            3     1      0     0       1  \n",
       "69997            1     1      0     0       1  \n",
       "69998            3     3      0     0       1  \n",
       "69999            3     3      0     0       0  \n",
       "\n",
       "[70000 rows x 11 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_tabsyn = pd.read_csv(\"centralized_data_gen/cardio_synthetic_tabsyn.csv\")\n",
    "cardio_tabsyn = cardio_tabsyn.iloc[:, 1:12]\n",
    "cardio_tabsyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>19240</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>19066</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>22431</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>20540</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0      18393       2     168    62.0    110     80            1     1      0   \n",
       "1      20228       1     156    85.0    140     90            3     1      0   \n",
       "2      18857       1     165    64.0    130     70            3     1      0   \n",
       "3      17623       2     169    82.0    150    100            1     1      0   \n",
       "4      17474       1     156    56.0    100     60            1     1      0   \n",
       "...      ...     ...     ...     ...    ...    ...          ...   ...    ...   \n",
       "69995  19240       2     168    76.0    120     80            1     1      1   \n",
       "69996  22601       1     158   126.0    140     90            2     2      0   \n",
       "69997  19066       2     183   105.0    180     90            3     1      0   \n",
       "69998  22431       1     163    72.0    135     80            1     2      0   \n",
       "69999  20540       1     170    72.0    120     80            2     1      0   \n",
       "\n",
       "       alco  active  \n",
       "0         0       1  \n",
       "1         0       1  \n",
       "2         0       0  \n",
       "3         0       1  \n",
       "4         0       0  \n",
       "...     ...     ...  \n",
       "69995     0       1  \n",
       "69996     0       1  \n",
       "69997     1       0  \n",
       "69998     0       0  \n",
       "69999     0       1  \n",
       "\n",
       "[70000 rows x 11 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_real = pd.read_csv(\"Data/cardio_train.csv\", sep=\";\")\n",
    "cardio_real = cardio_real.iloc[:, 1:-1]\n",
    "cardio_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8909749564838968\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(cardio_tabsyn.to_numpy(), cardio_real.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank - 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>10,065128</td>\n",
       "      <td>55,93166</td>\n",
       "      <td>92387,74</td>\n",
       "      <td>1</td>\n",
       "      <td>0,8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>21,362486</td>\n",
       "      <td>155,44936</td>\n",
       "      <td>95089,984</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>343,6373</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>53,982037</td>\n",
       "      <td>91311,17</td>\n",
       "      <td>0</td>\n",
       "      <td>0,18908173</td>\n",
       "      <td>1</td>\n",
       "      <td>137,3138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>34,410744</td>\n",
       "      <td>43,39613</td>\n",
       "      <td>95621,39</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1</td>\n",
       "      <td>1</td>\n",
       "      <td>148,27242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>70,47527</td>\n",
       "      <td>94304,56</td>\n",
       "      <td>0</td>\n",
       "      <td>1,8962245</td>\n",
       "      <td>1</td>\n",
       "      <td>171,70033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>27,16002</td>\n",
       "      <td>0</td>\n",
       "      <td>79,37031</td>\n",
       "      <td>95835,51</td>\n",
       "      <td>0</td>\n",
       "      <td>1,6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>109,13015</td>\n",
       "      <td>95837,234</td>\n",
       "      <td>0</td>\n",
       "      <td>1,1</td>\n",
       "      <td>3</td>\n",
       "      <td>240,12534</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>46</td>\n",
       "      <td>17,454306</td>\n",
       "      <td>134,11798</td>\n",
       "      <td>92860,63</td>\n",
       "      <td>0</td>\n",
       "      <td>1,8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>29,378838</td>\n",
       "      <td>5</td>\n",
       "      <td>31,09678</td>\n",
       "      <td>95051,76</td>\n",
       "      <td>0</td>\n",
       "      <td>0,33038783</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>20,010065</td>\n",
       "      <td>91320</td>\n",
       "      <td>0</td>\n",
       "      <td>0,2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Age Experience     Income   ZIP Code  Family       CCAvg  \\\n",
       "0            38  10,065128   55,93166   92387,74       1         0,8   \n",
       "1            47  21,362486  155,44936  95089,984       0           2   \n",
       "2            53         31  53,982037   91311,17       0  0,18908173   \n",
       "3            60  34,410744   43,39613   95621,39       0         1,1   \n",
       "4            41         14   70,47527   94304,56       0   1,8962245   \n",
       "...         ...        ...        ...        ...     ...         ...   \n",
       "4995   27,16002          0   79,37031   95835,51       0         1,6   \n",
       "4996         55         25  109,13015  95837,234       0         1,1   \n",
       "4997         46  17,454306  134,11798   92860,63       0         1,8   \n",
       "4998  29,378838          5   31,09678   95051,76       0  0,33038783   \n",
       "4999         45         22  20,010065      91320       0         0,2   \n",
       "\n",
       "      Education   Mortgage  Personal Loan  Securities Account  CD Account  \\\n",
       "0             2          0              0                   1           1   \n",
       "1             1   343,6373              0                   0           0   \n",
       "2             1   137,3138              0                   0           0   \n",
       "3             1  148,27242              0                   0           0   \n",
       "4             1  171,70033              0                   0           0   \n",
       "...         ...        ...            ...                 ...         ...   \n",
       "4995          2          0              0                   0           0   \n",
       "4996          3  240,12534              0                   0           0   \n",
       "4997          1          0              0                   0           0   \n",
       "4998          1          0              0                   0           0   \n",
       "4999          1          0              0                   0           0   \n",
       "\n",
       "      Online  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "4995       1  \n",
       "4996       1  \n",
       "4997       1  \n",
       "4998       0  \n",
       "4999       0  \n",
       "\n",
       "[5000 rows x 12 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_tabsyn = pd.read_csv(\"centralized_data_gen/bank_tabsyn_synthetic.csv\", sep=\";\")\n",
    "bank_tabsyn = bank_tabsyn.iloc[:, 1:13]\n",
    "bank_tabsyn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>92697</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>92037</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>93023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>90034</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>92612</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "0      25           1      49     91107       4    1.6          1         0   \n",
       "1      45          19      34     90089       3    1.5          1         0   \n",
       "2      39          15      11     94720       1    1.0          1         0   \n",
       "3      35           9     100     94112       1    2.7          2         0   \n",
       "4      35           8      45     91330       4    1.0          2         0   \n",
       "...   ...         ...     ...       ...     ...    ...        ...       ...   \n",
       "4995   29           3      40     92697       1    1.9          3         0   \n",
       "4996   30           4      15     92037       4    0.4          1        85   \n",
       "4997   63          39      24     93023       2    0.3          3         0   \n",
       "4998   65          40      49     90034       3    0.5          2         0   \n",
       "4999   28           4      83     92612       3    0.8          1         0   \n",
       "\n",
       "      Personal Loan  Securities Account  CD Account  Online  \n",
       "0                 0                   1           0       0  \n",
       "1                 0                   1           0       0  \n",
       "2                 0                   0           0       0  \n",
       "3                 0                   0           0       0  \n",
       "4                 0                   0           0       0  \n",
       "...             ...                 ...         ...     ...  \n",
       "4995              0                   0           0       1  \n",
       "4996              0                   0           0       1  \n",
       "4997              0                   0           0       0  \n",
       "4998              0                   0           0       1  \n",
       "4999              0                   0           0       1  \n",
       "\n",
       "[5000 rows x 12 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_real = pd.read_csv(\"Data/bank.csv\", sep=\",\")\n",
    "bank_real = bank_real.iloc[:, 1:-1]\n",
    "bank_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.9269400954686651\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(bank_real.to_numpy(), bank_tabsyn.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes - 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.705798</td>\n",
       "      <td>99.219070</td>\n",
       "      <td>68.331560</td>\n",
       "      <td>32.854282</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>27.010769</td>\n",
       "      <td>0.186255</td>\n",
       "      <td>27.698145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.443970</td>\n",
       "      <td>120.690850</td>\n",
       "      <td>87.298210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>43.948967</td>\n",
       "      <td>0.263599</td>\n",
       "      <td>56.661964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.463222</td>\n",
       "      <td>133.451500</td>\n",
       "      <td>62.535930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>23.505415</td>\n",
       "      <td>0.127230</td>\n",
       "      <td>55.177315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>74.589380</td>\n",
       "      <td>69.129030</td>\n",
       "      <td>12.637459</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>26.711203</td>\n",
       "      <td>0.340420</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>183.244490</td>\n",
       "      <td>64.735660</td>\n",
       "      <td>32.231907</td>\n",
       "      <td>224.37398</td>\n",
       "      <td>27.595322</td>\n",
       "      <td>0.249921</td>\n",
       "      <td>57.578530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.743928</td>\n",
       "      <td>147.832150</td>\n",
       "      <td>63.244427</td>\n",
       "      <td>32.024063</td>\n",
       "      <td>132.08727</td>\n",
       "      <td>32.672330</td>\n",
       "      <td>0.760614</td>\n",
       "      <td>24.831545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>97.704155</td>\n",
       "      <td>73.403250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.904074</td>\n",
       "      <td>0.144953</td>\n",
       "      <td>42.103560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.116669</td>\n",
       "      <td>127.130470</td>\n",
       "      <td>83.693180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.091141</td>\n",
       "      <td>0.161974</td>\n",
       "      <td>25.291174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>6.050511</td>\n",
       "      <td>159.605290</td>\n",
       "      <td>87.569960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>31.508020</td>\n",
       "      <td>0.176828</td>\n",
       "      <td>59.165478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>3.169146</td>\n",
       "      <td>111.322350</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.277502</td>\n",
       "      <td>0.160153</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies     Glucose  BloodPressure  SkinThickness    Insulin  \\\n",
       "0       4.705798   99.219070      68.331560      32.854282    0.00000   \n",
       "1      16.443970  120.690850      87.298210       0.000000    0.00000   \n",
       "2       9.463222  133.451500      62.535930       0.000000    0.00000   \n",
       "3       4.000000   74.589380      69.129030      12.637459    0.00000   \n",
       "4       4.000000  183.244490      64.735660      32.231907  224.37398   \n",
       "..           ...         ...            ...            ...        ...   \n",
       "763     0.743928  147.832150      63.244427      32.024063  132.08727   \n",
       "764     4.000000   97.704155      73.403250       0.000000    0.00000   \n",
       "765     5.116669  127.130470      83.693180       0.000000    0.00000   \n",
       "766     6.050511  159.605290      87.569960       0.000000    0.00000   \n",
       "767     3.169146  111.322350       0.003821       0.000000    0.00000   \n",
       "\n",
       "           BMI  DiabetesPedigreeFunction        Age  \n",
       "0    27.010769                  0.186255  27.698145  \n",
       "1    43.948967                  0.263599  56.661964  \n",
       "2    23.505415                  0.127230  55.177315  \n",
       "3    26.711203                  0.340420  23.000000  \n",
       "4    27.595322                  0.249921  57.578530  \n",
       "..         ...                       ...        ...  \n",
       "763  32.672330                  0.760614  24.831545  \n",
       "764  30.904074                  0.144953  42.103560  \n",
       "765   6.091141                  0.161974  25.291174  \n",
       "766  31.508020                  0.176828  59.165478  \n",
       "767   5.277502                  0.160153  21.000000  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_tabsyn = pd.read_csv(\"centralized_data_gen/diabetes_synthetic_tabsyn.csv\")\n",
    "diabetes_tabsyn = diabetes_tabsyn.iloc[:, :8]\n",
    "diabetes_tabsyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  insu  mass   pedi  age\n",
       "0       6   148    72    35     0  33.6  0.627   50\n",
       "1       1    85    66    29     0  26.6  0.351   31\n",
       "2       8   183    64     0     0  23.3  0.672   32\n",
       "3       1    89    66    23    94  28.1  0.167   21\n",
       "4       0   137    40    35   168  43.1  2.288   33\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...\n",
       "763    10   101    76    48   180  32.9  0.171   63\n",
       "764     2   122    70    27     0  36.8  0.340   27\n",
       "765     5   121    72    23   112  26.2  0.245   30\n",
       "766     1   126    60     0     0  30.1  0.349   47\n",
       "767     1    93    70    31     0  30.4  0.315   23\n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_real = pd.read_csv(\"Data/diabetes.csv\")\n",
    "diabetes_real = diabetes_real.iloc[:, :-1]\n",
    "diabetes_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8436536206167308\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(diabetes_real.to_numpy(), diabetes_tabsyn.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('distributed-3-9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71c2bb8a9aafac3189ddd139a8a38ed74f79c7c3756567015645529460b394b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
