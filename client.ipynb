{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import pearsonr, ks_2samp\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_check():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diabetes_data(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cardio_data(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bank_data(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abalone_data(path, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "    ont_hot_encoder_abalone = OneHotEncoder(sparse_output=False)\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    one_hot_encoded = ont_hot_encoder_abalone.fit_transform(df[categorical_columns])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=ont_hot_encoder_abalone.get_feature_names_out(categorical_columns))\n",
    "\n",
    "    df_encoded = pd.concat([one_hot_df, df], axis=1)\n",
    "    # Drop the original categorical columns\n",
    "    df= df_encoded.drop(categorical_columns, axis=1)\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    df_base = df.iloc[:, :-1]\n",
    "    df_target = df.iloc[:,-1].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_train = standardizer.fit_transform(x)\n",
    "    x_train = torch.from_numpy(x_train).to(device)\n",
    "    return x_train, standardizer, df_target, ont_hot_encoder_abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path, data_loader_func ,sep=\",\"):\n",
    "        data = data_loader_func(path, sep)\n",
    "        self.x, self.standardizer, self.outcome = data[:3]\n",
    "        self.len=self.x.shape[0]\n",
    "\n",
    "        if len(data) > 3:\n",
    "            self.one_hot_encoder_abalone = data[3]\n",
    "        else:\n",
    "            self.one_hot_encoder_abalone = None\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n",
    "\n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "\n",
    "#         # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "#        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "#         # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "#         self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "#         self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "\n",
    "#         # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        lin1 = self.gelu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.gelu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.gelu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.fc1(lin3))\n",
    "\n",
    "        r1 = self.fc21(fc1) # Generating mu\n",
    "        r2 = self.fc22(fc1) # Generating sigma\n",
    "\n",
    "        return r1, r2\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_() # Convert it to std deviation\n",
    "            eps = Variable(std.data.new(std.size()).normal_()) # Generate a noise of same size as std\n",
    "            return eps.mul(std).add_(mu) # Perform reparameterization\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        fc3 = self.gelu(self.fc3(z)) # Not sure why these two are required. \n",
    "        fc4 = self.gelu(self.fc4(fc3))#.view(128, -1)\n",
    "\n",
    "        lin4 = self.gelu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.gelu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # self.decode(z) ist sp√§ter recon_batch, mu ist mu und logvar ist logvar\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def embed(self,x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    # x_recon is the reconstruction batch created in the forward pass of the model, x is the original x batch, mu is mu, and logvar is logvar\n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = cuda_check()\n",
    "# D_in = data_set.x.shape[1]\n",
    "# H = 50\n",
    "# H2 = 12\n",
    "# model = Autoencoder(D_in, H, H2).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_mse = customLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_model(data_set, H, H2):\n",
    "    device = cuda_check()\n",
    "    D_in = data_set.x.shape[1]\n",
    "    model = Autoencoder(D_in, H, H2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_mse = customLoss()\n",
    "    return model, loss_mse, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trian the Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(DATA_PATH, loader_func, sep=\",\"):\n",
    "    device = cuda_check()\n",
    "\n",
    "    data_set=DataBuilder(DATA_PATH, loader_func , sep)\n",
    "    trainloader=DataLoader(dataset=data_set,batch_size=32)\n",
    "\n",
    "\n",
    "    model, loss_mse, optimizer = declare_model(data_set, 50, 12)\n",
    "    # Refactor\n",
    "    epochs = 1000\n",
    "    train_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, data in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        if epoch % 2 == 0:\n",
    "            print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "                epoch, train_loss / len(trainloader.dataset)))\n",
    "            train_losses.append(train_loss / len(trainloader.dataset))\n",
    "    return model, trainloader.dataset.standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 15.2255\n",
      "====> Epoch: 4 Average loss: 13.2242\n",
      "====> Epoch: 6 Average loss: 11.6673\n",
      "====> Epoch: 8 Average loss: 10.8127\n",
      "====> Epoch: 10 Average loss: 10.0439\n",
      "====> Epoch: 12 Average loss: 9.6780\n",
      "====> Epoch: 14 Average loss: 9.1477\n",
      "====> Epoch: 16 Average loss: 8.7563\n",
      "====> Epoch: 18 Average loss: 8.3776\n",
      "====> Epoch: 20 Average loss: 8.1180\n",
      "====> Epoch: 22 Average loss: 7.8314\n",
      "====> Epoch: 24 Average loss: 7.7385\n",
      "====> Epoch: 26 Average loss: 7.5789\n",
      "====> Epoch: 28 Average loss: 7.3782\n",
      "====> Epoch: 30 Average loss: 7.1825\n",
      "====> Epoch: 32 Average loss: 7.1746\n",
      "====> Epoch: 34 Average loss: 7.0268\n",
      "====> Epoch: 36 Average loss: 6.9396\n",
      "====> Epoch: 38 Average loss: 6.7982\n",
      "====> Epoch: 40 Average loss: 6.8535\n",
      "====> Epoch: 42 Average loss: 6.7480\n",
      "====> Epoch: 44 Average loss: 6.5847\n",
      "====> Epoch: 46 Average loss: 6.6423\n",
      "====> Epoch: 48 Average loss: 6.5244\n",
      "====> Epoch: 50 Average loss: 6.5967\n",
      "====> Epoch: 52 Average loss: 6.4165\n",
      "====> Epoch: 54 Average loss: 6.3871\n",
      "====> Epoch: 56 Average loss: 6.4098\n",
      "====> Epoch: 58 Average loss: 6.4690\n",
      "====> Epoch: 60 Average loss: 6.4038\n",
      "====> Epoch: 62 Average loss: 6.4084\n",
      "====> Epoch: 64 Average loss: 6.2690\n",
      "====> Epoch: 66 Average loss: 6.2512\n",
      "====> Epoch: 68 Average loss: 6.2938\n",
      "====> Epoch: 70 Average loss: 6.1864\n",
      "====> Epoch: 72 Average loss: 6.2598\n",
      "====> Epoch: 74 Average loss: 6.2144\n",
      "====> Epoch: 76 Average loss: 6.3554\n",
      "====> Epoch: 78 Average loss: 6.2373\n",
      "====> Epoch: 80 Average loss: 6.1764\n",
      "====> Epoch: 82 Average loss: 6.1888\n",
      "====> Epoch: 84 Average loss: 6.1849\n",
      "====> Epoch: 86 Average loss: 6.1563\n",
      "====> Epoch: 88 Average loss: 6.1418\n",
      "====> Epoch: 90 Average loss: 6.1966\n",
      "====> Epoch: 92 Average loss: 6.1603\n",
      "====> Epoch: 94 Average loss: 6.2654\n",
      "====> Epoch: 96 Average loss: 6.2007\n",
      "====> Epoch: 98 Average loss: 6.2223\n",
      "====> Epoch: 100 Average loss: 6.2146\n",
      "====> Epoch: 102 Average loss: 6.2172\n",
      "====> Epoch: 104 Average loss: 6.1257\n",
      "====> Epoch: 106 Average loss: 6.1544\n",
      "====> Epoch: 108 Average loss: 6.2066\n",
      "====> Epoch: 110 Average loss: 6.1095\n",
      "====> Epoch: 112 Average loss: 6.1092\n",
      "====> Epoch: 114 Average loss: 6.1647\n",
      "====> Epoch: 116 Average loss: 6.1730\n",
      "====> Epoch: 118 Average loss: 6.0832\n",
      "====> Epoch: 120 Average loss: 6.2406\n",
      "====> Epoch: 122 Average loss: 6.1336\n",
      "====> Epoch: 124 Average loss: 6.0892\n",
      "====> Epoch: 126 Average loss: 6.1421\n",
      "====> Epoch: 128 Average loss: 6.2098\n",
      "====> Epoch: 130 Average loss: 6.0811\n",
      "====> Epoch: 132 Average loss: 6.1666\n",
      "====> Epoch: 134 Average loss: 6.1734\n",
      "====> Epoch: 136 Average loss: 6.0903\n",
      "====> Epoch: 138 Average loss: 6.0576\n",
      "====> Epoch: 140 Average loss: 6.1214\n",
      "====> Epoch: 142 Average loss: 6.1153\n",
      "====> Epoch: 144 Average loss: 6.0894\n",
      "====> Epoch: 146 Average loss: 6.0575\n",
      "====> Epoch: 148 Average loss: 6.1039\n",
      "====> Epoch: 150 Average loss: 6.1063\n",
      "====> Epoch: 152 Average loss: 6.0810\n",
      "====> Epoch: 154 Average loss: 6.0597\n",
      "====> Epoch: 156 Average loss: 6.2204\n",
      "====> Epoch: 158 Average loss: 6.0182\n",
      "====> Epoch: 160 Average loss: 6.0946\n",
      "====> Epoch: 162 Average loss: 6.1080\n",
      "====> Epoch: 164 Average loss: 6.0738\n",
      "====> Epoch: 166 Average loss: 6.0441\n",
      "====> Epoch: 168 Average loss: 6.1620\n",
      "====> Epoch: 170 Average loss: 6.0812\n",
      "====> Epoch: 172 Average loss: 6.0192\n",
      "====> Epoch: 174 Average loss: 6.1460\n",
      "====> Epoch: 176 Average loss: 6.0176\n",
      "====> Epoch: 178 Average loss: 6.0209\n",
      "====> Epoch: 180 Average loss: 6.0949\n",
      "====> Epoch: 182 Average loss: 6.0852\n",
      "====> Epoch: 184 Average loss: 6.0479\n",
      "====> Epoch: 186 Average loss: 6.0897\n",
      "====> Epoch: 188 Average loss: 6.1595\n",
      "====> Epoch: 190 Average loss: 6.0515\n",
      "====> Epoch: 192 Average loss: 6.0167\n",
      "====> Epoch: 194 Average loss: 6.0289\n",
      "====> Epoch: 196 Average loss: 6.0267\n",
      "====> Epoch: 198 Average loss: 6.0199\n",
      "====> Epoch: 200 Average loss: 6.0524\n",
      "====> Epoch: 202 Average loss: 6.0787\n",
      "====> Epoch: 204 Average loss: 6.0053\n",
      "====> Epoch: 206 Average loss: 6.0401\n",
      "====> Epoch: 208 Average loss: 6.0889\n",
      "====> Epoch: 210 Average loss: 5.9860\n",
      "====> Epoch: 212 Average loss: 6.0110\n",
      "====> Epoch: 214 Average loss: 6.0541\n",
      "====> Epoch: 216 Average loss: 6.0033\n",
      "====> Epoch: 218 Average loss: 6.0592\n",
      "====> Epoch: 220 Average loss: 6.0430\n",
      "====> Epoch: 222 Average loss: 6.0338\n",
      "====> Epoch: 224 Average loss: 6.0214\n",
      "====> Epoch: 226 Average loss: 6.0822\n",
      "====> Epoch: 228 Average loss: 6.0603\n",
      "====> Epoch: 230 Average loss: 5.9677\n",
      "====> Epoch: 232 Average loss: 5.9866\n",
      "====> Epoch: 234 Average loss: 5.9594\n",
      "====> Epoch: 236 Average loss: 5.9688\n",
      "====> Epoch: 238 Average loss: 6.0046\n",
      "====> Epoch: 240 Average loss: 5.9926\n",
      "====> Epoch: 242 Average loss: 6.0172\n",
      "====> Epoch: 244 Average loss: 5.9983\n",
      "====> Epoch: 246 Average loss: 5.9963\n",
      "====> Epoch: 248 Average loss: 5.9866\n",
      "====> Epoch: 250 Average loss: 6.0089\n",
      "====> Epoch: 252 Average loss: 6.0011\n",
      "====> Epoch: 254 Average loss: 6.0072\n",
      "====> Epoch: 256 Average loss: 6.0766\n",
      "====> Epoch: 258 Average loss: 6.0456\n",
      "====> Epoch: 260 Average loss: 6.0435\n",
      "====> Epoch: 262 Average loss: 6.0060\n",
      "====> Epoch: 264 Average loss: 5.9395\n",
      "====> Epoch: 266 Average loss: 5.9743\n",
      "====> Epoch: 268 Average loss: 6.0275\n",
      "====> Epoch: 270 Average loss: 6.0479\n",
      "====> Epoch: 272 Average loss: 6.0074\n",
      "====> Epoch: 274 Average loss: 5.9457\n",
      "====> Epoch: 276 Average loss: 6.0031\n",
      "====> Epoch: 278 Average loss: 5.9357\n",
      "====> Epoch: 280 Average loss: 5.9650\n",
      "====> Epoch: 282 Average loss: 5.9012\n",
      "====> Epoch: 284 Average loss: 6.0203\n",
      "====> Epoch: 286 Average loss: 6.0336\n",
      "====> Epoch: 288 Average loss: 5.8842\n",
      "====> Epoch: 290 Average loss: 6.0600\n",
      "====> Epoch: 292 Average loss: 5.9921\n",
      "====> Epoch: 294 Average loss: 5.9864\n",
      "====> Epoch: 296 Average loss: 5.9709\n",
      "====> Epoch: 298 Average loss: 5.9664\n",
      "====> Epoch: 300 Average loss: 5.9299\n",
      "====> Epoch: 302 Average loss: 5.8914\n",
      "====> Epoch: 304 Average loss: 5.9916\n",
      "====> Epoch: 306 Average loss: 6.0162\n",
      "====> Epoch: 308 Average loss: 5.8672\n",
      "====> Epoch: 310 Average loss: 5.9308\n",
      "====> Epoch: 312 Average loss: 5.9593\n",
      "====> Epoch: 314 Average loss: 5.9432\n",
      "====> Epoch: 316 Average loss: 5.9063\n",
      "====> Epoch: 318 Average loss: 5.9387\n",
      "====> Epoch: 320 Average loss: 5.9729\n",
      "====> Epoch: 322 Average loss: 5.9215\n",
      "====> Epoch: 324 Average loss: 5.9134\n",
      "====> Epoch: 326 Average loss: 5.9334\n",
      "====> Epoch: 328 Average loss: 5.9510\n",
      "====> Epoch: 330 Average loss: 5.8696\n",
      "====> Epoch: 332 Average loss: 5.9059\n",
      "====> Epoch: 334 Average loss: 5.9695\n",
      "====> Epoch: 336 Average loss: 5.9302\n",
      "====> Epoch: 338 Average loss: 5.8741\n",
      "====> Epoch: 340 Average loss: 5.9342\n",
      "====> Epoch: 342 Average loss: 5.9579\n",
      "====> Epoch: 344 Average loss: 5.9515\n",
      "====> Epoch: 346 Average loss: 5.9721\n",
      "====> Epoch: 348 Average loss: 5.9578\n",
      "====> Epoch: 350 Average loss: 5.9224\n",
      "====> Epoch: 352 Average loss: 5.9736\n",
      "====> Epoch: 354 Average loss: 5.8554\n",
      "====> Epoch: 356 Average loss: 5.8923\n",
      "====> Epoch: 358 Average loss: 5.8881\n",
      "====> Epoch: 360 Average loss: 5.9006\n",
      "====> Epoch: 362 Average loss: 5.8341\n",
      "====> Epoch: 364 Average loss: 5.8484\n",
      "====> Epoch: 366 Average loss: 5.8642\n",
      "====> Epoch: 368 Average loss: 5.9239\n",
      "====> Epoch: 370 Average loss: 5.8692\n",
      "====> Epoch: 372 Average loss: 5.9109\n",
      "====> Epoch: 374 Average loss: 5.8931\n",
      "====> Epoch: 376 Average loss: 5.8738\n",
      "====> Epoch: 378 Average loss: 5.8886\n",
      "====> Epoch: 380 Average loss: 5.7963\n",
      "====> Epoch: 382 Average loss: 5.8972\n",
      "====> Epoch: 384 Average loss: 5.8749\n",
      "====> Epoch: 386 Average loss: 5.9097\n",
      "====> Epoch: 388 Average loss: 5.8955\n",
      "====> Epoch: 390 Average loss: 5.7977\n",
      "====> Epoch: 392 Average loss: 5.8962\n",
      "====> Epoch: 394 Average loss: 5.9463\n",
      "====> Epoch: 396 Average loss: 5.8476\n",
      "====> Epoch: 398 Average loss: 5.8496\n",
      "====> Epoch: 400 Average loss: 5.9918\n",
      "====> Epoch: 402 Average loss: 5.9496\n",
      "====> Epoch: 404 Average loss: 5.9059\n",
      "====> Epoch: 406 Average loss: 5.9378\n",
      "====> Epoch: 408 Average loss: 5.9575\n",
      "====> Epoch: 410 Average loss: 5.8678\n",
      "====> Epoch: 412 Average loss: 5.8227\n",
      "====> Epoch: 414 Average loss: 5.8907\n",
      "====> Epoch: 416 Average loss: 5.8792\n",
      "====> Epoch: 418 Average loss: 5.8645\n",
      "====> Epoch: 420 Average loss: 5.9534\n",
      "====> Epoch: 422 Average loss: 5.9322\n",
      "====> Epoch: 424 Average loss: 5.9637\n",
      "====> Epoch: 426 Average loss: 5.8031\n",
      "====> Epoch: 428 Average loss: 5.8298\n",
      "====> Epoch: 430 Average loss: 5.8999\n",
      "====> Epoch: 432 Average loss: 5.8958\n",
      "====> Epoch: 434 Average loss: 5.9068\n",
      "====> Epoch: 436 Average loss: 5.9102\n",
      "====> Epoch: 438 Average loss: 5.8584\n",
      "====> Epoch: 440 Average loss: 5.9046\n",
      "====> Epoch: 442 Average loss: 5.8928\n",
      "====> Epoch: 444 Average loss: 5.8759\n",
      "====> Epoch: 446 Average loss: 5.7468\n",
      "====> Epoch: 448 Average loss: 5.8266\n",
      "====> Epoch: 450 Average loss: 5.7858\n",
      "====> Epoch: 452 Average loss: 5.9292\n",
      "====> Epoch: 454 Average loss: 5.9097\n",
      "====> Epoch: 456 Average loss: 5.8553\n",
      "====> Epoch: 458 Average loss: 5.8267\n",
      "====> Epoch: 460 Average loss: 5.8513\n",
      "====> Epoch: 462 Average loss: 5.8596\n",
      "====> Epoch: 464 Average loss: 5.9016\n",
      "====> Epoch: 466 Average loss: 5.8744\n",
      "====> Epoch: 468 Average loss: 5.8176\n",
      "====> Epoch: 470 Average loss: 5.8937\n",
      "====> Epoch: 472 Average loss: 5.8605\n",
      "====> Epoch: 474 Average loss: 5.8347\n",
      "====> Epoch: 476 Average loss: 5.7884\n",
      "====> Epoch: 478 Average loss: 5.8462\n",
      "====> Epoch: 480 Average loss: 5.8960\n",
      "====> Epoch: 482 Average loss: 5.8760\n",
      "====> Epoch: 484 Average loss: 5.7953\n",
      "====> Epoch: 486 Average loss: 5.9447\n",
      "====> Epoch: 488 Average loss: 5.9261\n",
      "====> Epoch: 490 Average loss: 5.8362\n",
      "====> Epoch: 492 Average loss: 5.8357\n",
      "====> Epoch: 494 Average loss: 5.8529\n",
      "====> Epoch: 496 Average loss: 5.8264\n",
      "====> Epoch: 498 Average loss: 5.8290\n",
      "====> Epoch: 500 Average loss: 5.8841\n",
      "====> Epoch: 502 Average loss: 5.7909\n",
      "====> Epoch: 504 Average loss: 5.8905\n",
      "====> Epoch: 506 Average loss: 5.8270\n",
      "====> Epoch: 508 Average loss: 5.8996\n",
      "====> Epoch: 510 Average loss: 5.8462\n",
      "====> Epoch: 512 Average loss: 5.8690\n",
      "====> Epoch: 514 Average loss: 5.8743\n",
      "====> Epoch: 516 Average loss: 5.8416\n",
      "====> Epoch: 518 Average loss: 5.9078\n",
      "====> Epoch: 520 Average loss: 5.8460\n",
      "====> Epoch: 522 Average loss: 5.9218\n",
      "====> Epoch: 524 Average loss: 5.8280\n",
      "====> Epoch: 526 Average loss: 5.9144\n",
      "====> Epoch: 528 Average loss: 5.7837\n",
      "====> Epoch: 530 Average loss: 5.8517\n",
      "====> Epoch: 532 Average loss: 5.8537\n",
      "====> Epoch: 534 Average loss: 5.8048\n",
      "====> Epoch: 536 Average loss: 5.8119\n",
      "====> Epoch: 538 Average loss: 5.8058\n",
      "====> Epoch: 540 Average loss: 5.9100\n",
      "====> Epoch: 542 Average loss: 5.9313\n",
      "====> Epoch: 544 Average loss: 5.8117\n",
      "====> Epoch: 546 Average loss: 5.7997\n",
      "====> Epoch: 548 Average loss: 5.7852\n",
      "====> Epoch: 550 Average loss: 5.7961\n",
      "====> Epoch: 552 Average loss: 5.8468\n",
      "====> Epoch: 554 Average loss: 5.8259\n",
      "====> Epoch: 556 Average loss: 5.8439\n",
      "====> Epoch: 558 Average loss: 5.8164\n",
      "====> Epoch: 560 Average loss: 5.8026\n",
      "====> Epoch: 562 Average loss: 5.8305\n",
      "====> Epoch: 564 Average loss: 5.8157\n",
      "====> Epoch: 566 Average loss: 5.8399\n",
      "====> Epoch: 568 Average loss: 5.8267\n",
      "====> Epoch: 570 Average loss: 5.7808\n",
      "====> Epoch: 572 Average loss: 5.7988\n",
      "====> Epoch: 574 Average loss: 5.9310\n",
      "====> Epoch: 576 Average loss: 5.8082\n",
      "====> Epoch: 578 Average loss: 5.8839\n",
      "====> Epoch: 580 Average loss: 5.8537\n",
      "====> Epoch: 582 Average loss: 5.8007\n",
      "====> Epoch: 584 Average loss: 5.7966\n",
      "====> Epoch: 586 Average loss: 5.7690\n",
      "====> Epoch: 588 Average loss: 5.7916\n",
      "====> Epoch: 590 Average loss: 5.8013\n",
      "====> Epoch: 592 Average loss: 5.8212\n",
      "====> Epoch: 594 Average loss: 5.8060\n",
      "====> Epoch: 596 Average loss: 5.8043\n",
      "====> Epoch: 598 Average loss: 5.8853\n",
      "====> Epoch: 600 Average loss: 5.8324\n",
      "====> Epoch: 602 Average loss: 5.8150\n",
      "====> Epoch: 604 Average loss: 5.8412\n",
      "====> Epoch: 606 Average loss: 5.7712\n",
      "====> Epoch: 608 Average loss: 5.7716\n",
      "====> Epoch: 610 Average loss: 5.8390\n",
      "====> Epoch: 612 Average loss: 5.7703\n",
      "====> Epoch: 614 Average loss: 5.8369\n",
      "====> Epoch: 616 Average loss: 5.7703\n",
      "====> Epoch: 618 Average loss: 5.7294\n",
      "====> Epoch: 620 Average loss: 5.8205\n",
      "====> Epoch: 622 Average loss: 5.7300\n",
      "====> Epoch: 624 Average loss: 5.7964\n",
      "====> Epoch: 626 Average loss: 5.6322\n",
      "====> Epoch: 628 Average loss: 5.8155\n",
      "====> Epoch: 630 Average loss: 5.8112\n",
      "====> Epoch: 632 Average loss: 5.7209\n",
      "====> Epoch: 634 Average loss: 5.8069\n",
      "====> Epoch: 636 Average loss: 5.8189\n",
      "====> Epoch: 638 Average loss: 5.8632\n",
      "====> Epoch: 640 Average loss: 5.7953\n",
      "====> Epoch: 642 Average loss: 5.8039\n",
      "====> Epoch: 644 Average loss: 5.7001\n",
      "====> Epoch: 646 Average loss: 5.7504\n",
      "====> Epoch: 648 Average loss: 5.7995\n",
      "====> Epoch: 650 Average loss: 5.8135\n",
      "====> Epoch: 652 Average loss: 5.7614\n",
      "====> Epoch: 654 Average loss: 5.8275\n",
      "====> Epoch: 656 Average loss: 5.7826\n",
      "====> Epoch: 658 Average loss: 5.9217\n",
      "====> Epoch: 660 Average loss: 5.7747\n",
      "====> Epoch: 662 Average loss: 5.7688\n",
      "====> Epoch: 664 Average loss: 5.8715\n",
      "====> Epoch: 666 Average loss: 5.7858\n",
      "====> Epoch: 668 Average loss: 5.7974\n",
      "====> Epoch: 670 Average loss: 5.8208\n",
      "====> Epoch: 672 Average loss: 5.7830\n",
      "====> Epoch: 674 Average loss: 5.8782\n",
      "====> Epoch: 676 Average loss: 5.7997\n",
      "====> Epoch: 678 Average loss: 5.8743\n",
      "====> Epoch: 680 Average loss: 5.7396\n",
      "====> Epoch: 682 Average loss: 5.7867\n",
      "====> Epoch: 684 Average loss: 5.7847\n",
      "====> Epoch: 686 Average loss: 5.7314\n",
      "====> Epoch: 688 Average loss: 5.7455\n",
      "====> Epoch: 690 Average loss: 5.7522\n",
      "====> Epoch: 692 Average loss: 5.8016\n",
      "====> Epoch: 694 Average loss: 5.7250\n",
      "====> Epoch: 696 Average loss: 5.7857\n",
      "====> Epoch: 698 Average loss: 5.7305\n",
      "====> Epoch: 700 Average loss: 5.8017\n",
      "====> Epoch: 702 Average loss: 5.7876\n",
      "====> Epoch: 704 Average loss: 5.7496\n",
      "====> Epoch: 706 Average loss: 5.6781\n",
      "====> Epoch: 708 Average loss: 5.8570\n",
      "====> Epoch: 710 Average loss: 5.8375\n",
      "====> Epoch: 712 Average loss: 5.7748\n",
      "====> Epoch: 714 Average loss: 5.7316\n",
      "====> Epoch: 716 Average loss: 5.7956\n",
      "====> Epoch: 718 Average loss: 5.6925\n",
      "====> Epoch: 720 Average loss: 5.6888\n",
      "====> Epoch: 722 Average loss: 5.7442\n",
      "====> Epoch: 724 Average loss: 5.7844\n",
      "====> Epoch: 726 Average loss: 5.7660\n",
      "====> Epoch: 728 Average loss: 5.7984\n",
      "====> Epoch: 730 Average loss: 5.8112\n",
      "====> Epoch: 732 Average loss: 5.8035\n",
      "====> Epoch: 734 Average loss: 5.7556\n",
      "====> Epoch: 736 Average loss: 5.7071\n",
      "====> Epoch: 738 Average loss: 5.7711\n",
      "====> Epoch: 740 Average loss: 5.7701\n",
      "====> Epoch: 742 Average loss: 5.7568\n",
      "====> Epoch: 744 Average loss: 5.7656\n",
      "====> Epoch: 746 Average loss: 5.7006\n",
      "====> Epoch: 748 Average loss: 5.7923\n",
      "====> Epoch: 750 Average loss: 5.7313\n",
      "====> Epoch: 752 Average loss: 5.7588\n",
      "====> Epoch: 754 Average loss: 5.7999\n",
      "====> Epoch: 756 Average loss: 5.6490\n",
      "====> Epoch: 758 Average loss: 5.7256\n",
      "====> Epoch: 760 Average loss: 5.7294\n",
      "====> Epoch: 762 Average loss: 5.7163\n",
      "====> Epoch: 764 Average loss: 5.7289\n",
      "====> Epoch: 766 Average loss: 5.7400\n",
      "====> Epoch: 768 Average loss: 5.7272\n",
      "====> Epoch: 770 Average loss: 5.8815\n",
      "====> Epoch: 772 Average loss: 5.7217\n",
      "====> Epoch: 774 Average loss: 5.7718\n",
      "====> Epoch: 776 Average loss: 5.7548\n",
      "====> Epoch: 778 Average loss: 5.6791\n",
      "====> Epoch: 780 Average loss: 5.7408\n",
      "====> Epoch: 782 Average loss: 5.7464\n",
      "====> Epoch: 784 Average loss: 5.8165\n",
      "====> Epoch: 786 Average loss: 5.7439\n",
      "====> Epoch: 788 Average loss: 5.7161\n",
      "====> Epoch: 790 Average loss: 5.7470\n",
      "====> Epoch: 792 Average loss: 5.7847\n",
      "====> Epoch: 794 Average loss: 5.7417\n",
      "====> Epoch: 796 Average loss: 5.7658\n",
      "====> Epoch: 798 Average loss: 5.8024\n",
      "====> Epoch: 800 Average loss: 5.7530\n",
      "====> Epoch: 802 Average loss: 5.7882\n",
      "====> Epoch: 804 Average loss: 5.7179\n",
      "====> Epoch: 806 Average loss: 5.7151\n",
      "====> Epoch: 808 Average loss: 5.7729\n",
      "====> Epoch: 810 Average loss: 5.7138\n",
      "====> Epoch: 812 Average loss: 5.7822\n",
      "====> Epoch: 814 Average loss: 5.7725\n",
      "====> Epoch: 816 Average loss: 5.7397\n",
      "====> Epoch: 818 Average loss: 5.7114\n",
      "====> Epoch: 820 Average loss: 5.6700\n",
      "====> Epoch: 822 Average loss: 5.6958\n",
      "====> Epoch: 824 Average loss: 5.7511\n",
      "====> Epoch: 826 Average loss: 5.7037\n",
      "====> Epoch: 828 Average loss: 5.7603\n",
      "====> Epoch: 830 Average loss: 5.7945\n",
      "====> Epoch: 832 Average loss: 5.7062\n",
      "====> Epoch: 834 Average loss: 5.6793\n",
      "====> Epoch: 836 Average loss: 5.6660\n",
      "====> Epoch: 838 Average loss: 5.7905\n",
      "====> Epoch: 840 Average loss: 5.7679\n",
      "====> Epoch: 842 Average loss: 5.6612\n",
      "====> Epoch: 844 Average loss: 5.7797\n",
      "====> Epoch: 846 Average loss: 5.6616\n",
      "====> Epoch: 848 Average loss: 5.7460\n",
      "====> Epoch: 850 Average loss: 5.7146\n",
      "====> Epoch: 852 Average loss: 5.7389\n",
      "====> Epoch: 854 Average loss: 5.7731\n",
      "====> Epoch: 856 Average loss: 5.7874\n",
      "====> Epoch: 858 Average loss: 5.7766\n",
      "====> Epoch: 860 Average loss: 5.6013\n",
      "====> Epoch: 862 Average loss: 5.8012\n",
      "====> Epoch: 864 Average loss: 5.7907\n",
      "====> Epoch: 866 Average loss: 5.8183\n",
      "====> Epoch: 868 Average loss: 5.7617\n",
      "====> Epoch: 870 Average loss: 5.7251\n",
      "====> Epoch: 872 Average loss: 5.8115\n",
      "====> Epoch: 874 Average loss: 5.7633\n",
      "====> Epoch: 876 Average loss: 5.6814\n",
      "====> Epoch: 878 Average loss: 5.7301\n",
      "====> Epoch: 880 Average loss: 5.6919\n",
      "====> Epoch: 882 Average loss: 5.6978\n",
      "====> Epoch: 884 Average loss: 5.6757\n",
      "====> Epoch: 886 Average loss: 5.6843\n",
      "====> Epoch: 888 Average loss: 5.7641\n",
      "====> Epoch: 890 Average loss: 5.6984\n",
      "====> Epoch: 892 Average loss: 5.7156\n",
      "====> Epoch: 894 Average loss: 5.7326\n",
      "====> Epoch: 896 Average loss: 5.7252\n",
      "====> Epoch: 898 Average loss: 5.6544\n",
      "====> Epoch: 900 Average loss: 5.6964\n",
      "====> Epoch: 902 Average loss: 5.6898\n",
      "====> Epoch: 904 Average loss: 5.7263\n",
      "====> Epoch: 906 Average loss: 5.6940\n",
      "====> Epoch: 908 Average loss: 5.7139\n",
      "====> Epoch: 910 Average loss: 5.7169\n",
      "====> Epoch: 912 Average loss: 5.7893\n",
      "====> Epoch: 914 Average loss: 5.6777\n",
      "====> Epoch: 916 Average loss: 5.7198\n",
      "====> Epoch: 918 Average loss: 5.7654\n",
      "====> Epoch: 920 Average loss: 5.7240\n",
      "====> Epoch: 922 Average loss: 5.7553\n",
      "====> Epoch: 924 Average loss: 5.7567\n",
      "====> Epoch: 926 Average loss: 5.7460\n",
      "====> Epoch: 928 Average loss: 5.7208\n",
      "====> Epoch: 930 Average loss: 5.7324\n",
      "====> Epoch: 932 Average loss: 5.7299\n",
      "====> Epoch: 934 Average loss: 5.8264\n",
      "====> Epoch: 936 Average loss: 5.7535\n",
      "====> Epoch: 938 Average loss: 5.7374\n",
      "====> Epoch: 940 Average loss: 5.6365\n",
      "====> Epoch: 942 Average loss: 5.6778\n",
      "====> Epoch: 944 Average loss: 5.7002\n",
      "====> Epoch: 946 Average loss: 5.7109\n",
      "====> Epoch: 948 Average loss: 5.7538\n",
      "====> Epoch: 950 Average loss: 5.6816\n",
      "====> Epoch: 952 Average loss: 5.7720\n",
      "====> Epoch: 954 Average loss: 5.7457\n",
      "====> Epoch: 956 Average loss: 5.7217\n",
      "====> Epoch: 958 Average loss: 5.7519\n",
      "====> Epoch: 960 Average loss: 5.7331\n",
      "====> Epoch: 962 Average loss: 5.6742\n",
      "====> Epoch: 964 Average loss: 5.7753\n",
      "====> Epoch: 966 Average loss: 5.7511\n",
      "====> Epoch: 968 Average loss: 5.7007\n",
      "====> Epoch: 970 Average loss: 5.6942\n",
      "====> Epoch: 972 Average loss: 5.6922\n",
      "====> Epoch: 974 Average loss: 5.6500\n",
      "====> Epoch: 976 Average loss: 5.8051\n",
      "====> Epoch: 978 Average loss: 5.6897\n",
      "====> Epoch: 980 Average loss: 5.7332\n",
      "====> Epoch: 982 Average loss: 5.7007\n",
      "====> Epoch: 984 Average loss: 5.6651\n",
      "====> Epoch: 986 Average loss: 5.6184\n",
      "====> Epoch: 988 Average loss: 5.6313\n",
      "====> Epoch: 990 Average loss: 5.7117\n",
      "====> Epoch: 992 Average loss: 5.7600\n",
      "====> Epoch: 994 Average loss: 5.7206\n",
      "====> Epoch: 996 Average loss: 5.7068\n",
      "====> Epoch: 998 Average loss: 5.6692\n",
      "====> Epoch: 1000 Average loss: 5.6663\n"
     ]
    }
   ],
   "source": [
    "model, standardizer = train_encoder('Data/diabetes.csv', load_diabetes_data, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_for_diabetes_ds(model):\n",
    "    DATA_PATH = \"Data/diabetes.csv\"\n",
    "    df = load_diabetes_data(DATA_PATH, sep=\",\")\n",
    "    actual_data = df[0]\n",
    "    outcomes = df[2]\n",
    "    outcomes_numeric = [1 if outcome == \"b'tested_positive'\" else 0 for outcome in outcomes]\n",
    "\n",
    "    latents = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, e in enumerate(actual_data):\n",
    "            sample = e.unsqueeze(0)  # Add batch dimension\n",
    "            latent = model.embed(sample)  # Get the latent representation\n",
    "            latents.append(latent.squeeze().cpu().numpy())\n",
    "\n",
    "    latents_df = pd.DataFrame(latents)\n",
    "    outcomes_df = pd.DataFrame(outcomes_numeric)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv('latent_data/diabetes_latent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_for_diabetes_ds(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_of_latent(model, npy_x_data_file_path, npy_y_data_file_path, standardizer):\n",
    "    generated_latent_x = np.load(npy_x_data_file_path)\n",
    "    generaated_latent_y = np.load(npy_y_data_file_path)\n",
    "    generated_torch_data = torch.from_numpy(generated_latent_x).float()\n",
    "\n",
    "    z = model.decode(generated_torch_data)\n",
    "\n",
    "    generated_data_x = standardizer.inverse_transform(z.cpu().detach().numpy())\n",
    "    return generated_data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real = load_diabetes_data(\"Data/diabetes.csv\")\n",
    "x_real = x_real[1].inverse_transform(x_real[0].cpu().detach().numpy())\n",
    "x_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_syn = reconstruction_of_latent(model, 'X_num_train.npy', 'y_diabetes_train.npy', standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorical_similarity(col_real, col_synthetic):\n",
    "    # Compute Theil's U for categorical features\n",
    "    p_real = pd.Series(col_real).value_counts(normalize=True)\n",
    "    p_synthetic = pd.Series(col_synthetic).value_counts(normalize=True)\n",
    "    u = (p_real * np.log(p_real / p_synthetic)).sum()\n",
    "    return 1 - u\n",
    "def column_similarity(real_data, synthetic_data):\n",
    "    similarities = []\n",
    "    for col_real, col_synthetic in zip(real_data, synthetic_data):\n",
    "        correlation, _ = pearsonr(col_real, col_synthetic)\n",
    "        similarity = correlation\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "def correlation_similarity(real_data, synthetic_data):\n",
    "    real_corr = np.corrcoef(real_data, rowvar=False)\n",
    "    synthetic_corr = np.corrcoef(synthetic_data, rowvar=False)\n",
    "    correlation, _ = pearsonr(real_corr.flatten(), synthetic_corr.flatten())\n",
    "    return correlation\n",
    "def jensen_shannon_similarity(real_data, synthetic_data):\n",
    "    similarities = []\n",
    "    for col_real, col_synthetic in zip(real_data.T, synthetic_data.T):\n",
    "        # Compute probability distributions and Jensen-Shannon divergence\n",
    "        p_real = np.histogram(col_real, bins=10, density=True)[0]\n",
    "        p_synthetic = np.histogram(col_synthetic, bins=10, density=True)[0]\n",
    "        similarity = 1 - jensenshannon(p_real, p_synthetic)\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "def kolmogorov_smirnov_similarity(real_data, synthetic_data):\n",
    "    similarities = []\n",
    "    for col_real, col_synthetic in zip(real_data.T, synthetic_data.T):\n",
    "        # Compute cumulative distributions and Kolmogorov-Smirnov distance\n",
    "        _, p_value = ks_2samp(col_real, col_synthetic)\n",
    "        similarity = 1 - p_value\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "def propensity_mean_absolute_similarity(real_data, synthetic_data):\n",
    "    # Train XGBoost classifier to discriminate between real and synthetic samples\n",
    "    X = np.vstack([real_data, synthetic_data])\n",
    "    y = np.concatenate([np.ones(len(real_data)), np.zeros(len(synthetic_data))])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    classifier = XGBClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Compute mean absolute error of classifier probabilities\n",
    "    y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    error = mean_absolute_error(y_test, y_pred_proba)\n",
    "    return 1 - error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resemblance_measure(real_data, synthetic_data):\n",
    "    resemblance_score = (\n",
    "        column_similarity(real_data, synthetic_data) +\n",
    "        correlation_similarity(real_data, synthetic_data) +\n",
    "        jensen_shannon_similarity(real_data, synthetic_data) +\n",
    "        kolmogorov_smirnov_similarity(real_data, synthetic_data) +\n",
    "        propensity_mean_absolute_similarity(real_data, synthetic_data)\n",
    "    ) / 5\n",
    "    print(\"Resemblance Score:\", resemblance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8612875825328349\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_syn[:768], x_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cardio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 9.8872\n",
      "====> Epoch: 4 Average loss: 9.6558\n",
      "====> Epoch: 6 Average loss: 9.5732\n",
      "====> Epoch: 8 Average loss: 9.4705\n",
      "====> Epoch: 10 Average loss: 9.4279\n",
      "====> Epoch: 12 Average loss: 9.3978\n",
      "====> Epoch: 14 Average loss: 9.3661\n",
      "====> Epoch: 16 Average loss: 9.3469\n",
      "====> Epoch: 18 Average loss: 9.3409\n",
      "====> Epoch: 20 Average loss: 9.3224\n",
      "====> Epoch: 22 Average loss: 9.3005\n",
      "====> Epoch: 24 Average loss: 9.2791\n",
      "====> Epoch: 26 Average loss: 9.2800\n",
      "====> Epoch: 28 Average loss: 9.2714\n",
      "====> Epoch: 30 Average loss: 9.2637\n",
      "====> Epoch: 32 Average loss: 9.2540\n",
      "====> Epoch: 34 Average loss: 9.2464\n",
      "====> Epoch: 36 Average loss: 9.2477\n",
      "====> Epoch: 38 Average loss: 9.2233\n",
      "====> Epoch: 40 Average loss: 9.2268\n",
      "====> Epoch: 42 Average loss: 9.2185\n",
      "====> Epoch: 44 Average loss: 9.2141\n",
      "====> Epoch: 46 Average loss: 9.2151\n",
      "====> Epoch: 48 Average loss: 9.2138\n",
      "====> Epoch: 50 Average loss: 9.2058\n",
      "====> Epoch: 52 Average loss: 9.1916\n",
      "====> Epoch: 54 Average loss: 9.1927\n",
      "====> Epoch: 56 Average loss: 9.1916\n",
      "====> Epoch: 58 Average loss: 9.2004\n",
      "====> Epoch: 60 Average loss: 9.1845\n",
      "====> Epoch: 62 Average loss: 9.1757\n",
      "====> Epoch: 64 Average loss: 9.1734\n",
      "====> Epoch: 66 Average loss: 9.1695\n",
      "====> Epoch: 68 Average loss: 9.1796\n",
      "====> Epoch: 70 Average loss: 9.1648\n",
      "====> Epoch: 72 Average loss: 9.1730\n",
      "====> Epoch: 74 Average loss: 9.1710\n",
      "====> Epoch: 76 Average loss: 9.1635\n",
      "====> Epoch: 78 Average loss: 9.1540\n",
      "====> Epoch: 80 Average loss: 9.1527\n",
      "====> Epoch: 82 Average loss: 9.1537\n",
      "====> Epoch: 84 Average loss: 9.1543\n",
      "====> Epoch: 86 Average loss: 9.1468\n",
      "====> Epoch: 88 Average loss: 9.1508\n",
      "====> Epoch: 90 Average loss: 9.1432\n",
      "====> Epoch: 92 Average loss: 9.1460\n",
      "====> Epoch: 94 Average loss: 9.1468\n",
      "====> Epoch: 96 Average loss: 9.1407\n",
      "====> Epoch: 98 Average loss: 9.1363\n",
      "====> Epoch: 100 Average loss: 9.1406\n",
      "====> Epoch: 102 Average loss: 9.1384\n",
      "====> Epoch: 104 Average loss: 9.1314\n",
      "====> Epoch: 106 Average loss: 9.1359\n",
      "====> Epoch: 108 Average loss: 9.1270\n",
      "====> Epoch: 110 Average loss: 9.1329\n",
      "====> Epoch: 112 Average loss: 9.1311\n",
      "====> Epoch: 114 Average loss: 9.1359\n",
      "====> Epoch: 116 Average loss: 9.1259\n",
      "====> Epoch: 118 Average loss: 9.1256\n",
      "====> Epoch: 120 Average loss: 9.1257\n",
      "====> Epoch: 122 Average loss: 9.1314\n",
      "====> Epoch: 124 Average loss: 9.1286\n",
      "====> Epoch: 126 Average loss: 9.1179\n",
      "====> Epoch: 128 Average loss: 9.1294\n",
      "====> Epoch: 130 Average loss: 9.1141\n",
      "====> Epoch: 132 Average loss: 9.1122\n",
      "====> Epoch: 134 Average loss: 9.1187\n",
      "====> Epoch: 136 Average loss: 9.1074\n",
      "====> Epoch: 138 Average loss: 9.1176\n",
      "====> Epoch: 140 Average loss: 9.1179\n",
      "====> Epoch: 142 Average loss: 9.1213\n",
      "====> Epoch: 144 Average loss: 9.1103\n",
      "====> Epoch: 146 Average loss: 9.1269\n",
      "====> Epoch: 148 Average loss: 9.1165\n",
      "====> Epoch: 150 Average loss: 9.1154\n",
      "====> Epoch: 152 Average loss: 9.1065\n",
      "====> Epoch: 154 Average loss: 9.1152\n",
      "====> Epoch: 156 Average loss: 9.1115\n",
      "====> Epoch: 158 Average loss: 9.1228\n",
      "====> Epoch: 160 Average loss: 9.1062\n",
      "====> Epoch: 162 Average loss: 9.1099\n",
      "====> Epoch: 164 Average loss: 9.1030\n",
      "====> Epoch: 166 Average loss: 9.1106\n",
      "====> Epoch: 168 Average loss: 9.1181\n",
      "====> Epoch: 170 Average loss: 9.1156\n",
      "====> Epoch: 172 Average loss: 9.1138\n",
      "====> Epoch: 174 Average loss: 9.1158\n",
      "====> Epoch: 176 Average loss: 9.1126\n",
      "====> Epoch: 178 Average loss: 9.0951\n",
      "====> Epoch: 180 Average loss: 9.1114\n",
      "====> Epoch: 182 Average loss: 9.1137\n",
      "====> Epoch: 184 Average loss: 9.1079\n",
      "====> Epoch: 186 Average loss: 9.0964\n",
      "====> Epoch: 188 Average loss: 9.1074\n",
      "====> Epoch: 190 Average loss: 9.1073\n",
      "====> Epoch: 192 Average loss: 9.1126\n",
      "====> Epoch: 194 Average loss: 9.1159\n",
      "====> Epoch: 196 Average loss: 9.1060\n",
      "====> Epoch: 198 Average loss: 9.0949\n",
      "====> Epoch: 200 Average loss: 9.1050\n",
      "====> Epoch: 202 Average loss: 9.1042\n",
      "====> Epoch: 204 Average loss: 9.0982\n",
      "====> Epoch: 206 Average loss: 9.0969\n",
      "====> Epoch: 208 Average loss: 9.0960\n",
      "====> Epoch: 210 Average loss: 9.0811\n",
      "====> Epoch: 212 Average loss: 9.0910\n",
      "====> Epoch: 214 Average loss: 9.0889\n",
      "====> Epoch: 216 Average loss: 9.0979\n",
      "====> Epoch: 218 Average loss: 9.0955\n",
      "====> Epoch: 220 Average loss: 9.0972\n",
      "====> Epoch: 222 Average loss: 9.0945\n",
      "====> Epoch: 224 Average loss: 9.0957\n",
      "====> Epoch: 226 Average loss: 9.0950\n",
      "====> Epoch: 228 Average loss: 9.0910\n",
      "====> Epoch: 230 Average loss: 9.0904\n",
      "====> Epoch: 232 Average loss: 9.0906\n",
      "====> Epoch: 234 Average loss: 9.0880\n",
      "====> Epoch: 236 Average loss: 9.0856\n",
      "====> Epoch: 238 Average loss: 9.0868\n",
      "====> Epoch: 240 Average loss: 9.0856\n",
      "====> Epoch: 242 Average loss: 9.0907\n",
      "====> Epoch: 244 Average loss: 9.0813\n",
      "====> Epoch: 246 Average loss: 9.0874\n",
      "====> Epoch: 248 Average loss: 9.0782\n",
      "====> Epoch: 250 Average loss: 9.0802\n",
      "====> Epoch: 252 Average loss: 9.0737\n",
      "====> Epoch: 254 Average loss: 9.0779\n",
      "====> Epoch: 256 Average loss: 9.0817\n",
      "====> Epoch: 258 Average loss: 9.0843\n",
      "====> Epoch: 260 Average loss: 9.0850\n",
      "====> Epoch: 262 Average loss: 9.0759\n",
      "====> Epoch: 264 Average loss: 9.0801\n",
      "====> Epoch: 266 Average loss: 9.0762\n",
      "====> Epoch: 268 Average loss: 9.0822\n",
      "====> Epoch: 270 Average loss: 9.0811\n",
      "====> Epoch: 272 Average loss: 9.0796\n",
      "====> Epoch: 274 Average loss: 9.0829\n",
      "====> Epoch: 276 Average loss: 9.0776\n",
      "====> Epoch: 278 Average loss: 9.0964\n",
      "====> Epoch: 280 Average loss: 9.0846\n",
      "====> Epoch: 282 Average loss: 9.0778\n",
      "====> Epoch: 284 Average loss: 9.0766\n",
      "====> Epoch: 286 Average loss: 9.0745\n",
      "====> Epoch: 288 Average loss: 9.0707\n",
      "====> Epoch: 290 Average loss: 9.0779\n",
      "====> Epoch: 292 Average loss: 9.0879\n",
      "====> Epoch: 294 Average loss: 9.0828\n",
      "====> Epoch: 296 Average loss: 9.0712\n",
      "====> Epoch: 298 Average loss: 9.0713\n",
      "====> Epoch: 300 Average loss: 9.0782\n",
      "====> Epoch: 302 Average loss: 9.0698\n",
      "====> Epoch: 304 Average loss: 9.0757\n",
      "====> Epoch: 306 Average loss: 9.0849\n",
      "====> Epoch: 308 Average loss: 9.0780\n",
      "====> Epoch: 310 Average loss: 9.0610\n",
      "====> Epoch: 312 Average loss: 9.0797\n",
      "====> Epoch: 314 Average loss: 9.0801\n",
      "====> Epoch: 316 Average loss: 9.0632\n",
      "====> Epoch: 318 Average loss: 9.0766\n",
      "====> Epoch: 320 Average loss: 9.0640\n",
      "====> Epoch: 322 Average loss: 9.0759\n",
      "====> Epoch: 324 Average loss: 9.0706\n",
      "====> Epoch: 326 Average loss: 9.0725\n",
      "====> Epoch: 328 Average loss: 9.0660\n",
      "====> Epoch: 330 Average loss: 9.0644\n",
      "====> Epoch: 332 Average loss: 9.0664\n",
      "====> Epoch: 334 Average loss: 9.0642\n",
      "====> Epoch: 336 Average loss: 9.0621\n",
      "====> Epoch: 338 Average loss: 9.0579\n",
      "====> Epoch: 340 Average loss: 9.0642\n",
      "====> Epoch: 342 Average loss: 9.0724\n",
      "====> Epoch: 344 Average loss: 9.0723\n",
      "====> Epoch: 346 Average loss: 9.0682\n",
      "====> Epoch: 348 Average loss: 9.0770\n",
      "====> Epoch: 350 Average loss: 9.0626\n",
      "====> Epoch: 352 Average loss: 9.0562\n",
      "====> Epoch: 354 Average loss: 9.0680\n",
      "====> Epoch: 356 Average loss: 9.0732\n",
      "====> Epoch: 358 Average loss: 9.0603\n",
      "====> Epoch: 360 Average loss: 9.0694\n",
      "====> Epoch: 362 Average loss: 9.0608\n",
      "====> Epoch: 364 Average loss: 9.0755\n",
      "====> Epoch: 366 Average loss: 9.0569\n",
      "====> Epoch: 368 Average loss: 9.0617\n",
      "====> Epoch: 370 Average loss: 9.0583\n",
      "====> Epoch: 372 Average loss: 9.0615\n",
      "====> Epoch: 374 Average loss: 9.0620\n",
      "====> Epoch: 376 Average loss: 9.0657\n",
      "====> Epoch: 378 Average loss: 9.0687\n",
      "====> Epoch: 380 Average loss: 9.0705\n",
      "====> Epoch: 382 Average loss: 9.0654\n",
      "====> Epoch: 384 Average loss: 9.0598\n",
      "====> Epoch: 386 Average loss: 9.0554\n",
      "====> Epoch: 388 Average loss: 9.0629\n",
      "====> Epoch: 390 Average loss: 9.0637\n",
      "====> Epoch: 392 Average loss: 9.0579\n",
      "====> Epoch: 394 Average loss: 9.0573\n",
      "====> Epoch: 396 Average loss: 9.0568\n",
      "====> Epoch: 398 Average loss: 9.0641\n",
      "====> Epoch: 400 Average loss: 9.0700\n",
      "====> Epoch: 402 Average loss: 9.0540\n",
      "====> Epoch: 404 Average loss: 9.0652\n",
      "====> Epoch: 406 Average loss: 9.0554\n",
      "====> Epoch: 408 Average loss: 9.0611\n",
      "====> Epoch: 410 Average loss: 9.0500\n",
      "====> Epoch: 412 Average loss: 9.0589\n",
      "====> Epoch: 414 Average loss: 9.0592\n",
      "====> Epoch: 416 Average loss: 9.0574\n",
      "====> Epoch: 418 Average loss: 9.0569\n",
      "====> Epoch: 420 Average loss: 9.0542\n",
      "====> Epoch: 422 Average loss: 9.0568\n",
      "====> Epoch: 424 Average loss: 9.0508\n",
      "====> Epoch: 426 Average loss: 9.0622\n",
      "====> Epoch: 428 Average loss: 9.0619\n",
      "====> Epoch: 430 Average loss: 9.0510\n",
      "====> Epoch: 432 Average loss: 9.0613\n",
      "====> Epoch: 434 Average loss: 9.0499\n",
      "====> Epoch: 436 Average loss: 9.0517\n",
      "====> Epoch: 438 Average loss: 9.0524\n",
      "====> Epoch: 440 Average loss: 9.0572\n",
      "====> Epoch: 442 Average loss: 9.0587\n",
      "====> Epoch: 444 Average loss: 9.0617\n",
      "====> Epoch: 446 Average loss: 9.0462\n",
      "====> Epoch: 448 Average loss: 9.0579\n",
      "====> Epoch: 450 Average loss: 9.0518\n",
      "====> Epoch: 452 Average loss: 9.0507\n",
      "====> Epoch: 454 Average loss: 9.0603\n",
      "====> Epoch: 456 Average loss: 9.0570\n",
      "====> Epoch: 458 Average loss: 9.0520\n",
      "====> Epoch: 460 Average loss: 9.0499\n",
      "====> Epoch: 462 Average loss: 9.0523\n",
      "====> Epoch: 464 Average loss: 9.0520\n",
      "====> Epoch: 466 Average loss: 9.0511\n",
      "====> Epoch: 468 Average loss: 9.0623\n",
      "====> Epoch: 470 Average loss: 9.0566\n",
      "====> Epoch: 472 Average loss: 9.0533\n",
      "====> Epoch: 474 Average loss: 9.0566\n",
      "====> Epoch: 476 Average loss: 9.0624\n",
      "====> Epoch: 478 Average loss: 9.0632\n",
      "====> Epoch: 480 Average loss: 9.0611\n",
      "====> Epoch: 482 Average loss: 9.0524\n",
      "====> Epoch: 484 Average loss: 9.0504\n",
      "====> Epoch: 486 Average loss: 9.0491\n",
      "====> Epoch: 488 Average loss: 9.0572\n",
      "====> Epoch: 490 Average loss: 9.0484\n",
      "====> Epoch: 492 Average loss: 9.0512\n",
      "====> Epoch: 494 Average loss: 9.0663\n",
      "====> Epoch: 496 Average loss: 9.0593\n",
      "====> Epoch: 498 Average loss: 9.0538\n",
      "====> Epoch: 500 Average loss: 9.0543\n",
      "====> Epoch: 502 Average loss: 9.0524\n",
      "====> Epoch: 504 Average loss: 9.0510\n",
      "====> Epoch: 506 Average loss: 9.0534\n",
      "====> Epoch: 508 Average loss: 9.0543\n",
      "====> Epoch: 510 Average loss: 9.0550\n",
      "====> Epoch: 512 Average loss: 9.0427\n",
      "====> Epoch: 514 Average loss: 9.0550\n",
      "====> Epoch: 516 Average loss: 9.0613\n",
      "====> Epoch: 518 Average loss: 9.0559\n",
      "====> Epoch: 520 Average loss: 9.0507\n",
      "====> Epoch: 522 Average loss: 9.0514\n",
      "====> Epoch: 524 Average loss: 9.0511\n",
      "====> Epoch: 526 Average loss: 9.0437\n",
      "====> Epoch: 528 Average loss: 9.0485\n",
      "====> Epoch: 530 Average loss: 9.0529\n",
      "====> Epoch: 532 Average loss: 9.0539\n",
      "====> Epoch: 534 Average loss: 9.0557\n",
      "====> Epoch: 536 Average loss: 9.0574\n",
      "====> Epoch: 538 Average loss: 9.0531\n",
      "====> Epoch: 540 Average loss: 9.0515\n",
      "====> Epoch: 542 Average loss: 9.0542\n",
      "====> Epoch: 544 Average loss: 9.0435\n",
      "====> Epoch: 546 Average loss: 9.0537\n",
      "====> Epoch: 548 Average loss: 9.0512\n",
      "====> Epoch: 550 Average loss: 9.0497\n",
      "====> Epoch: 552 Average loss: 9.0470\n",
      "====> Epoch: 554 Average loss: 9.0516\n",
      "====> Epoch: 556 Average loss: 9.0442\n",
      "====> Epoch: 558 Average loss: 9.0499\n",
      "====> Epoch: 560 Average loss: 9.0507\n",
      "====> Epoch: 562 Average loss: 9.0369\n",
      "====> Epoch: 564 Average loss: 9.0439\n",
      "====> Epoch: 566 Average loss: 9.0430\n",
      "====> Epoch: 568 Average loss: 9.0432\n",
      "====> Epoch: 570 Average loss: 9.0499\n",
      "====> Epoch: 572 Average loss: 9.0496\n",
      "====> Epoch: 574 Average loss: 9.0409\n",
      "====> Epoch: 576 Average loss: 9.0489\n",
      "====> Epoch: 578 Average loss: 9.0414\n",
      "====> Epoch: 580 Average loss: 9.0501\n",
      "====> Epoch: 582 Average loss: 9.0402\n",
      "====> Epoch: 584 Average loss: 9.0537\n",
      "====> Epoch: 586 Average loss: 9.0451\n",
      "====> Epoch: 588 Average loss: 9.0494\n",
      "====> Epoch: 590 Average loss: 9.0459\n",
      "====> Epoch: 592 Average loss: 9.0399\n",
      "====> Epoch: 594 Average loss: 9.0381\n",
      "====> Epoch: 596 Average loss: 9.0384\n",
      "====> Epoch: 598 Average loss: 9.0426\n",
      "====> Epoch: 600 Average loss: 9.0333\n",
      "====> Epoch: 602 Average loss: 9.0403\n",
      "====> Epoch: 604 Average loss: 9.0414\n",
      "====> Epoch: 606 Average loss: 9.0306\n",
      "====> Epoch: 608 Average loss: 9.0446\n",
      "====> Epoch: 610 Average loss: 9.0403\n",
      "====> Epoch: 612 Average loss: 9.0475\n",
      "====> Epoch: 614 Average loss: 9.0413\n",
      "====> Epoch: 616 Average loss: 9.0454\n",
      "====> Epoch: 618 Average loss: 9.0510\n",
      "====> Epoch: 620 Average loss: 9.0476\n",
      "====> Epoch: 622 Average loss: 9.0348\n",
      "====> Epoch: 624 Average loss: 9.0377\n",
      "====> Epoch: 626 Average loss: 9.0503\n",
      "====> Epoch: 628 Average loss: 9.0341\n",
      "====> Epoch: 630 Average loss: 9.0419\n",
      "====> Epoch: 632 Average loss: 9.0374\n",
      "====> Epoch: 634 Average loss: 9.0490\n",
      "====> Epoch: 636 Average loss: 9.0481\n",
      "====> Epoch: 638 Average loss: 9.0415\n",
      "====> Epoch: 640 Average loss: 9.0465\n",
      "====> Epoch: 642 Average loss: 9.0375\n",
      "====> Epoch: 644 Average loss: 9.0414\n",
      "====> Epoch: 646 Average loss: 9.0506\n",
      "====> Epoch: 648 Average loss: 9.0384\n",
      "====> Epoch: 650 Average loss: 9.0371\n",
      "====> Epoch: 652 Average loss: 9.0439\n",
      "====> Epoch: 654 Average loss: 9.0389\n",
      "====> Epoch: 656 Average loss: 9.0400\n",
      "====> Epoch: 658 Average loss: 9.0286\n",
      "====> Epoch: 660 Average loss: 9.0311\n",
      "====> Epoch: 662 Average loss: 9.0369\n",
      "====> Epoch: 664 Average loss: 9.0284\n",
      "====> Epoch: 666 Average loss: 9.0330\n",
      "====> Epoch: 668 Average loss: 9.0427\n",
      "====> Epoch: 670 Average loss: 9.0373\n",
      "====> Epoch: 672 Average loss: 9.0353\n",
      "====> Epoch: 674 Average loss: 9.0351\n",
      "====> Epoch: 676 Average loss: 9.0280\n",
      "====> Epoch: 678 Average loss: 9.0492\n",
      "====> Epoch: 680 Average loss: 9.0314\n",
      "====> Epoch: 682 Average loss: 9.0324\n",
      "====> Epoch: 684 Average loss: 9.0341\n",
      "====> Epoch: 686 Average loss: 9.0335\n",
      "====> Epoch: 688 Average loss: 9.0310\n",
      "====> Epoch: 690 Average loss: 9.0422\n",
      "====> Epoch: 692 Average loss: 9.0300\n",
      "====> Epoch: 694 Average loss: 9.0332\n",
      "====> Epoch: 696 Average loss: 9.0320\n",
      "====> Epoch: 698 Average loss: 9.0237\n",
      "====> Epoch: 700 Average loss: 9.0305\n",
      "====> Epoch: 702 Average loss: 9.0270\n",
      "====> Epoch: 704 Average loss: 9.0364\n",
      "====> Epoch: 706 Average loss: 9.0200\n",
      "====> Epoch: 708 Average loss: 9.0244\n",
      "====> Epoch: 710 Average loss: 9.0212\n",
      "====> Epoch: 712 Average loss: 9.0257\n",
      "====> Epoch: 714 Average loss: 9.0301\n",
      "====> Epoch: 716 Average loss: 9.0215\n",
      "====> Epoch: 718 Average loss: 9.0265\n",
      "====> Epoch: 720 Average loss: 9.0314\n",
      "====> Epoch: 722 Average loss: 9.0287\n",
      "====> Epoch: 724 Average loss: 9.0332\n",
      "====> Epoch: 726 Average loss: 9.0272\n",
      "====> Epoch: 728 Average loss: 9.0194\n",
      "====> Epoch: 730 Average loss: 9.0296\n",
      "====> Epoch: 732 Average loss: 9.0238\n",
      "====> Epoch: 734 Average loss: 9.0338\n",
      "====> Epoch: 736 Average loss: 9.0235\n",
      "====> Epoch: 738 Average loss: 9.0255\n",
      "====> Epoch: 740 Average loss: 9.0177\n",
      "====> Epoch: 742 Average loss: 9.0216\n",
      "====> Epoch: 744 Average loss: 9.0201\n",
      "====> Epoch: 746 Average loss: 9.0112\n",
      "====> Epoch: 748 Average loss: 9.0251\n",
      "====> Epoch: 750 Average loss: 9.0216\n",
      "====> Epoch: 752 Average loss: 9.0241\n",
      "====> Epoch: 754 Average loss: 9.0259\n",
      "====> Epoch: 756 Average loss: 9.0201\n",
      "====> Epoch: 758 Average loss: 9.0160\n",
      "====> Epoch: 760 Average loss: 9.0223\n",
      "====> Epoch: 762 Average loss: 9.0164\n",
      "====> Epoch: 764 Average loss: 9.0199\n",
      "====> Epoch: 766 Average loss: 9.0256\n",
      "====> Epoch: 768 Average loss: 9.0196\n",
      "====> Epoch: 770 Average loss: 9.0087\n",
      "====> Epoch: 772 Average loss: 9.0178\n",
      "====> Epoch: 774 Average loss: 9.0166\n",
      "====> Epoch: 776 Average loss: 9.0085\n",
      "====> Epoch: 778 Average loss: 9.0178\n",
      "====> Epoch: 780 Average loss: 9.0259\n",
      "====> Epoch: 782 Average loss: 9.0125\n",
      "====> Epoch: 784 Average loss: 9.0167\n",
      "====> Epoch: 786 Average loss: 9.0192\n",
      "====> Epoch: 788 Average loss: 9.0215\n",
      "====> Epoch: 790 Average loss: 9.0196\n",
      "====> Epoch: 792 Average loss: 9.0162\n",
      "====> Epoch: 794 Average loss: 9.0165\n",
      "====> Epoch: 796 Average loss: 9.0161\n",
      "====> Epoch: 798 Average loss: 9.0137\n",
      "====> Epoch: 800 Average loss: 9.0202\n",
      "====> Epoch: 802 Average loss: 9.0205\n",
      "====> Epoch: 804 Average loss: 9.0167\n",
      "====> Epoch: 806 Average loss: 9.0152\n",
      "====> Epoch: 808 Average loss: 9.0233\n",
      "====> Epoch: 810 Average loss: 9.0187\n",
      "====> Epoch: 812 Average loss: 9.0149\n",
      "====> Epoch: 814 Average loss: 9.0097\n",
      "====> Epoch: 816 Average loss: 9.0033\n",
      "====> Epoch: 818 Average loss: 9.0070\n",
      "====> Epoch: 820 Average loss: 9.0122\n",
      "====> Epoch: 822 Average loss: 9.0193\n",
      "====> Epoch: 824 Average loss: 9.0092\n",
      "====> Epoch: 826 Average loss: 9.0169\n",
      "====> Epoch: 828 Average loss: 9.0143\n",
      "====> Epoch: 830 Average loss: 9.0048\n",
      "====> Epoch: 832 Average loss: 9.0160\n",
      "====> Epoch: 834 Average loss: 9.0084\n",
      "====> Epoch: 836 Average loss: 9.0115\n",
      "====> Epoch: 838 Average loss: 9.0120\n",
      "====> Epoch: 840 Average loss: 9.0243\n",
      "====> Epoch: 842 Average loss: 9.0165\n",
      "====> Epoch: 844 Average loss: 9.0084\n",
      "====> Epoch: 846 Average loss: 9.0131\n",
      "====> Epoch: 848 Average loss: 9.0153\n",
      "====> Epoch: 850 Average loss: 9.0137\n",
      "====> Epoch: 852 Average loss: 9.0079\n",
      "====> Epoch: 854 Average loss: 9.0151\n",
      "====> Epoch: 856 Average loss: 9.0097\n",
      "====> Epoch: 858 Average loss: 9.0091\n",
      "====> Epoch: 860 Average loss: 9.0191\n",
      "====> Epoch: 862 Average loss: 9.0084\n",
      "====> Epoch: 864 Average loss: 9.0062\n",
      "====> Epoch: 866 Average loss: 9.0133\n",
      "====> Epoch: 868 Average loss: 9.0203\n",
      "====> Epoch: 870 Average loss: 9.0027\n",
      "====> Epoch: 872 Average loss: 8.9952\n",
      "====> Epoch: 874 Average loss: 8.9983\n",
      "====> Epoch: 876 Average loss: 9.0157\n",
      "====> Epoch: 878 Average loss: 9.0114\n",
      "====> Epoch: 880 Average loss: 9.0172\n",
      "====> Epoch: 882 Average loss: 9.0148\n",
      "====> Epoch: 884 Average loss: 9.0036\n",
      "====> Epoch: 886 Average loss: 9.0065\n",
      "====> Epoch: 888 Average loss: 9.0097\n",
      "====> Epoch: 890 Average loss: 9.0083\n",
      "====> Epoch: 892 Average loss: 9.0144\n",
      "====> Epoch: 894 Average loss: 9.0131\n",
      "====> Epoch: 896 Average loss: 9.0066\n",
      "====> Epoch: 898 Average loss: 9.0164\n",
      "====> Epoch: 900 Average loss: 9.0150\n",
      "====> Epoch: 902 Average loss: 9.0052\n",
      "====> Epoch: 904 Average loss: 9.0073\n",
      "====> Epoch: 906 Average loss: 9.0057\n",
      "====> Epoch: 908 Average loss: 8.9988\n",
      "====> Epoch: 910 Average loss: 9.0047\n",
      "====> Epoch: 912 Average loss: 9.0098\n",
      "====> Epoch: 914 Average loss: 8.9970\n",
      "====> Epoch: 916 Average loss: 9.0105\n",
      "====> Epoch: 918 Average loss: 9.0104\n",
      "====> Epoch: 920 Average loss: 9.0037\n",
      "====> Epoch: 922 Average loss: 9.0005\n",
      "====> Epoch: 924 Average loss: 9.0093\n",
      "====> Epoch: 926 Average loss: 9.0114\n",
      "====> Epoch: 928 Average loss: 9.0193\n",
      "====> Epoch: 930 Average loss: 9.0091\n",
      "====> Epoch: 932 Average loss: 9.0079\n",
      "====> Epoch: 934 Average loss: 9.0028\n",
      "====> Epoch: 936 Average loss: 9.0228\n",
      "====> Epoch: 938 Average loss: 9.0106\n",
      "====> Epoch: 940 Average loss: 9.0135\n",
      "====> Epoch: 942 Average loss: 9.0170\n",
      "====> Epoch: 944 Average loss: 9.0128\n",
      "====> Epoch: 946 Average loss: 9.0207\n",
      "====> Epoch: 948 Average loss: 9.0021\n",
      "====> Epoch: 950 Average loss: 8.9969\n",
      "====> Epoch: 952 Average loss: 8.9948\n",
      "====> Epoch: 954 Average loss: 9.0058\n",
      "====> Epoch: 956 Average loss: 9.0072\n",
      "====> Epoch: 958 Average loss: 9.0155\n",
      "====> Epoch: 960 Average loss: 9.0075\n",
      "====> Epoch: 962 Average loss: 9.0144\n",
      "====> Epoch: 964 Average loss: 9.0114\n",
      "====> Epoch: 966 Average loss: 9.0028\n",
      "====> Epoch: 968 Average loss: 9.0054\n",
      "====> Epoch: 970 Average loss: 9.0087\n",
      "====> Epoch: 972 Average loss: 9.0016\n",
      "====> Epoch: 974 Average loss: 8.9947\n",
      "====> Epoch: 976 Average loss: 9.0040\n",
      "====> Epoch: 978 Average loss: 9.0023\n",
      "====> Epoch: 980 Average loss: 9.0077\n",
      "====> Epoch: 982 Average loss: 9.0049\n",
      "====> Epoch: 984 Average loss: 9.0004\n",
      "====> Epoch: 986 Average loss: 9.0176\n",
      "====> Epoch: 988 Average loss: 9.0094\n",
      "====> Epoch: 990 Average loss: 8.9873\n",
      "====> Epoch: 992 Average loss: 9.0166\n",
      "====> Epoch: 994 Average loss: 8.9940\n",
      "====> Epoch: 996 Average loss: 9.0028\n",
      "====> Epoch: 998 Average loss: 8.9971\n",
      "====> Epoch: 1000 Average loss: 8.9971\n"
     ]
    }
   ],
   "source": [
    "model_cardio, standardizer_cardio = train_encoder('Data/cardio_train.csv', load_diabetes_data, \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_for_cardio_ds(model):\n",
    "    DATA_PATH = \"Data/cardio_train.csv\"\n",
    "    df = load_diabetes_data(DATA_PATH, sep=\";\")\n",
    "    actual_data = df[0]\n",
    "    outcomes = df[2]\n",
    "\n",
    "    latents = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, e in enumerate(actual_data):\n",
    "            sample = e.unsqueeze(0)  # Add batch dimension\n",
    "            latent = model.embed(sample)  # Get the latent representation\n",
    "            latents.append(latent.squeeze().cpu().numpy())\n",
    "\n",
    "    latents_df = pd.DataFrame(latents)\n",
    "    outcomes_df = pd.DataFrame(outcomes)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv('latent_data/cardio_latent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_for_cardio_ds(model_cardio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cardio_syn = reconstruction_of_latent(model_cardio, 'syn_latent/cardio_synthetic/X_num_unnorm.npy', 'syn_latent/cardio_synthetic/y_train.npy', standardizer_cardio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 12)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cardio_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 12)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cardio_real = load_diabetes_data(\"Data/cardio_train.csv\", \";\")\n",
    "x_cardio_real = x_cardio_real[1].inverse_transform(x_cardio_real[0].cpu().detach().numpy())\n",
    "x_cardio_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.7650878484389891\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_cardio_syn, x_cardio_real[:9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Medium Level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 18.2724\n",
      "====> Epoch: 4 Average loss: 14.4822\n",
      "====> Epoch: 6 Average loss: 12.8783\n",
      "====> Epoch: 8 Average loss: 12.2937\n",
      "====> Epoch: 10 Average loss: 11.9030\n",
      "====> Epoch: 12 Average loss: 11.7320\n",
      "====> Epoch: 14 Average loss: 11.5080\n",
      "====> Epoch: 16 Average loss: 11.3381\n",
      "====> Epoch: 18 Average loss: 11.2089\n",
      "====> Epoch: 20 Average loss: 11.1627\n",
      "====> Epoch: 22 Average loss: 11.0722\n",
      "====> Epoch: 24 Average loss: 11.0496\n",
      "====> Epoch: 26 Average loss: 10.9655\n",
      "====> Epoch: 28 Average loss: 10.9049\n",
      "====> Epoch: 30 Average loss: 10.8614\n",
      "====> Epoch: 32 Average loss: 10.7989\n",
      "====> Epoch: 34 Average loss: 10.7828\n",
      "====> Epoch: 36 Average loss: 10.7792\n",
      "====> Epoch: 38 Average loss: 10.7873\n",
      "====> Epoch: 40 Average loss: 10.6894\n",
      "====> Epoch: 42 Average loss: 10.6745\n",
      "====> Epoch: 44 Average loss: 10.6588\n",
      "====> Epoch: 46 Average loss: 10.6109\n",
      "====> Epoch: 48 Average loss: 10.6120\n",
      "====> Epoch: 50 Average loss: 10.6297\n",
      "====> Epoch: 52 Average loss: 10.6019\n",
      "====> Epoch: 54 Average loss: 10.5862\n",
      "====> Epoch: 56 Average loss: 10.5884\n",
      "====> Epoch: 58 Average loss: 10.5513\n",
      "====> Epoch: 60 Average loss: 10.5472\n",
      "====> Epoch: 62 Average loss: 10.5122\n",
      "====> Epoch: 64 Average loss: 10.5320\n",
      "====> Epoch: 66 Average loss: 10.4930\n",
      "====> Epoch: 68 Average loss: 10.4784\n",
      "====> Epoch: 70 Average loss: 10.4554\n",
      "====> Epoch: 72 Average loss: 10.4956\n",
      "====> Epoch: 74 Average loss: 10.4964\n",
      "====> Epoch: 76 Average loss: 10.4473\n",
      "====> Epoch: 78 Average loss: 10.4761\n",
      "====> Epoch: 80 Average loss: 10.4103\n",
      "====> Epoch: 82 Average loss: 10.4463\n",
      "====> Epoch: 84 Average loss: 10.4260\n",
      "====> Epoch: 86 Average loss: 10.4583\n",
      "====> Epoch: 88 Average loss: 10.4451\n",
      "====> Epoch: 90 Average loss: 10.4033\n",
      "====> Epoch: 92 Average loss: 10.4168\n",
      "====> Epoch: 94 Average loss: 10.4337\n",
      "====> Epoch: 96 Average loss: 10.4256\n",
      "====> Epoch: 98 Average loss: 10.3873\n",
      "====> Epoch: 100 Average loss: 10.4283\n",
      "====> Epoch: 102 Average loss: 10.3847\n",
      "====> Epoch: 104 Average loss: 10.3665\n",
      "====> Epoch: 106 Average loss: 10.3965\n",
      "====> Epoch: 108 Average loss: 10.3899\n",
      "====> Epoch: 110 Average loss: 10.4026\n",
      "====> Epoch: 112 Average loss: 10.3487\n",
      "====> Epoch: 114 Average loss: 10.3551\n",
      "====> Epoch: 116 Average loss: 10.3723\n",
      "====> Epoch: 118 Average loss: 10.3450\n",
      "====> Epoch: 120 Average loss: 10.3315\n",
      "====> Epoch: 122 Average loss: 10.3014\n",
      "====> Epoch: 124 Average loss: 10.3456\n",
      "====> Epoch: 126 Average loss: 10.3372\n",
      "====> Epoch: 128 Average loss: 10.3306\n",
      "====> Epoch: 130 Average loss: 10.2968\n",
      "====> Epoch: 132 Average loss: 10.3163\n",
      "====> Epoch: 134 Average loss: 10.3295\n",
      "====> Epoch: 136 Average loss: 10.3029\n",
      "====> Epoch: 138 Average loss: 10.3004\n",
      "====> Epoch: 140 Average loss: 10.2826\n",
      "====> Epoch: 142 Average loss: 10.2756\n",
      "====> Epoch: 144 Average loss: 10.2932\n",
      "====> Epoch: 146 Average loss: 10.3090\n",
      "====> Epoch: 148 Average loss: 10.2703\n",
      "====> Epoch: 150 Average loss: 10.2448\n",
      "====> Epoch: 152 Average loss: 10.2545\n",
      "====> Epoch: 154 Average loss: 10.2418\n",
      "====> Epoch: 156 Average loss: 10.2436\n",
      "====> Epoch: 158 Average loss: 10.3362\n",
      "====> Epoch: 160 Average loss: 10.3031\n",
      "====> Epoch: 162 Average loss: 10.2651\n",
      "====> Epoch: 164 Average loss: 10.2275\n",
      "====> Epoch: 166 Average loss: 10.2092\n",
      "====> Epoch: 168 Average loss: 10.2461\n",
      "====> Epoch: 170 Average loss: 10.2450\n",
      "====> Epoch: 172 Average loss: 10.2386\n",
      "====> Epoch: 174 Average loss: 10.2672\n",
      "====> Epoch: 176 Average loss: 10.2128\n",
      "====> Epoch: 178 Average loss: 10.2634\n",
      "====> Epoch: 180 Average loss: 10.2344\n",
      "====> Epoch: 182 Average loss: 10.2392\n",
      "====> Epoch: 184 Average loss: 10.1930\n",
      "====> Epoch: 186 Average loss: 10.2051\n",
      "====> Epoch: 188 Average loss: 10.1816\n",
      "====> Epoch: 190 Average loss: 10.1835\n",
      "====> Epoch: 192 Average loss: 10.2236\n",
      "====> Epoch: 194 Average loss: 10.1675\n",
      "====> Epoch: 196 Average loss: 10.2110\n",
      "====> Epoch: 198 Average loss: 10.1557\n",
      "====> Epoch: 200 Average loss: 10.2166\n",
      "====> Epoch: 202 Average loss: 10.1803\n",
      "====> Epoch: 204 Average loss: 10.1603\n",
      "====> Epoch: 206 Average loss: 10.2368\n",
      "====> Epoch: 208 Average loss: 10.2203\n",
      "====> Epoch: 210 Average loss: 10.1829\n",
      "====> Epoch: 212 Average loss: 10.1460\n",
      "====> Epoch: 214 Average loss: 10.1418\n",
      "====> Epoch: 216 Average loss: 10.1828\n",
      "====> Epoch: 218 Average loss: 10.1713\n",
      "====> Epoch: 220 Average loss: 10.1602\n",
      "====> Epoch: 222 Average loss: 10.1871\n",
      "====> Epoch: 224 Average loss: 10.1662\n",
      "====> Epoch: 226 Average loss: 10.1462\n",
      "====> Epoch: 228 Average loss: 10.1707\n",
      "====> Epoch: 230 Average loss: 10.1680\n",
      "====> Epoch: 232 Average loss: 10.1386\n",
      "====> Epoch: 234 Average loss: 10.1808\n",
      "====> Epoch: 236 Average loss: 10.2425\n",
      "====> Epoch: 238 Average loss: 10.1636\n",
      "====> Epoch: 240 Average loss: 10.1773\n",
      "====> Epoch: 242 Average loss: 10.1599\n",
      "====> Epoch: 244 Average loss: 10.1137\n",
      "====> Epoch: 246 Average loss: 10.0957\n",
      "====> Epoch: 248 Average loss: 10.1347\n",
      "====> Epoch: 250 Average loss: 10.1073\n",
      "====> Epoch: 252 Average loss: 10.1129\n",
      "====> Epoch: 254 Average loss: 10.1224\n",
      "====> Epoch: 256 Average loss: 10.0982\n",
      "====> Epoch: 258 Average loss: 10.0868\n",
      "====> Epoch: 260 Average loss: 10.1204\n",
      "====> Epoch: 262 Average loss: 10.1080\n",
      "====> Epoch: 264 Average loss: 10.1372\n",
      "====> Epoch: 266 Average loss: 10.0794\n",
      "====> Epoch: 268 Average loss: 10.1130\n",
      "====> Epoch: 270 Average loss: 10.0921\n",
      "====> Epoch: 272 Average loss: 10.1068\n",
      "====> Epoch: 274 Average loss: 10.1193\n",
      "====> Epoch: 276 Average loss: 10.1077\n",
      "====> Epoch: 278 Average loss: 10.0878\n",
      "====> Epoch: 280 Average loss: 10.1131\n",
      "====> Epoch: 282 Average loss: 10.1000\n",
      "====> Epoch: 284 Average loss: 10.0696\n",
      "====> Epoch: 286 Average loss: 10.0906\n",
      "====> Epoch: 288 Average loss: 10.1008\n",
      "====> Epoch: 290 Average loss: 10.0598\n",
      "====> Epoch: 292 Average loss: 10.0410\n",
      "====> Epoch: 294 Average loss: 10.0556\n",
      "====> Epoch: 296 Average loss: 10.0750\n",
      "====> Epoch: 298 Average loss: 10.0557\n",
      "====> Epoch: 300 Average loss: 10.0517\n",
      "====> Epoch: 302 Average loss: 10.0682\n",
      "====> Epoch: 304 Average loss: 10.0742\n",
      "====> Epoch: 306 Average loss: 10.0875\n",
      "====> Epoch: 308 Average loss: 10.0700\n",
      "====> Epoch: 310 Average loss: 10.0452\n",
      "====> Epoch: 312 Average loss: 10.0457\n",
      "====> Epoch: 314 Average loss: 10.0211\n",
      "====> Epoch: 316 Average loss: 10.0360\n",
      "====> Epoch: 318 Average loss: 10.0384\n",
      "====> Epoch: 320 Average loss: 10.0581\n",
      "====> Epoch: 322 Average loss: 10.0472\n",
      "====> Epoch: 324 Average loss: 10.0165\n",
      "====> Epoch: 326 Average loss: 9.9981\n",
      "====> Epoch: 328 Average loss: 10.0167\n",
      "====> Epoch: 330 Average loss: 10.0233\n",
      "====> Epoch: 332 Average loss: 10.0366\n",
      "====> Epoch: 334 Average loss: 10.0200\n",
      "====> Epoch: 336 Average loss: 10.0383\n",
      "====> Epoch: 338 Average loss: 10.0348\n",
      "====> Epoch: 340 Average loss: 10.0293\n",
      "====> Epoch: 342 Average loss: 10.0049\n",
      "====> Epoch: 344 Average loss: 10.0012\n",
      "====> Epoch: 346 Average loss: 10.0211\n",
      "====> Epoch: 348 Average loss: 10.0195\n",
      "====> Epoch: 350 Average loss: 9.9962\n",
      "====> Epoch: 352 Average loss: 9.9922\n",
      "====> Epoch: 354 Average loss: 10.0201\n",
      "====> Epoch: 356 Average loss: 9.9982\n",
      "====> Epoch: 358 Average loss: 10.0248\n",
      "====> Epoch: 360 Average loss: 10.0071\n",
      "====> Epoch: 362 Average loss: 10.0044\n",
      "====> Epoch: 364 Average loss: 10.0057\n",
      "====> Epoch: 366 Average loss: 9.9890\n",
      "====> Epoch: 368 Average loss: 10.0128\n",
      "====> Epoch: 370 Average loss: 9.9762\n",
      "====> Epoch: 372 Average loss: 10.0070\n",
      "====> Epoch: 374 Average loss: 10.0124\n",
      "====> Epoch: 376 Average loss: 9.9630\n",
      "====> Epoch: 378 Average loss: 9.9824\n",
      "====> Epoch: 380 Average loss: 9.9577\n",
      "====> Epoch: 382 Average loss: 10.0122\n",
      "====> Epoch: 384 Average loss: 9.9768\n",
      "====> Epoch: 386 Average loss: 9.9889\n",
      "====> Epoch: 388 Average loss: 9.9267\n",
      "====> Epoch: 390 Average loss: 9.9891\n",
      "====> Epoch: 392 Average loss: 9.9560\n",
      "====> Epoch: 394 Average loss: 9.9751\n",
      "====> Epoch: 396 Average loss: 9.9377\n",
      "====> Epoch: 398 Average loss: 9.9578\n",
      "====> Epoch: 400 Average loss: 9.9984\n",
      "====> Epoch: 402 Average loss: 9.9722\n",
      "====> Epoch: 404 Average loss: 9.9641\n",
      "====> Epoch: 406 Average loss: 9.9499\n",
      "====> Epoch: 408 Average loss: 9.9562\n",
      "====> Epoch: 410 Average loss: 9.9542\n",
      "====> Epoch: 412 Average loss: 9.9678\n",
      "====> Epoch: 414 Average loss: 9.9734\n",
      "====> Epoch: 416 Average loss: 9.9670\n",
      "====> Epoch: 418 Average loss: 9.9508\n",
      "====> Epoch: 420 Average loss: 9.9505\n",
      "====> Epoch: 422 Average loss: 9.9419\n",
      "====> Epoch: 424 Average loss: 9.9807\n",
      "====> Epoch: 426 Average loss: 9.9141\n",
      "====> Epoch: 428 Average loss: 9.9613\n",
      "====> Epoch: 430 Average loss: 9.9481\n",
      "====> Epoch: 432 Average loss: 9.9833\n",
      "====> Epoch: 434 Average loss: 9.9729\n",
      "====> Epoch: 436 Average loss: 9.9663\n",
      "====> Epoch: 438 Average loss: 9.9335\n",
      "====> Epoch: 440 Average loss: 9.9555\n",
      "====> Epoch: 442 Average loss: 9.9548\n",
      "====> Epoch: 444 Average loss: 9.9406\n",
      "====> Epoch: 446 Average loss: 9.9326\n",
      "====> Epoch: 448 Average loss: 9.9609\n",
      "====> Epoch: 450 Average loss: 9.9452\n",
      "====> Epoch: 452 Average loss: 9.9348\n",
      "====> Epoch: 454 Average loss: 9.9358\n",
      "====> Epoch: 456 Average loss: 9.9427\n",
      "====> Epoch: 458 Average loss: 9.9429\n",
      "====> Epoch: 460 Average loss: 9.9259\n",
      "====> Epoch: 462 Average loss: 9.9664\n",
      "====> Epoch: 464 Average loss: 9.9121\n",
      "====> Epoch: 466 Average loss: 9.9944\n",
      "====> Epoch: 468 Average loss: 9.9252\n",
      "====> Epoch: 470 Average loss: 9.9250\n",
      "====> Epoch: 472 Average loss: 9.9660\n",
      "====> Epoch: 474 Average loss: 9.9248\n",
      "====> Epoch: 476 Average loss: 9.9145\n",
      "====> Epoch: 478 Average loss: 9.9276\n",
      "====> Epoch: 480 Average loss: 9.9258\n",
      "====> Epoch: 482 Average loss: 9.9421\n",
      "====> Epoch: 484 Average loss: 9.9392\n",
      "====> Epoch: 486 Average loss: 9.9403\n",
      "====> Epoch: 488 Average loss: 9.9265\n",
      "====> Epoch: 490 Average loss: 9.9044\n",
      "====> Epoch: 492 Average loss: 9.9334\n",
      "====> Epoch: 494 Average loss: 9.9303\n",
      "====> Epoch: 496 Average loss: 9.9120\n",
      "====> Epoch: 498 Average loss: 9.9192\n",
      "====> Epoch: 500 Average loss: 9.9330\n",
      "====> Epoch: 502 Average loss: 9.9384\n",
      "====> Epoch: 504 Average loss: 9.9049\n",
      "====> Epoch: 506 Average loss: 9.9104\n",
      "====> Epoch: 508 Average loss: 9.9229\n",
      "====> Epoch: 510 Average loss: 9.9278\n",
      "====> Epoch: 512 Average loss: 9.9087\n",
      "====> Epoch: 514 Average loss: 9.9079\n",
      "====> Epoch: 516 Average loss: 9.8994\n",
      "====> Epoch: 518 Average loss: 9.9258\n",
      "====> Epoch: 520 Average loss: 9.9617\n",
      "====> Epoch: 522 Average loss: 9.9182\n",
      "====> Epoch: 524 Average loss: 9.9299\n",
      "====> Epoch: 526 Average loss: 9.9206\n",
      "====> Epoch: 528 Average loss: 9.9176\n",
      "====> Epoch: 530 Average loss: 9.8960\n",
      "====> Epoch: 532 Average loss: 9.9405\n",
      "====> Epoch: 534 Average loss: 9.8856\n",
      "====> Epoch: 536 Average loss: 9.8962\n",
      "====> Epoch: 538 Average loss: 9.9070\n",
      "====> Epoch: 540 Average loss: 9.8890\n",
      "====> Epoch: 542 Average loss: 9.8823\n",
      "====> Epoch: 544 Average loss: 9.8907\n",
      "====> Epoch: 546 Average loss: 9.9209\n",
      "====> Epoch: 548 Average loss: 9.8962\n",
      "====> Epoch: 550 Average loss: 9.8852\n",
      "====> Epoch: 552 Average loss: 9.8686\n",
      "====> Epoch: 554 Average loss: 9.8916\n",
      "====> Epoch: 556 Average loss: 9.9204\n",
      "====> Epoch: 558 Average loss: 9.9055\n",
      "====> Epoch: 560 Average loss: 9.8837\n",
      "====> Epoch: 562 Average loss: 9.8979\n",
      "====> Epoch: 564 Average loss: 9.8724\n",
      "====> Epoch: 566 Average loss: 9.9311\n",
      "====> Epoch: 568 Average loss: 9.8900\n",
      "====> Epoch: 570 Average loss: 9.8896\n",
      "====> Epoch: 572 Average loss: 9.8944\n",
      "====> Epoch: 574 Average loss: 9.8867\n",
      "====> Epoch: 576 Average loss: 9.9014\n",
      "====> Epoch: 578 Average loss: 9.8844\n",
      "====> Epoch: 580 Average loss: 9.8941\n",
      "====> Epoch: 582 Average loss: 9.9092\n",
      "====> Epoch: 584 Average loss: 9.8863\n",
      "====> Epoch: 586 Average loss: 9.8932\n",
      "====> Epoch: 588 Average loss: 9.8941\n",
      "====> Epoch: 590 Average loss: 9.8853\n",
      "====> Epoch: 592 Average loss: 9.8729\n",
      "====> Epoch: 594 Average loss: 9.9201\n",
      "====> Epoch: 596 Average loss: 9.8813\n",
      "====> Epoch: 598 Average loss: 9.8840\n",
      "====> Epoch: 600 Average loss: 9.9295\n",
      "====> Epoch: 602 Average loss: 9.8849\n",
      "====> Epoch: 604 Average loss: 9.8603\n",
      "====> Epoch: 606 Average loss: 9.8614\n",
      "====> Epoch: 608 Average loss: 9.8921\n",
      "====> Epoch: 610 Average loss: 9.8553\n",
      "====> Epoch: 612 Average loss: 9.8687\n",
      "====> Epoch: 614 Average loss: 9.8544\n",
      "====> Epoch: 616 Average loss: 9.8374\n",
      "====> Epoch: 618 Average loss: 9.8524\n",
      "====> Epoch: 620 Average loss: 9.8668\n",
      "====> Epoch: 622 Average loss: 9.8796\n",
      "====> Epoch: 624 Average loss: 9.8879\n",
      "====> Epoch: 626 Average loss: 9.8475\n",
      "====> Epoch: 628 Average loss: 9.8733\n",
      "====> Epoch: 630 Average loss: 9.8731\n",
      "====> Epoch: 632 Average loss: 9.8887\n",
      "====> Epoch: 634 Average loss: 9.8822\n",
      "====> Epoch: 636 Average loss: 9.8502\n",
      "====> Epoch: 638 Average loss: 9.8528\n",
      "====> Epoch: 640 Average loss: 9.8845\n",
      "====> Epoch: 642 Average loss: 9.8431\n",
      "====> Epoch: 644 Average loss: 9.8647\n",
      "====> Epoch: 646 Average loss: 9.8474\n",
      "====> Epoch: 648 Average loss: 9.8753\n",
      "====> Epoch: 650 Average loss: 9.8412\n",
      "====> Epoch: 652 Average loss: 9.8369\n",
      "====> Epoch: 654 Average loss: 9.8262\n",
      "====> Epoch: 656 Average loss: 9.8751\n",
      "====> Epoch: 658 Average loss: 9.8378\n",
      "====> Epoch: 660 Average loss: 9.8508\n",
      "====> Epoch: 662 Average loss: 9.8430\n",
      "====> Epoch: 664 Average loss: 9.8562\n",
      "====> Epoch: 666 Average loss: 9.8454\n",
      "====> Epoch: 668 Average loss: 9.8502\n",
      "====> Epoch: 670 Average loss: 9.8529\n",
      "====> Epoch: 672 Average loss: 9.8738\n",
      "====> Epoch: 674 Average loss: 9.8205\n",
      "====> Epoch: 676 Average loss: 9.8065\n",
      "====> Epoch: 678 Average loss: 9.8472\n",
      "====> Epoch: 680 Average loss: 9.8475\n",
      "====> Epoch: 682 Average loss: 9.8355\n",
      "====> Epoch: 684 Average loss: 9.8274\n",
      "====> Epoch: 686 Average loss: 9.8227\n",
      "====> Epoch: 688 Average loss: 9.8572\n",
      "====> Epoch: 690 Average loss: 9.8296\n",
      "====> Epoch: 692 Average loss: 9.8832\n",
      "====> Epoch: 694 Average loss: 9.8474\n",
      "====> Epoch: 696 Average loss: 9.8339\n",
      "====> Epoch: 698 Average loss: 9.8540\n",
      "====> Epoch: 700 Average loss: 9.8365\n",
      "====> Epoch: 702 Average loss: 9.8487\n",
      "====> Epoch: 704 Average loss: 9.8428\n",
      "====> Epoch: 706 Average loss: 9.8143\n",
      "====> Epoch: 708 Average loss: 9.8378\n",
      "====> Epoch: 710 Average loss: 9.8431\n",
      "====> Epoch: 712 Average loss: 9.8301\n",
      "====> Epoch: 714 Average loss: 9.8286\n",
      "====> Epoch: 716 Average loss: 9.8527\n",
      "====> Epoch: 718 Average loss: 9.8128\n",
      "====> Epoch: 720 Average loss: 9.8173\n",
      "====> Epoch: 722 Average loss: 9.8125\n",
      "====> Epoch: 724 Average loss: 9.8343\n",
      "====> Epoch: 726 Average loss: 9.8530\n",
      "====> Epoch: 728 Average loss: 9.8685\n",
      "====> Epoch: 730 Average loss: 9.8515\n",
      "====> Epoch: 732 Average loss: 9.8370\n",
      "====> Epoch: 734 Average loss: 9.7971\n",
      "====> Epoch: 736 Average loss: 9.8087\n",
      "====> Epoch: 738 Average loss: 9.8264\n",
      "====> Epoch: 740 Average loss: 9.8247\n",
      "====> Epoch: 742 Average loss: 9.8209\n",
      "====> Epoch: 744 Average loss: 9.8093\n",
      "====> Epoch: 746 Average loss: 9.8338\n",
      "====> Epoch: 748 Average loss: 9.7999\n",
      "====> Epoch: 750 Average loss: 9.8273\n",
      "====> Epoch: 752 Average loss: 9.8392\n",
      "====> Epoch: 754 Average loss: 9.8290\n",
      "====> Epoch: 756 Average loss: 9.8076\n",
      "====> Epoch: 758 Average loss: 9.8155\n",
      "====> Epoch: 760 Average loss: 9.8000\n",
      "====> Epoch: 762 Average loss: 9.8299\n",
      "====> Epoch: 764 Average loss: 9.8111\n",
      "====> Epoch: 766 Average loss: 9.7839\n",
      "====> Epoch: 768 Average loss: 9.8092\n",
      "====> Epoch: 770 Average loss: 9.8085\n",
      "====> Epoch: 772 Average loss: 9.8363\n",
      "====> Epoch: 774 Average loss: 9.7730\n",
      "====> Epoch: 776 Average loss: 9.8173\n",
      "====> Epoch: 778 Average loss: 9.8011\n",
      "====> Epoch: 780 Average loss: 9.8098\n",
      "====> Epoch: 782 Average loss: 9.8055\n",
      "====> Epoch: 784 Average loss: 9.8081\n",
      "====> Epoch: 786 Average loss: 9.8101\n",
      "====> Epoch: 788 Average loss: 9.8168\n",
      "====> Epoch: 790 Average loss: 9.8331\n",
      "====> Epoch: 792 Average loss: 9.8086\n",
      "====> Epoch: 794 Average loss: 9.8351\n",
      "====> Epoch: 796 Average loss: 9.8051\n",
      "====> Epoch: 798 Average loss: 9.7950\n",
      "====> Epoch: 800 Average loss: 9.8330\n",
      "====> Epoch: 802 Average loss: 9.8180\n",
      "====> Epoch: 804 Average loss: 9.7840\n",
      "====> Epoch: 806 Average loss: 9.8314\n",
      "====> Epoch: 808 Average loss: 9.7851\n",
      "====> Epoch: 810 Average loss: 9.8127\n",
      "====> Epoch: 812 Average loss: 9.8169\n",
      "====> Epoch: 814 Average loss: 9.8183\n",
      "====> Epoch: 816 Average loss: 9.8096\n",
      "====> Epoch: 818 Average loss: 9.8069\n",
      "====> Epoch: 820 Average loss: 9.8180\n",
      "====> Epoch: 822 Average loss: 9.7988\n",
      "====> Epoch: 824 Average loss: 9.8181\n",
      "====> Epoch: 826 Average loss: 9.7809\n",
      "====> Epoch: 828 Average loss: 9.8517\n",
      "====> Epoch: 830 Average loss: 9.8091\n",
      "====> Epoch: 832 Average loss: 9.7938\n",
      "====> Epoch: 834 Average loss: 9.7838\n",
      "====> Epoch: 836 Average loss: 9.8314\n",
      "====> Epoch: 838 Average loss: 9.8494\n",
      "====> Epoch: 840 Average loss: 9.8070\n",
      "====> Epoch: 842 Average loss: 9.8020\n",
      "====> Epoch: 844 Average loss: 9.8002\n",
      "====> Epoch: 846 Average loss: 9.8036\n",
      "====> Epoch: 848 Average loss: 9.8015\n",
      "====> Epoch: 850 Average loss: 9.8132\n",
      "====> Epoch: 852 Average loss: 9.7923\n",
      "====> Epoch: 854 Average loss: 9.8001\n",
      "====> Epoch: 856 Average loss: 9.7759\n",
      "====> Epoch: 858 Average loss: 9.7772\n",
      "====> Epoch: 860 Average loss: 9.8056\n",
      "====> Epoch: 862 Average loss: 9.8034\n",
      "====> Epoch: 864 Average loss: 9.8482\n",
      "====> Epoch: 866 Average loss: 9.7745\n",
      "====> Epoch: 868 Average loss: 9.8069\n",
      "====> Epoch: 870 Average loss: 9.7935\n",
      "====> Epoch: 872 Average loss: 9.7709\n",
      "====> Epoch: 874 Average loss: 9.7862\n",
      "====> Epoch: 876 Average loss: 9.7783\n",
      "====> Epoch: 878 Average loss: 9.7558\n",
      "====> Epoch: 880 Average loss: 9.7827\n",
      "====> Epoch: 882 Average loss: 9.8163\n",
      "====> Epoch: 884 Average loss: 9.7876\n",
      "====> Epoch: 886 Average loss: 9.7952\n",
      "====> Epoch: 888 Average loss: 9.8220\n",
      "====> Epoch: 890 Average loss: 9.7846\n",
      "====> Epoch: 892 Average loss: 9.7755\n",
      "====> Epoch: 894 Average loss: 9.7913\n",
      "====> Epoch: 896 Average loss: 9.7787\n",
      "====> Epoch: 898 Average loss: 9.7812\n",
      "====> Epoch: 900 Average loss: 9.7920\n",
      "====> Epoch: 902 Average loss: 9.7660\n",
      "====> Epoch: 904 Average loss: 9.7805\n",
      "====> Epoch: 906 Average loss: 9.8058\n",
      "====> Epoch: 908 Average loss: 9.8176\n",
      "====> Epoch: 910 Average loss: 9.7908\n",
      "====> Epoch: 912 Average loss: 9.7922\n",
      "====> Epoch: 914 Average loss: 9.7606\n",
      "====> Epoch: 916 Average loss: 9.7848\n",
      "====> Epoch: 918 Average loss: 9.7641\n",
      "====> Epoch: 920 Average loss: 9.7878\n",
      "====> Epoch: 922 Average loss: 9.7872\n",
      "====> Epoch: 924 Average loss: 9.7819\n",
      "====> Epoch: 926 Average loss: 9.7580\n",
      "====> Epoch: 928 Average loss: 9.8807\n",
      "====> Epoch: 930 Average loss: 9.7842\n",
      "====> Epoch: 932 Average loss: 9.7698\n",
      "====> Epoch: 934 Average loss: 9.7653\n",
      "====> Epoch: 936 Average loss: 9.8369\n",
      "====> Epoch: 938 Average loss: 9.7691\n",
      "====> Epoch: 940 Average loss: 9.7737\n",
      "====> Epoch: 942 Average loss: 9.7790\n",
      "====> Epoch: 944 Average loss: 9.7761\n",
      "====> Epoch: 946 Average loss: 9.7740\n",
      "====> Epoch: 948 Average loss: 9.7484\n",
      "====> Epoch: 950 Average loss: 9.7507\n",
      "====> Epoch: 952 Average loss: 9.7958\n",
      "====> Epoch: 954 Average loss: 9.7643\n",
      "====> Epoch: 956 Average loss: 9.7750\n",
      "====> Epoch: 958 Average loss: 9.7720\n",
      "====> Epoch: 960 Average loss: 9.7786\n",
      "====> Epoch: 962 Average loss: 9.7689\n",
      "====> Epoch: 964 Average loss: 9.7579\n",
      "====> Epoch: 966 Average loss: 9.7624\n",
      "====> Epoch: 968 Average loss: 9.7933\n",
      "====> Epoch: 970 Average loss: 9.7664\n",
      "====> Epoch: 972 Average loss: 9.7714\n",
      "====> Epoch: 974 Average loss: 9.7733\n",
      "====> Epoch: 976 Average loss: 9.7906\n",
      "====> Epoch: 978 Average loss: 9.7791\n",
      "====> Epoch: 980 Average loss: 9.7492\n",
      "====> Epoch: 982 Average loss: 9.7777\n",
      "====> Epoch: 984 Average loss: 9.7502\n",
      "====> Epoch: 986 Average loss: 9.7782\n",
      "====> Epoch: 988 Average loss: 9.8000\n",
      "====> Epoch: 990 Average loss: 9.7653\n",
      "====> Epoch: 992 Average loss: 9.8140\n",
      "====> Epoch: 994 Average loss: 9.7618\n",
      "====> Epoch: 996 Average loss: 9.7855\n",
      "====> Epoch: 998 Average loss: 9.7611\n",
      "====> Epoch: 1000 Average loss: 9.7660\n"
     ]
    }
   ],
   "source": [
    "model_bank, bank_standardizer = train_encoder(\"Data/bank.csv\", load_bank_data, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_for_bank_ds(model):\n",
    "    DATA_PATH = \"Data/bank.csv\"\n",
    "    df = load_diabetes_data(DATA_PATH, sep=\",\")\n",
    "    actual_data = df[0]\n",
    "    outcomes = df[2]\n",
    "\n",
    "    latents = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, e in enumerate(actual_data):\n",
    "            sample = e.unsqueeze(0)  # Add batch dimension\n",
    "            latent = model.embed(sample)  # Get the latent representation\n",
    "            latents.append(latent.squeeze().cpu().numpy())\n",
    "\n",
    "    latents_df = pd.DataFrame(latents)\n",
    "    outcomes_df = pd.DataFrame(outcomes)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv('latent_data/bank_latent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latent_for_bank_ds(model_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bank_syn = reconstruction_of_latent(model_bank, 'syn_latent/bank_synth/X_num_unnorm.npy', 'syn_latent/bank_synth/y_train.npy', bank_standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 13)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bank_real = load_diabetes_data(\"Data/bank.csv\", \",\")\n",
    "x_bank_real = x_bank_real[1].inverse_transform(x_bank_real[0].cpu().detach().numpy())\n",
    "x_bank_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 13)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bank_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resemblance Score: 0.8743903071420271\n"
     ]
    }
   ],
   "source": [
    "resemblance_measure(x_bank_syn[:4999], x_bank_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('distributed-3-9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71c2bb8a9aafac3189ddd139a8a38ed74f79c7c3756567015645529460b394b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
